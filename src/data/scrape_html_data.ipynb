{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The html parsing code in this file is largely based on the help of ChatGPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Date and Time: 2024-02-22 14:25:24\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "\n",
    "import datetime\n",
    "\n",
    "current_time = datetime.datetime.now()\n",
    "print(\"Current Date and Time:\", current_time.strftime(\"%Y-%m-%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LTI Faculty Data\n",
    "[https://lti.cs.cmu.edu/directory/all/154/1](https://lti.cs.cmu.edu/directory/all/154/1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()  \n",
    "urls = [\"https://www.lti.cmu.edu/directory/all/154/1\", \"https://www.lti.cmu.edu/directory/all/154/1?page=1\"]\n",
    "\n",
    "scrape_data = []\n",
    "\n",
    "# create a txt file to store the data\n",
    "data_folder = '../../data/'\n",
    "filename = 'lti_department_data'\n",
    "with open(data_folder + filename + '.txt', 'w') as f:\n",
    "    f.write('')\n",
    "\n",
    "def combine_faculty_info_into_paragraph(name, title, email, office, phone, research_areas):\n",
    "    res = name + 'is a ' + title + 'a at LTI. '\n",
    "    if email != 'N/A':\n",
    "        res += 'Their email is ' + email + '. '\n",
    "    if office != 'N/A':\n",
    "        res += 'Their office is located at ' + office + '. '\n",
    "    if phone != 'N/A':\n",
    "        res += 'You can reach ' + name + ' at ' + phone + '. '\n",
    "    if research_areas != 'N/A':\n",
    "        res += 'Their research areas include ' + research_areas + '.'\n",
    "    res += '\\n'\n",
    "    return res\n",
    "\n",
    "# Iterate through each URL and scrape the data\n",
    "for url in urls:\n",
    "    driver.get(url)\n",
    "    html = driver.page_source\n",
    "    soup = bs(html, 'html.parser')\n",
    "\n",
    "    table = soup.find('table', class_='views-view-grid')\n",
    "    rows = table.find_all('tr')\n",
    "\n",
    "    for row in rows:\n",
    "        cells = row.find_all('td')\n",
    "        row_data = []\n",
    "        for cell in cells:\n",
    "            # Extracting info\n",
    "            # impute N/A if there is no data for a particular field\n",
    "            name_element = cell.find('h2')\n",
    "            if not name_element:\n",
    "                continue\n",
    "            name = name_element.text.strip()\n",
    "            \n",
    "            title_element = cell.find('div', class_='views-field-field-computed-prof-title')\n",
    "            title = title_element.text.strip() if title_element else \"N/A\"\n",
    "            \n",
    "            email_element = cell.find('a', href=lambda href: href and 'mailto' in href)\n",
    "            email = email_element.text.strip() if email_element else \"N/A\"\n",
    "            \n",
    "            office_element = cell.find('div', class_='views-field-field-computed-building')\n",
    "            office = office_element.text.strip().split(':')[1].strip() if office_element else \"N/A\"\n",
    "            \n",
    "            phone_element = cell.find('div', class_='views-field-field-computed-phone')\n",
    "            phone = phone_element.text.strip().split(':')[1].strip() if phone_element else \"N/A\"\n",
    "            \n",
    "            research_areas_element = cell.find('div', class_='views-field-field-research-areas')\n",
    "            research_areas = research_areas_element.text.strip().split(':')[1].strip() if research_areas_element else \"N/A\"\n",
    "\n",
    "            # construct a paragraph for each faculty member\n",
    "            row_paragraph = combine_faculty_info_into_paragraph(name, title, email, office, phone, research_areas)\n",
    "            # write to txt file\n",
    "            with open(data_folder + filename + '.txt', 'a') as f:\n",
    "                f.write(row_paragraph)\n",
    "\n",
    "            # Append the data to the list to form a csv output later\n",
    "            row_data.append({\n",
    "                'Name': name,\n",
    "                'Title': title,\n",
    "                'Email': email,\n",
    "                'Office': office,\n",
    "                'Phone': phone,\n",
    "                'Research Areas': research_areas\n",
    "            })\n",
    "        scrape_data.extend(row_data)\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "df = pd.DataFrame(scrape_data)\n",
    "df.to_csv(data_folder + filename + '.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CMU Commencement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pages = ['schedule', 'graduates', 'visitors', 'faq', 'contact-us']\n",
    "# base_url = 'https://www.cmu.edu/commencement/'\n",
    "\n",
    "# def extract_info(url):\n",
    "#     response = requests.get(url)\n",
    "#     soup = bs(response.content, 'html.parser')\n",
    "    \n",
    "#     content_divs = soup.find_all(\"div\", class_=\"content\")\n",
    "#     content_data = [div.get_text(separator=\"\\n\", strip=True) for div in content_divs]\n",
    "    \n",
    "#     grid_div = soup.find(\"div\", class_=\"grid\")\n",
    "#     grid_data = grid_div.get_text(separator=\"\\n\", strip=True) if grid_div else \"\"\n",
    "    \n",
    "#     return content_data, grid_data\n",
    "\n",
    "# # Save each page scraped data\n",
    "# for page in pages:\n",
    "#     url = base_url + page + '/index.html'\n",
    "#     content_data, grid_data = extract_info(url)   \n",
    "#     scrape_content = pd.DataFrame({\"content\": content_data})    \n",
    "#     scrape_grid = pd.DataFrame({\"grid\": [grid_data]})\n",
    "    \n",
    "#     scrape = pd.concat([scrape_content, scrape_grid], axis=1)\n",
    "#     # scrape.to_csv('commencement/' + page + '_data.csv', index=False)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### commencement schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info(url):\n",
    "    response = requests.get(url)\n",
    "    soup = bs(response.content, 'html.parser')\n",
    "\n",
    "    # content_grid_divs = soup.find_all('div', class_=[\"content\", \"grid\"])\n",
    "    target_divs = soup.find_all('div', class_=lambda x: x and (x == \"content\" or x == \"grid\" and x != \"sidebar\"))\n",
    "    content_grid_data = [div.get_text(separator=\" \", strip=True) for div in target_divs]\n",
    "    \n",
    "    return content_grid_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a txt file to store the data\n",
    "data_folder = '../../data/'\n",
    "filename = 'commencement_schedule_data'\n",
    "with open(data_folder + filename + '.txt', 'w') as f:\n",
    "    f.write('')\n",
    "\n",
    "\n",
    "############ commencement index page ############\n",
    "url = 'https://www.cmu.edu/commencement/index.html'\n",
    "content_grid_data = extract_info(url)\n",
    "content_grid_data = [data.replace(u'\\xa0', r' ') for data in content_grid_data]\n",
    "\n",
    "for data in content_grid_data:\n",
    "    with open(data_folder + filename + '.txt', 'a') as f:\n",
    "        f.write(data + '\\n')\n",
    "\n",
    "\n",
    "############ schedule index page ############\n",
    "url = 'https://www.cmu.edu/commencement/schedule/index.html'\n",
    "content_grid_data = extract_info(url)\n",
    "content_grid_data = [data.replace(u'\\xa0', r' ') for data in content_grid_data]\n",
    "\n",
    "for data in content_grid_data:\n",
    "    with open(data_folder + filename + '.txt', 'a') as f:\n",
    "        f.write(data + '\\n')\n",
    "\n",
    "\n",
    "############## main ceremony ############\n",
    "# Main Commencement Ceremony\n",
    "# 2023 Honorary Degree Recipients\n",
    "# Student Speaker\n",
    "url = 'https://www.cmu.edu/commencement/schedule/main-ceremony.html'\n",
    "content_grid_data = extract_info(url)\n",
    "content_grid_data = [data.replace(u'\\xa0', r' ') for data in content_grid_data]\n",
    "\n",
    "# combine the honorable degree recipients into one paragraph\n",
    "honorary_degree_recipients = content_grid_data[1:3]\n",
    "honorary_degree_recipients = ' '.join(honorary_degree_recipients)\n",
    "content_grid_data = [content_grid_data[0], honorary_degree_recipients] + content_grid_data[3:]\n",
    "\n",
    "for data in content_grid_data:\n",
    "    with open(data_folder + filename + '.txt', 'a') as f:\n",
    "        f.write(data + '\\n')\n",
    "\n",
    "\n",
    "############# Diploma Ceremonies page ############\n",
    "url = 'https://www.cmu.edu/commencement/schedule/diploma-ceremonies.html'\n",
    "response = requests.get(url)\n",
    "soup = bs(response.content, 'html.parser')\n",
    "target_divs = soup.find_all('div', class_=\"content\")\n",
    "content_data = [div.get_text(separator=\" \", strip=True) for div in target_divs]\n",
    "### find the grid div: contains schedule for each diploma ceremony\n",
    "grid_div = soup.find(\"div\", class_=\"grid\")\n",
    "# find all div within grid_div\n",
    "div_in_grid_div = grid_div.find_all('div')\n",
    "# extract the text from each div\n",
    "grid_data = [div.get_text(separator=\" \", strip=True) for div in div_in_grid_div]\n",
    "# append \"Diploma Ceremonies schedule for \" to the beginning of the grid_data\n",
    "grid_data = ['Diploma Ceremonies schedule for '+data for data in grid_data]\n",
    "for data in content_data + grid_data:\n",
    "    with open(data_folder + filename + '.txt', 'a') as f:\n",
    "        f.write(data + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spring Carnival schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_folder = '../../data/'\n",
    "# filename = 'carnival_schedule_data'\n",
    "# with open(data_folder + filename + '.txt', 'w') as f:\n",
    "#     f.write('')\n",
    "\n",
    "# ## Spring Carnival schedule\n",
    "# driver = webdriver.Chrome() \n",
    "# url = 'https://web.cvent.com/event/ab7f7aba-4e7c-4637-a1fc-dd1f608702c4/websitePage:645d57e4-75eb-4769-b2c0-f201a0bfc6ce?locale=en'\n",
    "# headers = {\n",
    "#     'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36'\n",
    "# }\n",
    "# response = requests.get(url, headers=headers)\n",
    "# soup = bs(response.content, 'html.parser')\n",
    "\n",
    "# # Weekend Highlights\n",
    "# target_divs = soup.find('div', class_='container')\n",
    "# content_data = [div.get_text(separator=\" \", strip=True) for div in target_divs]\n",
    "# content_data\n",
    "\n",
    "# <div class=\"Grid__row___K84Rr Grid__small___joj2e\"><div class=\"Grid__column___L6Qbn Grid__col_4___eGcOB Grid__small___joj2e\"><div id=\"widget:4f6248af-a58f-443b-a807-4fee751ffa37\" class=\"App__widgetIdcontainer___ms2gn\"><div data-cvent-id=\"widget-NucleusText-widget:4f6248af-a58f-443b-a807-4fee751ffa37\" class=\"Text__container___Z6gol\" style=\"border-width: 0px; border-color: rgb(255, 255, 255); border-style: none; padding: 15px; background-color: transparent; border-radius: 0px; flex: 1 1 auto;\"><div class=\"\" style=\"font-family: &quot;Open Sans&quot;, Helvetica, Arial, sans-serif; font-weight: 400; font-size: 16px; color: rgb(102, 102, 102); line-height: 1.3; font-style: normal; text-align: center; background-color: transparent; border-radius: 0px; border-width: 0px; border-color: rgb(255, 255, 255); border-style: none; padding: 0px;\"><div class=\"css-vsf5of\"><p style=\"text-align:left;\" class=\"carina-rte-public-DraftStyleDefault-block\"><span style=\"color: rgb(102,102,102);\"><span style=\"font-weight: bold;\">Booth, Rides and Dog Houses</span></span></p><ul class=\"carina-rte-public-DraftStyleDefault-ul\"><li style=\"text-align:left;\"><span style=\"color: rgb(102,102,102);\">Thursday: 3:30-11 p.m.</span></li><li style=\"text-align:left;\"><span style=\"color: rgb(102,102,102);\">Friday &amp; Saturday: 11 a.m.-11 p.m.&nbsp; </span></li></ul><p style=\"text-align:left;\" class=\"carina-rte-public-DraftStyleDefault-block\"><span style=\"color: rgb(102,102,102);\"><span style=\"font-weight: bold;\"><br>Carnival Headquarters Tent <br></span><span style=\"font-style: italic;\">Check-In &amp; Registration</span></span></p><ul class=\"carina-rte-public-DraftStyleDefault-ul\"><li style=\"text-align:left;\"><span style=\"color: rgb(102,102,102);\">Thursday: 8 a.m.-7 p.m.</span></li><li style=\"text-align:left;\"><span style=\"color: rgb(102,102,102);\">Friday: 8 a.m.-7 p.m.</span></li><li style=\"text-align:left;\"><span style=\"color: rgb(102,102,102);\">Saturday: 8 a.m.-7 p.m.</span></li></ul></div></div></div></div></div><div class=\"Grid__column___L6Qbn Grid__col_4___eGcOB Grid__small___joj2e\"><div id=\"widget:c8425b98-71bd-4a7c-9202-737a3d08aa06\" class=\"App__widgetIdcontainer___ms2gn\"><div data-cvent-id=\"widget-NucleusImage-widget:c8425b98-71bd-4a7c-9202-737a3d08aa06\" class=\"Image__container___bkibo\" style=\"border-width: 0px; border-color: rgb(255, 255, 255); border-style: none; padding: 15px; background-color: transparent; border-radius: 0px; flex: 1 1 auto; justify-content: center;\"><div class=\"Image__image___K4qxU\" style=\"text-align: center; font-family: Arial, Helvetica, sans-serif; font-weight: 400; font-style: normal;\"><div class=\"Image__imageWrapper___yrxys\"><img alt=\"\" srcset=\"https://images.cvent.com/3acb589e68044db09aa08f6a32c3e88e/pix/07114a9d814e406fac771607eaa45b20!_!17653e632bb8ab2cb04369b6a1f4eabe.png?d=320&amp;f=webp 320w,\n",
    "#         https://images.cvent.com/3acb589e68044db09aa08f6a32c3e88e/pix/07114a9d814e406fac771607eaa45b20!_!17653e632bb8ab2cb04369b6a1f4eabe.png?d=480&amp;f=webp 480w,\n",
    "#         https://images.cvent.com/3acb589e68044db09aa08f6a32c3e88e/pix/07114a9d814e406fac771607eaa45b20!_!17653e632bb8ab2cb04369b6a1f4eabe.png?f=webp 600w\" src=\"https://images.cvent.com/3acb589e68044db09aa08f6a32c3e88e/pix/07114a9d814e406fac771607eaa45b20!_!17653e632bb8ab2cb04369b6a1f4eabe.png?f=webp\" style=\"max-width: 600px;\"></div></div></div></div></div><div class=\"Grid__column___L6Qbn Grid__col_4___eGcOB Grid__small___joj2e\"><div id=\"widget:d12b93e6-edf2-46a5-89ad-0014743ed8e2\" class=\"App__widgetIdcontainer___ms2gn\"><div data-cvent-id=\"widget-NucleusText-widget:d12b93e6-edf2-46a5-89ad-0014743ed8e2\" class=\"Text__container___Z6gol\" style=\"border-width: 0px; border-color: rgb(255, 255, 255); border-style: none; padding: 15px; background-color: transparent; border-radius: 0px; flex: 1 1 auto;\"><div class=\"\" style=\"font-family: &quot;Open Sans&quot;, Helvetica, Arial, sans-serif; font-weight: 400; font-size: 16px; color: rgb(102, 102, 102); line-height: 1.3; font-style: normal; text-align: center; background-color: transparent; border-radius: 0px; border-width: 0px; border-color: rgb(255, 255, 255); border-style: none; padding: 0px;\"><div class=\"css-vsf5of\"><p style=\"text-align:left;\" class=\"carina-rte-public-DraftStyleDefault-block\"><span style=\"color: rgb(102,102,102);\"><span style=\"font-weight: bold;\">Buggy Races and Donut Tent</span></span></p><ul class=\"carina-rte-public-DraftStyleDefault-ul\"><li style=\"text-align:left;\"><span style=\"color: rgb(102,102,102);\">Friday's Preliminary Sweepstakes Race: 8 a.m.-Noon</span></li><li style=\"text-align:left;\"><span style=\"color: rgb(102,102,102);\">Saturday's Final Sweepstakes Race: 8 a.m.-Noon</span></li></ul><p style=\"text-align:left;\" class=\"carina-rte-public-DraftStyleDefault-block\"><br></p><p style=\"text-align:left;\" class=\"carina-rte-public-DraftStyleDefault-block\"><span style=\"color: rgb(102,102,102);\"><span style=\"font-weight: bold;\">Scotch'n'Soda Performance of </span><span style=\"font-style: italic;\"><span style=\"font-weight: bold;\">The Little Mermaid</span></span></span></p><ul class=\"carina-rte-public-DraftStyleDefault-ul\"><li style=\"text-align:left;\"><span style=\"color: rgb(102,102,102);\">Thursday: 7-9 p.m.</span></li><li style=\"text-align:left;\"><span style=\"color: rgb(102,102,102);\">Friday: 7-9 p.m. and 11 p.m.-1 a.m.</span></li><li style=\"text-align:left;\"><span style=\"color: rgb(102,102,102);\">Saturday: 3-5 p.m. and 7-9 p.m.</span></li></ul></div></div></div></div></div></div>\n",
    "\n",
    "# driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## History at CMU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 25 great things SCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info_h(url, headers):\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = bs(response.content, 'html.parser')\n",
    "\n",
    "    target_data = []\n",
    "    # find the h1\n",
    "    h1 = soup.find('h1')\n",
    "    head = h1.get_text(separator=\" \", strip=True)\n",
    "    target_data.append(head)\n",
    "\n",
    "    target_divs = soup.find('div', class_='field')\n",
    "    target_divs_div = target_divs.find('div', class_='collapse-text-text')\n",
    "    target_data.append(target_divs_div.get_text(separator=\" \", strip=True))\n",
    "    great_things_25 = target_divs.find_all('fieldset')\n",
    "    target_data += [ head + ': ' + div.get_text(separator=\" \", strip=True) for div in great_things_25]\n",
    "    \n",
    "    return target_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a txt file to store the data\n",
    "data_folder = '../../data/'\n",
    "filename = 'history_25_great_things'\n",
    "with open(data_folder + filename + '.txt', 'w') as f:\n",
    "    f.write('')\n",
    "\n",
    "\n",
    "url = 'https://www.cs.cmu.edu/scs25/25things'\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36'\n",
    "}\n",
    "target_data = extract_info_h(url, headers)\n",
    "target_data = [data.replace(u'\\xa0', r' ') for data in target_data]\n",
    "\n",
    "for data in target_data:\n",
    "    with open(data_folder + filename + '.txt', 'a') as f:\n",
    "        f.write(data + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### history of SCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info_h2(url, headers):\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = bs(response.content, 'html.parser')\n",
    "\n",
    "    target_data = []\n",
    "    # find the h1\n",
    "    h1 = soup.find('h1')\n",
    "    head = h1.get_text(separator=\" \", strip=True)\n",
    "    # target_data.append(head)\n",
    "\n",
    "    target_divs = soup.find('div', class_='field')\n",
    "\n",
    "    first_h2 = target_divs.find('h2')\n",
    "    # Find all preceding siblings of the first h2 tag\n",
    "    preceding_siblings = first_h2.find_all_previous()\n",
    "    # Extract text content of preceding siblings\n",
    "    text_before_h2 = ''.join([sibling.get_text(separator=\" \", strip=True) for sibling in preceding_siblings if sibling.name == 'p'])\n",
    "\n",
    "    target_data.append(head + ': ' + text_before_h2)\n",
    "    \n",
    "    # Find all h2 tags\n",
    "    h2_tags = target_divs.find_all('h2')\n",
    "    # Extract h2 and corresponding p tags\n",
    "    for h2_tag in h2_tags:\n",
    "        paragraph = head + ': ' + h2_tag.get_text(separator=\" \", strip=True)\n",
    "        # Find the next sibling p tags until the next h2 tag\n",
    "        next_element = h2_tag.find_next_sibling()\n",
    "        while next_element and next_element.name != 'h2':\n",
    "            if next_element.name == 'p':\n",
    "                paragraph += next_element.get_text(separator=\" \", strip=True)\n",
    "            next_element = next_element.find_next_sibling()\n",
    "        target_data.append(paragraph)\n",
    "\n",
    "    \n",
    "    return target_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a txt file to store the data\n",
    "data_folder = '../../data/'\n",
    "filename = 'history_of_scs'\n",
    "with open(data_folder + filename + '.txt', 'w') as f:\n",
    "    f.write('')\n",
    "\n",
    "url = 'https://www.cs.cmu.edu/scs25/history'\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36'\n",
    "}\n",
    "target_data = extract_info_h2(url, headers)\n",
    "target_data = [data.replace(u'\\xa0', r' ') for data in target_data]\n",
    "\n",
    "for data in target_data:\n",
    "    with open(data_folder + filename + '.txt', 'a') as f:\n",
    "        f.write(data + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### history CMU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info_h3(url, headers):\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = bs(response.content, 'html.parser')\n",
    "\n",
    "    target_data = []\n",
    "    # find the title\n",
    "    title = soup.title.string\n",
    "    # remove extra spaces\n",
    "    title = ' '.join(title.split())\n",
    "\n",
    "    target_divs = soup.select('div[class^=\"grid column2\"]') #soup.find('div', class_='grid column2')\n",
    "    target_data += [title + ': ' + div.get_text(separator=\" \", strip=True) for div in target_divs]\n",
    "    # target_divs_div = target_divs.find('div', class_='collapse-text-text')\n",
    "\n",
    "    \n",
    "    return target_data, target_divs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a txt file to store the data\n",
    "data_folder = '../../data/'\n",
    "filename = 'history_of_cmu'\n",
    "with open(data_folder + filename + '.txt', 'w') as f:\n",
    "    f.write('')\n",
    "\n",
    "url = 'https://www.cmu.edu/about/history.html'\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36'\n",
    "}\n",
    "target_data, divs = extract_info_h3(url, headers)\n",
    "target_data = [data.replace(u'\\xa0', r' ') for data in target_data]\n",
    "\n",
    "for data in target_data:\n",
    "    with open(data_folder + filename + '.txt', 'a') as f:\n",
    "        f.write(data + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### buggy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_info_b(url, headers):\n",
    "#     response = requests.get(url, headers=headers)\n",
    "#     soup = bs(response.content, 'html.parser')\n",
    "\n",
    "#     target_data = []\n",
    "#     # find the title\n",
    "#     title = soup.title.string\n",
    "#     # remove extra spaces\n",
    "#     title = ' '.join(title.split())\n",
    "\n",
    "#     target_divs = soup.find('div', class_='content')\n",
    "#     target_data += [div.get_text(separator=\" \", strip=True) for div in target_divs]\n",
    "#     # target_divs_div = target_divs.find('div', class_='collapse-text-text')\n",
    "\n",
    "    \n",
    "#     return target_data, target_divs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create a txt file to store the data\n",
    "# data_folder = '../../data/'\n",
    "# filename = 'buggy'\n",
    "# with open(data_folder + filename + '.txt', 'w') as f:\n",
    "#     f.write('')\n",
    "    \n",
    "# url = 'https://www.cmu.edu/news/stories/archives/2019/april/spring-carnival-buggy.html'\n",
    "# headers = {\n",
    "#     'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36'\n",
    "# }\n",
    "# target_data, divs = extract_info_b(url, headers)\n",
    "# target_data = [data.replace(u'\\xa0', r' ') for data in target_data]\n",
    "\n",
    "# for data in target_data:\n",
    "#     with open(data_folder + filename + '.txt', 'a') as f:\n",
    "#         f.write(data + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Athletics - Tartans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info_a(url, headers):\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = bs(response.content, 'html.parser')\n",
    "\n",
    "    target_data = []\n",
    "    # find the title\n",
    "    title = soup.title.string\n",
    "    # remove extra spaces\n",
    "    title = ' '.join(title.split())\n",
    "    print(title)\n",
    "\n",
    "    target_div = soup.find('div', class_='article-text')\n",
    "    # target_data += [title + ': ' + div.get_text(separator=\" \", strip=True) for div in target_divs]\n",
    "\n",
    "    contents = target_div.find_all('p', recursive=True)  # Only immediate children\n",
    "    \n",
    "    for content in contents:\n",
    "        if content.strong:\n",
    "            # Print the subtitle text and its corresponding content\n",
    "            subtitle = content.get_text(separator=\" \", strip=True)\n",
    "            # print(\"Subtitle:\", content.text)\n",
    "            # Find the following sibling p elements until the next subtitle\n",
    "            next_sibling = content.find_next_sibling()\n",
    "            content = ' '\n",
    "            while next_sibling and not next_sibling.strong:\n",
    "                content += next_sibling.get_text(separator=\" \", strip=True)\n",
    "                # print(\"\\t\", next_sibling.text.strip())\n",
    "                next_sibling = next_sibling.find_next_sibling()\n",
    "            target_data.append(title + ': ' + subtitle + content)\n",
    "\n",
    "    \n",
    "    return target_data, contents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tartan Facts - Carnegie Mellon University Athletics\n"
     ]
    }
   ],
   "source": [
    "# create a txt file to store the data\n",
    "data_folder = '../../data/'\n",
    "filename = 'athletics'\n",
    "with open(data_folder + filename + '.txt', 'w') as f:\n",
    "    f.write('')\n",
    "    \n",
    "url = 'https://athletics.cmu.edu/athletics/tartanfacts'\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36'\n",
    "}\n",
    "target_data, _ = extract_info_a(url, headers)\n",
    "target_data = [data.replace(u'\\xa0', r' ') for data in target_data]\n",
    "\n",
    "for data in target_data:\n",
    "    with open(data_folder + filename + '.txt', 'a') as f:\n",
    "        f.write(data + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Athletics - Scotty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info_a2(url, headers):\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = bs(response.content, 'html.parser')\n",
    "\n",
    "    target_data = []\n",
    "    # find the h2\n",
    "    h2 = soup.find('h2')\n",
    "    head = h2.get_text(separator=\" \", strip=True)\n",
    "    # target_data.append(head)\n",
    "\n",
    "    target_divs = soup.select('div[class^=\"content\"]')\n",
    "    target_data += [head + ': ' + div.get_text(separator=\" \", strip=True) for div in target_divs]\n",
    "    \n",
    "    return target_data, target_divs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## write to the same file\n",
    "# # create a txt file to store the data\n",
    "# data_folder = '../../data/'\n",
    "# filename = 'athletics'\n",
    "# with open(data_folder + filename + '.txt', 'w') as f:\n",
    "#     f.write('')\n",
    "    \n",
    "url = 'https://athletics.cmu.edu/athletics/mascot/about'\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36'\n",
    "}\n",
    "target_data, _ = extract_info_a2(url, headers)\n",
    "target_data = [data.replace(u'\\xa0', r' ') for data in target_data]\n",
    "\n",
    "for data in target_data:\n",
    "    with open(data_folder + filename + '.txt', 'a') as f:\n",
    "        f.write(data + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Athletics - Kiltie Band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info_a3(url, headers):\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = bs(response.content, 'html.parser')\n",
    "\n",
    "    target_data = []\n",
    "    # find the h1\n",
    "    h1 = soup.find('h1')\n",
    "    head = h1.get_text(separator=\" \", strip=True)\n",
    "    # target_data.append(head)\n",
    "\n",
    "    target_divs = soup.find('table')\n",
    "    target_data.append(head + ': ' + target_divs.get_text(separator=\" \", strip=True))\n",
    "\n",
    "    \n",
    "    return target_data, target_divs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## write to the same file\n",
    "# # create a txt file to store the data\n",
    "# data_folder = '../../data/'\n",
    "# filename = 'athletics'\n",
    "# with open(data_folder + filename + '.txt', 'w') as f:\n",
    "#     f.write('')\n",
    "    \n",
    "url = 'https://athletics.cmu.edu/athletics/kiltieband/index'\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36'\n",
    "}\n",
    "target_data, target_divs = extract_info_a3(url, headers)\n",
    "target_data = [data.replace(u'\\xa0', r' ') for data in target_data]\n",
    "\n",
    "for data in target_data:\n",
    "    with open(data_folder + filename + '.txt', 'a') as f:\n",
    "        f.write(data + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.7 ('llama_hw': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "8fd8091be585587e5f645e0ce439e38696b2a01426dc385fc69b6172a172d49d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
