{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LTI Faculty Data\n",
    "[https://lti.cs.cmu.edu/directory/all/154/1](https://lti.cs.cmu.edu/directory/all/154/1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()  \n",
    "urls = [\"https://www.lti.cmu.edu/directory/all/154/1\", \"https://www.lti.cmu.edu/directory/all/154/1?page=1\"]\n",
    "\n",
    "scrape_data = []\n",
    "\n",
    "# create a txt file to store the data\n",
    "data_folder = '../../data/'\n",
    "filename = 'lti_department_data'\n",
    "with open(data_folder + filename + '.txt', 'w') as f:\n",
    "    f.write('')\n",
    "\n",
    "def combine_faculty_info_into_paragraph(name, title, email, office, phone, research_areas):\n",
    "    res = name + 'is a ' + title + 'a at LTI. '\n",
    "    if email != 'N/A':\n",
    "        res += 'Their email is ' + email + '. '\n",
    "    if office != 'N/A':\n",
    "        res += 'Their office is located at ' + office + '. '\n",
    "    if phone != 'N/A':\n",
    "        res += 'You can reach ' + name + ' at ' + phone + '. '\n",
    "    if research_areas != 'N/A':\n",
    "        res += 'Their research areas include ' + research_areas + '.'\n",
    "    res += '\\n'\n",
    "    return res\n",
    "\n",
    "# Iterate through each URL and scrape the data\n",
    "for url in urls:\n",
    "    driver.get(url)\n",
    "    html = driver.page_source\n",
    "    soup = bs(html, 'html.parser')\n",
    "\n",
    "    table = soup.find('table', class_='views-view-grid')\n",
    "    rows = table.find_all('tr')\n",
    "\n",
    "    for row in rows:\n",
    "        cells = row.find_all('td')\n",
    "        row_data = []\n",
    "        for cell in cells:\n",
    "            # Extracting info\n",
    "            # impute N/A if there is no data for a particular field\n",
    "            name_element = cell.find('h2')\n",
    "            if not name_element:\n",
    "                continue\n",
    "            name = name_element.text.strip()\n",
    "            \n",
    "            title_element = cell.find('div', class_='views-field-field-computed-prof-title')\n",
    "            title = title_element.text.strip() if title_element else \"N/A\"\n",
    "            \n",
    "            email_element = cell.find('a', href=lambda href: href and 'mailto' in href)\n",
    "            email = email_element.text.strip() if email_element else \"N/A\"\n",
    "            \n",
    "            office_element = cell.find('div', class_='views-field-field-computed-building')\n",
    "            office = office_element.text.strip().split(':')[1].strip() if office_element else \"N/A\"\n",
    "            \n",
    "            phone_element = cell.find('div', class_='views-field-field-computed-phone')\n",
    "            phone = phone_element.text.strip().split(':')[1].strip() if phone_element else \"N/A\"\n",
    "            \n",
    "            research_areas_element = cell.find('div', class_='views-field-field-research-areas')\n",
    "            research_areas = research_areas_element.text.strip().split(':')[1].strip() if research_areas_element else \"N/A\"\n",
    "\n",
    "            # construct a paragraph for each faculty member\n",
    "            row_paragraph = combine_faculty_info_into_paragraph(name, title, email, office, phone, research_areas)\n",
    "            # write to txt file\n",
    "            with open(data_folder + filename + '.txt', 'a') as f:\n",
    "                f.write(row_paragraph)\n",
    "\n",
    "            # Append the data to the list to form a csv output later\n",
    "            row_data.append({\n",
    "                'Name': name,\n",
    "                'Title': title,\n",
    "                'Email': email,\n",
    "                'Office': office,\n",
    "                'Phone': phone,\n",
    "                'Research Areas': research_areas\n",
    "            })\n",
    "        scrape_data.extend(row_data)\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "df = pd.DataFrame(scrape_data)\n",
    "df.to_csv(data_folder + filename + '.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CMU Commencement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pages = ['schedule', 'graduates', 'visitors', 'faq', 'contact-us']\n",
    "# base_url = 'https://www.cmu.edu/commencement/'\n",
    "\n",
    "# def extract_info(url):\n",
    "#     response = requests.get(url)\n",
    "#     soup = bs(response.content, 'html.parser')\n",
    "    \n",
    "#     content_divs = soup.find_all(\"div\", class_=\"content\")\n",
    "#     content_data = [div.get_text(separator=\"\\n\", strip=True) for div in content_divs]\n",
    "    \n",
    "#     grid_div = soup.find(\"div\", class_=\"grid\")\n",
    "#     grid_data = grid_div.get_text(separator=\"\\n\", strip=True) if grid_div else \"\"\n",
    "    \n",
    "#     return content_data, grid_data\n",
    "\n",
    "# # Save each page scraped data\n",
    "# for page in pages:\n",
    "#     url = base_url + page + '/index.html'\n",
    "#     content_data, grid_data = extract_info(url)   \n",
    "#     scrape_content = pd.DataFrame({\"content\": content_data})    \n",
    "#     scrape_grid = pd.DataFrame({\"grid\": [grid_data]})\n",
    "    \n",
    "#     scrape = pd.concat([scrape_content, scrape_grid], axis=1)\n",
    "#     # scrape.to_csv('commencement/' + page + '_data.csv', index=False)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### commencement schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info(url):\n",
    "    response = requests.get(url)\n",
    "    soup = bs(response.content, 'html.parser')\n",
    "\n",
    "    # content_grid_divs = soup.find_all('div', class_=[\"content\", \"grid\"])\n",
    "    target_divs = soup.find_all('div', class_=lambda x: x and (x == \"content\" or x == \"grid\" and x != \"sidebar\"))\n",
    "    content_grid_data = [div.get_text(separator=\" \", strip=True) for div in target_divs]\n",
    "    \n",
    "    return content_grid_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a txt file to store the data\n",
    "data_folder = '../../data/'\n",
    "filename = 'commencement_schedule_data'\n",
    "with open(data_folder + filename + '.txt', 'w') as f:\n",
    "    f.write('')\n",
    "\n",
    "\n",
    "############ commencement index page ############\n",
    "url = 'https://www.cmu.edu/commencement/index.html'\n",
    "content_grid_data = extract_info(url)\n",
    "content_grid_data = [data.replace(u'\\xa0', r' ') for data in content_grid_data]\n",
    "\n",
    "for data in content_grid_data:\n",
    "    with open(data_folder + filename + '.txt', 'a') as f:\n",
    "        f.write(data + '\\n')\n",
    "\n",
    "\n",
    "############ schedule index page ############\n",
    "url = 'https://www.cmu.edu/commencement/schedule/index.html'\n",
    "content_grid_data = extract_info(url)\n",
    "content_grid_data = [data.replace(u'\\xa0', r' ') for data in content_grid_data]\n",
    "\n",
    "for data in content_grid_data:\n",
    "    with open(data_folder + filename + '.txt', 'a') as f:\n",
    "        f.write(data + '\\n')\n",
    "\n",
    "\n",
    "############## main ceremony ############\n",
    "# Main Commencement Ceremony\n",
    "# 2023 Honorary Degree Recipients\n",
    "# Student Speaker\n",
    "url = 'https://www.cmu.edu/commencement/schedule/main-ceremony.html'\n",
    "content_grid_data = extract_info(url)\n",
    "content_grid_data = [data.replace(u'\\xa0', r' ') for data in content_grid_data]\n",
    "\n",
    "# combine the honorable degree recipients into one paragraph\n",
    "honorary_degree_recipients = content_grid_data[1:3]\n",
    "honorary_degree_recipients = ' '.join(honorary_degree_recipients)\n",
    "content_grid_data = [content_grid_data[0], honorary_degree_recipients] + content_grid_data[3:]\n",
    "\n",
    "for data in content_grid_data:\n",
    "    with open(data_folder + filename + '.txt', 'a') as f:\n",
    "        f.write(data + '\\n')\n",
    "\n",
    "\n",
    "############# Diploma Ceremonies page ############\n",
    "url = 'https://www.cmu.edu/commencement/schedule/diploma-ceremonies.html'\n",
    "response = requests.get(url)\n",
    "soup = bs(response.content, 'html.parser')\n",
    "target_divs = soup.find_all('div', class_=\"content\")\n",
    "content_data = [div.get_text(separator=\" \", strip=True) for div in target_divs]\n",
    "### find the grid div: contains schedule for each diploma ceremony\n",
    "grid_div = soup.find(\"div\", class_=\"grid\")\n",
    "# find all div within grid_div\n",
    "div_in_grid_div = grid_div.find_all('div')\n",
    "# extract the text from each div\n",
    "grid_data = [div.get_text(separator=\" \", strip=True) for div in div_in_grid_div]\n",
    "# append \"Diploma Ceremonies schedule for \" to the beginning of the grid_data\n",
    "grid_data = ['Diploma Ceremonies schedule for '+data for data in grid_data]\n",
    "for data in content_data + grid_data:\n",
    "    with open(data_folder + filename + '.txt', 'a') as f:\n",
    "        f.write(data + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
