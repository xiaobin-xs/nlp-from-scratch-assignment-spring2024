IMPROVING MASSIVELY MULTILINGUAL ASR WITH AUXILIARY CTC OBJECTIVES
William Chen, Brian Yan, Jiatong Shi, Yifan Peng, Soumi Maiti, Shinji Watanabe
Language Technologies Institute, Carnegie Mellon University, USA
ABSTRACT
Multilingual Automatic Speech Recognition (ASR) models have
extended the usability of speech technologies to a wide variety of
languages. With how many languages these models have to handle,
however, a key to understanding their imbalanced performance across
different languages is to examine if the model actually knows which
language it should transcribe. In this paper, we introduce our work
on improving performance on FLEURS, a 102-language open ASR
benchmark, by conditioning the entire model on language identity
(LID). We investigate techniques inspired from recent Connectionist
Temporal Classiﬁcation (CTC) studies to help the model handle the
large number of languages, conditioning on the LID predictions of
auxiliary tasks. Our experimental results demonstrate the effective-
ness of our technique over standard CTC/Attention-based hybrid mod-
els. Furthermore, our state-of-the-art systems using self-supervised
models with the Conformer architecture improve over the results of
prior work on FLEURS by a relative 28.4% CER. Trained models
and reproducible recipes are available at https://github.com/
espnet/espnet/tree/master/egs2/fleurs/asr1 .
Index Terms —multilingual ASR, low-resource ASR, CTC
1. INTRODUCTION
Recent advancements in multilingual speech processing have shown
great promise towards building speech systems for all, expanding
language coverage beyond the high-resources [1–16]. In particular,
practitioners have demonstrated that neural techniques are capable of
large-scale cross-lingual representation learning by training multilin-
gual automatic speech recognition (ASR) systems on massive private
[8, 10, 14, 16] or public speech corpora [2, 17–21]; however, these
works demonstrate that performance still varies across languages.
One of the inherent challenges in building a single system which
can recognize many languages is the vast variability of phonology,
grammar, and written scripts [22]. Therefore, a key to understanding
why multilingual ASR systems exhibit certain errors is to examine
whether the underlying model actually knows which language it
should be transcribing – in other words, if there was a correct language
identiﬁcation (LID). To this end, systems that jointly model LID and
ASR via multi-tasking [6, 16, 23] offer one view to the inner-workings
of the multilingual decision process. However, we are interested in
frameworks which more explicitly model LID as a dependency for
ASR under the presumption that knowing the correct language of an
utterance makes it easier to be transcribed.
Therefore, in this work, we seek to build massively multilingual
models which 1) condition transcription predictions on language
identity likelihoods and 2) contribute our reproducible models and
recipes which use publicly available data, with the broader objective
of improving explain-ability and use-ability.
To achieve this, our work focuses on the FLEURS ASR
dataset [21]. FLEURS contains 102 languages from across theglobe, the typological diversity of which makes language identiﬁca-
tion a relevant component of transcription. Languages in FLEURS
are also individually low-resourced: each language has only 7-10
hours of training data. This makes FLEURS a unique challenge
that can help ASR progress to the long-tail of the world’s ∼7000
languages. We apply self-conditioned CTC [24–32], which uses
Connectionist Temporal Classiﬁcation (CTC) models in intermediate
encoder layers to condition subsequent layers on intermediate pre-
dictions, to Hybrid CTC/Attention architectures [23] as a basis for
our LID conditioning approach. We then design intermediate LID
targets of varying granularity and use these to examine the effect
of conditioning our encoder-decoder models on LID predictions
starting from early layers of the encoder. Our proposed method,
which allows early encoder layers to focus on LID while subsequent
encoder and decoder layers focus on ASR, is beneﬁcial compared
to standard self-conditioning. Together with self-supervised models
and Conformer architectures, our state-of-the-art (SOTA) systems
obtained 10.1 CER on FLEURS, a relative 28.4% reduction over
prior work.
2. BACKGROUND
In this section, we discuss the CTC studies that we build upon to cre-
ate our fully LID-conditioned model. These studies propose different
multi-task training methods [23, 24] and condition the encoder on
intermediate predictions [27, 29].
2.1. Hybrid CTC/Attention
We use a Hybrid CTC/Attention architecture [23] as our model
foundation. Let X= (xt∈RD|t= 1,...,T )be aT-length
input sequence based on D-dimensional acoustic feature xtand
Y= (ys∈V|s= 1,...,S )be anS-length output sequence with
vocabularyV. CTC [33] optimizes a model to predict the monotonic
alignment between XandY. It models the conditional probability of
Pctc(Y|X)as a series of latent sequences at each input frame. This
latent sequence is obtained by introducing a blank token ∅intoY,
such thatZctc= (zctc
t∈V∪{ ∅}|t= 1,...,T ).
PCTC(Y|X)≈∑
ZCTC∈F−1(Y)T∏
t=1PCTC(zCTC
t|X,z1:t−1) (1)
WhereF−1is the function of all latent sequences ZctcgivenY. CTC
operates with the assumption that only the observation Xis required
to determine the latent state zctc
tat any given frame. The Hybrid
CTC/Attention encoder ﬁrst converts input Xinto the hidden vector
hin Equation (2). The hidden vector is then used to obtain the frame-
wise CTC posterior (Equation 3) and token-wise attention posterior
distributions of X(Equation 4).
h=ENC(X) (2)
PCTC(Z|X) =CTC(h) (3)
PAttn(yl|X,y 1:l−1) =DEC(h,y1:l−1) (4)arXiv:2302.12829v2  [cs.CL]  27 Feb 2023
Combining Equations (1, 3, 4) obtains the logarithmic linear combi-
nation of these posterior distributions over all frames and decoded
tokens used to optimize the encoder-decoder network:
L=−(1−λ) logPAttn(Y|X)−λlogPCTC(Y|X) (5)
Whereλis the CTC weighting term. Hybrid CTC/Attention thus
jointly optimizes a shared encoder with both CTC and attention losses,
while the decoder is trained purely on the attention loss.
2.2. Intermediate CTC
InterCTC [24–26] was proposed to regularize the training of deep
encoder networks by using the CTC loss of intermediate layers as
part of a multi-task objective. The intermediate posterior distribution
can be obtained in a manner similar to Equation (3), with the hidden
vector hintof an intermediate encoder layer.
Pint
CTC(Zint|X) =CTCint(hint) (6)
The log-likelihood of Equation (6) can then be used as the objective
function of the intermediate layer. Self-conditioned CTC (SC-CTC)
[27] also uses this intermediate output to condition the encoder by
passing intermediate outputs to the next layer. The normalized hidden
representation of the intermediate layer hintis summed with a linear
projection of the intermediate posterior distribution Zintto the hidden
dimension, and input into the next encoder layer (Equation 7).
h=ENCﬁn(NRM(hint) +LIN(Zint))) (7)
PCTC(Z|X,Zint) =CTC(h) (8)
Equation (7) is recursively applied for each intermediate layer,
until the output of the ﬁnal layer is passed into Equation (8). This
allows the CTC posterior distribution of the entire encoder network
to be conditioned on the intermediate predictions Zint.
3. PROPOSED METHOD
In this section, we propose an auxiliary CTC objective such that
early encoder layers can focus on language identiﬁcation, and the
transcription predictions of later layers can be conditioned on the
predicted language. We also deﬁne a hierarchy of CTC objectives to
take advantage of both LID and self-conditioning, and frame it within
a Hybrid CTC/Attention setup.
3.1. Explicit multilingual conditioning
While SC-CTC’s conditioning on early predictions beneﬁts non-
autoregressive models [27, 30], it could be a drawback in the auto-
regressive setting by conditioning later encoder layers on the noisy
transcription predictions of earlier layers. We want to condition en-
coder layers on the LID without these noisy early predictions. To
accomplish this, we propose the following method: train intermediate
layers to only classify the spoken language and propagate their pre-
dictions to future layers via self-conditioning. We ﬁrst introduce the
latent variable Ito Equation (1), where Irepresents the intermediate
LID predictions, such that Equation (1) is modiﬁed as follows:
PCTC(Y|X) =∑
I∈IPCTC(Y|I,X)PCTC(I|X) (9)
The formulation of Equation (9) can be realized by modifying the
intermediate CTC network (Equations 7 and 8) to predict the LID
instead of transcriptions as follows:
h=ENCﬁn(NRM(hlid) +LIN(Zlid))) (10)Table 1 . Comparison of different labels in the multi-task framework
(Sec. 3.1). In LID tok, all tokens are replaced with LIDs, while LID utt
only retains a single LID label.
Task Label
ASR [EN US] ALL YOU NEED
LID tok [EN US] [EN US] [EN US] [EN US]
LID utt [EN US]
Fig. 1 . Proposed hierarchical architecture. The LID predictions of
the intermediate layer are used to train the next layer.
PCTC(Z|X,Zlid) =CTC(h) (11)
This allows the encoder to condition its predictions on the LID. We
then deﬁne an auxiliary CTC task with Equations (1) and (9), where
the model’s intermediate layers attempt to predict the language, I.
Llid=−logPCTC(I|X) (12)
We then create two different sets of labels that can represent I:
utterance-level LIDs (LID utt) and token-level LIDs (LID tok). An
utterance-level LID is a single label from the set of all possible lan-
guagesiutt∈B. In other words, only a single LID token is used as
the ground truth. Alternatively, we can deﬁne a S-length token-level
LID sequence corresponding to each S-length label sequence as fol-
lows:Itok={itok
s∈B|s= 1,...,S}. LID tokthus explicitly aligns
the language with each spoken word. This approach, inspired by
code-switching [34], forces the model to predict both the frame-level
alignment and segmentation between tokens. The task effectively
becomes one of identifying the language of each token rather than
each utterance. We hypothesize this will aid the model in mapping
the audio to the large multilingual text space, even without any code-
switched utterances. Example labels can be found in Table (1).
3.2. Hierarchical conditioning
Explicitly training all intermediate layers on LID allows the model
to condition on language information, but perhaps early layers may
be sufﬁcient to predict LID, allowing later layers to predict inter-
mediate transcripts instead. This progression can be realized using
hierarchical conditioning [26, 29, 35], where layers perform incre-
mentally more complex predictions. We construct a hierarchical setup
ofKintermediate layers, such that the k= 1intermediate layer is
trained using Equation (12) to predict the spoken language (Figure
1). The auxiliary LID task is given to an earlier intermediate layer,
such that following encoder layers can be conditioned on its predic-
tion. Later intermediate layers are trained with SC-CTC to keep the
regularization beneﬁts.
hint=ENCint(NRM(hlid) +LIN(Zlid))) (13)
h=ENCﬁn(NRM(hint) +LIN(Zint))) (14)
PCTC(Z|X,Zlid) =CTC(h) (15)
The encoder output his therefore both self-conditioned and LID-
conditioned. Equation 12 can be summed with the loss of all SC-CTC
layers, which is then averaged to produce the objective function used
to train the intermediate layers:
Lhier=1
K(Llid+K∑
k=2Linter
k) (16)
WhereLinteris the negative log-likelihood of Equation (6), the poste-
rior CTC distribution of an intermediate layer. The overall CTC loss
can then be obtained in Equation (17) with a weighted sum of the
hierarchical loss (Equation 16) with the CTC loss of the full encoder
network, where wis the weight of the intermediate losses.
LCTC= (1−w)LCTC
enc+wLhier(17)
Substituting Equation (17) into Equation (5) yields the complete loss
function used to train our encoder-decoder network with both LID
conditioning and SC-CTC:
L= (1−λ)Latt+λ((1−w)LCTC
enc+wLhier) (18)
Speciﬁcally, our model jointly optimizes the encoder with the LID-
conditioned intermediate loss, the CTC loss, and the encoder-decoder
attention loss. The decoder is conditioned on the prepended LID
token and optimized with the attention loss (Figure 1).
4. EXPERIMENTS
4.1. Datasets
As discussed in Sec. 1, the experiments were conducted on FLEURS
[21], a 102-language ASR dataset. Each utterance is a news snippet
read by a speaker and contains only one language. Each language in
FLEURS has around 7-10 hours of training data, for a total of 987
training hours. Due to the limited amount of supervised data for each
language, we experimented with two pre-trained Self-Supervised
Learning (SSL) features that performed well on SUPERB [36]: XLS-
R [19] and WavLM [37]. The acoustic inputs are augmented by
SpecAugment [38] and speech perturbation [39]. Input text was
prepended by language identiﬁcation tokens, before tokenization by
SentencePiece [40] with a vocabulary size of 6500.
4.2. Model Conﬁguration
All experiments were conducted through ESPnet2 [41]. We use an
encoder-decoder setup trained on the hybrid CTC/Attention [23]
multi-task objective, with a CTC weight of λ= 0.3. We experiment
with both Transformer [42] and Conformer [43] architectures as
the encoder. The encoder has either 18 Transformer layers or 12Table 2 . Comparing the effectiveness of SSL features, reporting CER,
MER, LID % accuracy on FLEURS. XLS-R signiﬁcantly outperforms
WavLM in multilingual ASR.
ID Model SSL Features Test
CER(↓) MER(↓) LID(↑)
Transformer
A1 CTC/Attention WavLM 14.6 41.8 95.09
A2 +SC-CTC WavLM 14.4 40.8 94.47
B1 CTC/Attention XLS-R 13.9 39.7 95.73
B2 +SC-CTC XLS-R 13.7 38.8 95.39
Conformer layers. Each encoder layer has 8 attention heads and 2048
hidden units. The 6-layer Transformer decoder also has 8 attention
heads and 2048 hidden units each. We average 3 checkpoints with
the highest validation accuracy. We perform joint CTC/Attention
decoding with a language model, using a beam size of 10 and CTC
weight of 0.3. Model parameters totaled to around 102 million.
4.2.1. Baseline Models
CTC/Attention: A hybrid CTC/Attention model trained multilin-
gually without any intermediate CTC objectives.
SC-CTC: A model trained with intermediate self-conditioned
CTC [27], as discussed in Sec. 2.2. The intermediate label is identical
to the ASR ground truth. We use the same Transformer SC-CTC
parameters as [27]: 5 intermediate layers (3, 6, 9, 12, 15) with an
intermediate CTC weight of w= 0.5. For the 12-layer Conformer
encoder, we use intermediate layers 3,6, and 9.
4.2.2. Proposed Models
LID utt& LID tok:Models trained with the proposed intermediate
tasks that explicitly leverage the LID described in Sec. 3.1. The
intermediate layer conﬁguration is the same as SC-CTC. In the LID utt
model, all intermediate layers use a single LID token as the output
label. For LID tok, the ground truth is comprised of an LID token for
each token in the original utterance.
HierLID utt& HierLID tok:Our proposed model that incorporates
the LID prediction task into a hierarchical setup (Sec. 3.2). The ﬁrst
intermediate layer (layer 3) uses the LID as the CTC objective, while
deeper intermediate layers (6,9,12,15) use the ASR text. We report
results for both LID uttand LID tokas the ﬁrst objective.
5. RESULTS
We report both Character Error Rate (CER) and Mixed Error Rate
(MER), along with the language identiﬁcation accuracy (LID). MER
is calculated using the CER for languages not delimited by white
space, and Word Error Rate (WER) for all other languages. Table 2
shows our early experiments with different pre-trained SSL models.
While self-conditioning improved the results of both models ( A1
vs.A2,B1vs.B2), XLS-R consistently outperformed WavLM
and achieved SOTA performance. This result was apparent in early
development, so we did not continue experimentation with WavLM.
Table 3 presents our main results in four partitions: 1) prior
work, 2) Transformer baselines, 3) Transformers with the proposed
methods, and 4) extended studies with Conformers. Our baseline ( B1)
improves upon previous works ( Z1andZ2) by using XLS-R SSL
features [19] with a CTC/Attention architecture. Conditioning on both
LID and transcriptions further improves ASR performance ( B1vs
B2). Moreover, explicitly conditioning on the LID is more beneﬁcial
than self-conditioning ( B2vs.C1,C2). Speciﬁcally, LID tokis more
effective than LID utt(C1v.s.C2); the former even outperforms SC-
CTC by 3.0 MER absolute ( B2vs.C2). The addition of hierarchical
Table 3 . Character error rate (CER), mixed error rate (MER), and
language identiﬁcation % accuracy (LID) on FLEURS.
ID Model Test
CER(↓) MER(↓) LID (↑)
Prior Work
Z1 w2v-bert-51 [21] 14.1 - -
Z2 mSLAM-101 [21] 14.6 - -
Transformer
B1 CTC/Attention 13.9 39.7 95.73
B2 +SC-CTC 13.7 38.8 95.39
C1 +LID utt 13.6 37.2 95.62
C2 +LID tok 13.4 35.8 95.86
C3 +HierLID utt 13.3 36.1 95.43
C4 +HierLID tok 13.3 36.0 95.31
Conformer
D1 +SC-CTC 10.4 32.9 95.41
D2 +HierLID utt 10.1 31.5 94.92
Table 4 . Languages with largest differences in Character Error Rate
(CER) (↓) between HierLID uttConformer and w2v-bert: Georgian
(Ka), Cantonese (Yue), Hebrew (He), Swedish (Sv), and Umbundu
(Umb).
ID Model Ka Yue He Oc Sv Umb
Z1 w2v-bert-51 [21] 30.7 37.0 37.2 11.7 7.6 13.1
Z2 mSLAM-101 [21] 31.0 39.8 42.5 12.7 7.8 14.0
D1 SC-CTC 8.0 15.4 18.1 14.4 11.7 23.7
D2 HierLID utt 8.1 15.3 17.0 17.6 15.7 22.4
conditioning, however, shrinks this gap ( C3vsC4). The combination
of both LID uttand SC-CTC improves over solely LID utt-conditioning
by a large degree ( C1vs.C3), suggesting that some amount of token-
level conditioning is necessary to take advantage of the technique.
We further push ASR performance by applying these methods to
the Conformer. All Conformer models outperform their Transformer
variants, and HierLID uttmaintains its advantage over SC-CTC ( D1
vs.D2). However, due to the increased training instability of the
Conformer [44], the other methods do not converge with the same
optimization settings. Therefore, due to this difference in training
stability and the similar performance of the proposed methods in
our Transformer trials, we prefer evaluating HierLID utt(D2) when
training Conformer models. The combination of HierLID uttand the
Conformer yields our best result ( D2), which outperforms the CER of
previous work in equivalent settings by a wide margin: 4.0 absolute1.
5.1. Analysis
To better understand effectiveness of our technique, we conducted an
analysis of our results by language. Table 4 compares the best/worst
performing languages by HierLID uttConformer ( D2) relative to w2v-
bert ( Z1), which can vary as much as 22.7 CER. These large dis-
crepancies are likely derived from differences in SSL pre-training.
Compared to w2v-bert (600M parameters), XLS-R (300M parame-
ters) was pretrained on an additional 6.6K hours of data (436K total)
that extended its language coverage by 77. We suspect that the larger
parameter size and smaller pool of languages allowed w2v-bert to
learn better representations in the languages that it covered, which
carried over to ASR. Similarly, Table 5 compares the languages with
the largest change in LID accuracy between our two Conformer mod-
1One concurrent work [45] further improves CER by 1.4, albeit with
additional training data [17, 18, 46, 47], while another [48] was evaluated
zero-shot on a subset of languages.Table 5 . Languages with largest differences in LID accuracy ( ↑)
between HierLID uttand SC-CTC Conformer: Zulu (Zu), Hindi (Hi),
Bosnian (Bs), Occitan (Oc), Swedish (Sw), and Umbundu (Umb).
ID Model Zu Hi Bs Oc Sv Umb
D1 SC-CTC 66.8 80.4 32.1 48.1 95.3 91.7
D2 HierLID utt 83.6 91.4 42.9 35.4 75.9 60.0
Table 6 . Average Conformer CER ( ↓) compared to prior work for
each language group.
ID Model WE EE CMN SSA SA SEA CJK
Z1 w2v-bert-51 [21] 10.7 9.9 14.5 15.6 17.4 14.7 25.0
Z2 mSLAM-101 [21] 10.6 10.0 14.8 16.4 19.2 14.9 24.6
D1 SC-CTC 9.0 7.5 9.1 12.6 16.3 14.6 17.9
D2 HierLID utt 9.3 7.5 9.2 12.0 15.5 13.5 18.3
Table 7 . Average Conformer LID % accuracy ( ↑) compared to prior
work for each language group.
ID Model WE EE CMN SSA SA SEA CJK
Z1 w2v-bert-51 [21] 85.3 78.4 72.9 59.1 52.0 65.7 89.7
Z2 mSLAM-101 [21] 84.6 81.3 75.9 62.2 51.7 73.4 87.8
D1 SC-CTC 94.1 95.1 98.9 96.6 89.6 94.1 99.3
D2 HierLID utt 92.5 94.2 97.7 96.4 90.5 95.4 98.9
els. We found that degradations in LID accuracy were often caused
by confusion with a related language. However, this was generally
accompanied by improvements in the other language, such as with
the case of Serbian and Bosnian. In extreme cases, misclassiﬁcations
considerably affected CER, such as for Swedish and Occitan (Tables
4 and 5), which were frequently misidentiﬁed as Norwegian and
French respectively.
We also performed a region-level analysis. Table 6 shows the
CERs for each group in FLEURS: Western Europe (WE), Eastern
Europe (EE), Central-Asian, Middle-East and North-Africa (CMN),
Sub-Saharan Africa (SSA), South Asia (SA), South-East Asia (SEA),
and East Asia (CJK). Both Conformer models improve across-the-
board compared to prior work [21], with notable CER reductions
in the CJK and CMN language groups. The HierLID utttechnique
is particularly effective on the SSA, SA, and SEA language groups
compared to SC-CTC, with a small performance cost in WE, CMN,
and CJK ( D1vs.D2). Table 7 makes a similar comparison using
LID accuracy across language groups. Both Conformer models again
out-perform previous work across all language groups, but the LID
accuracy of HierLID uttdegrades in all but two language groups when
compared to SC-CTC ( D1vs.D2).
6. CONCLUSION
Improving multilingual ASR can help extend speech technologies to
new languages. However, these models face the challenge of handling
the typological diversity of so many languages. To help handle this,
we introduce a framework using hierarchical CTC that can leverage
language identity throughout the entire encoder-decoder network, hy-
pothesizing that correctly identifying the language eases transcription
modelling. We evaluate our technique on the 102-language FLEURS
dataset to show its effectiveness and improve over the results of prior
work. In the future, we hope to extend our approach to an even larger
set of languages [4] and data [11], so that these trained models can
also in downstream tools, such as with speech alignment and data
cleaning, that can further help extend speech technologies to more
languages.
7. ACKNOWLEDGEMENTS
This work used the Bridges2 system [49], supported by NSF award
number ACI-1445606, at the Pittsburgh Supercomputing Center.
8. REFERENCES
[1] S. Watanabe, T. Hori, and J. R. Hershey, “Language independent end-to-end
architecture for joint language identiﬁcation and speech recognition,” in Proc.
ASRU , 2017, pp. 265–271.
[2] A. Bapna, C. Cherry, Y . Zhang, et al. , “mSLAM: Massively multilingual joint
pre-training for speech and text,” arXiv preprint arXiv:2202.01374 , 2022.
[3] Y . Lu, M. Huang, X. Qu, et al. , “Language adaptive cross-lingual speech rep-
resentation learning with sparse sharing sub-networks,” in Proc. ICASSP , 2022,
pp. 6882–6886.
[4] X. Li, F. Metze, D. R. Mortensen, et al. , “ASR2K: Speech Recognition for
Around 2000 Languages without Audio,” in Proc. Interspeech , 2022, pp. 4885–
4889.
[5] P. Ogayo, G. Neubig, and A. W Black, “Building African V oices,” in Proc. In-
terspeech , 2022, pp. 1263–1267.
[6] C. Zhang, B. Li, T. Sainath, et al. , “Streaming end-to-end multilingual speech
recognition with joint language identiﬁcation,” Proc. Interspeech , 2022.
[7] J. Bai, B. Li, Y . Zhang, et al. , “Joint unsupervised and supervised training for
multilingual asr,” in Proc. ICASSP , 2022.
[8] B. Li, R. Pang, Y . Zhang, et al. , “Massively multilingual asr: A lifelong learning
solution,” in Proc. ICASSP , 2022, pp. 6397–6401.
[9] O. Adams, M. Wiesner, S. Watanabe, et al. , “Massively multilingual adversarial
speech recognition,” in Proc. NAACL-HLT , 2019, pp. 96–108.
[10] V . Pratap, A. Sriram, P. Tomasello, et al. , “Massively Multilingual ASR: 50 Lan-
guages, 1 Model, 1 Billion Parameters,” in Proc. Interspeech , 2020, pp. 4751–
4755.
[11] W. Hou, Y . Dong, B. Zhuang, et al. , “Large-Scale End-to-End Multilingual
Speech Recognition and Language Identiﬁcation with Multi-Task Learning,” in
Proc. Interspeech , 2020, pp. 1037–1041.
[12] X. Li, S. Dalmia, J. Li, et al. , “Universal phone recognition with a multilingual
allophone system,” in Proc. ICASSP , 2020, pp. 8249–8253.
[13] A. Conneau, A. Baevski, R. Collobert, et al. , “Unsupervised Cross-Lingual
Representation Learning for Speech Recognition,” in Proc. Interspeech , 2021,
pp. 2426–2430.
[14] B. Li, R. Pang, T. N. Sainath, et al. , “Scaling end-to-end models for large-scale
multilingual asr,” in Proc. ASRU , 2021, pp. 1011–1018.
[15] B. Yan, S. Dalmia, D. R. Mortensen, et al. , “Differentiable allophone graphs for
language-universal speech recognition,” in Proc. Interspeech , 2021.
[16] L. Zhou, J. Li, E. Sun, et al. , “A conﬁgurable multilingual model is all you need
to recognize all languages,” in Proc. ICASSP , 2022, pp. 6422–6426.
[17] M. J. F. Gales, K. M. Knill, A. Ragni, et al. , “Speech recognition and key-
word spotting for low-resource languages: Babel project research at CUED,”
inProc. 4th Workshop on Spoken Language Technologies for Under-Resourced
Languages (SLTU 2014) , 2014, pp. 16–23.
[18] R. Ardila, M. Branson, K. Davis, et al. , “Common voice: A massively-
multilingual speech corpus,” English, in Proceedings of the Twelfth Language
Resources and Evaluation Conference , Marseille, France: European Language
Resources Association, May 2020.
[19] A. Babu, C. Wang, A. Tjandra, et al. , “XLS-R: Self-supervised cross-lingual
speech representation learning at scale,” arXiv preprint arXiv:2111.09296 , 2021.
[20] Y .-A. Chung, Y . Zhang, W. Han, et al. , “W2v-bert: Combining contrastive learn-
ing and masked language modeling for self-supervised speech pre-training,” in
Proc. ASRU , 2021, pp. 244–250.
[21] A. Conneau, M. Ma, S. Khanuja, et al. , “Fleurs: Few-shot learning evaluation of
universal representations of speech,” arXiv preprint arXiv:2205.12446 , 2022.
[22] M. P. Lewis, Ethnologue: Languages of the world . SIL international, 2009.
[23] S. Watanabe, T. Hori, S. Kim, et al. , “Hybrid ctc/attention architecture for end-to-
end speech recognition,” IEEE Journal of Selected Topics in Signal Processing ,
vol. 11, no. 8, pp. 1240–1253, 2017.
[24] J. Lee and S. Watanabe, “Intermediate loss regularization for ctc-based speech
recognition,” in Proc. ICASSP 2021 , 2021, pp. 6224–6228.
[25] A. Tjandra, C. Liu, F. Zhang, et al. , “Deja-vu: Double feature presentation and
iterated loss in deep transformer networks,” in Proc. ICASSP , 2020, pp. 6899–
6903.
[26] R. Sanabria and F. Metze, “Hierarchical multitask learning with ctc,” in Proc.
SLT, 2018, pp. 485–490.
[27] J. Nozaki and T. Komatsu, “Relaxing the conditional independence assumption
of CTC-based ASR by conditioning on intermediate predictions,” in Proc. Inter-
speech , 2021, pp. 3735–3739.
[28] J. Zhang, Y . Peng, H. Xu, et al. , “Intermediate-layer output regularization
for attention-based speech recognition with shared decoder,” arXiv preprint
arXiv:2207.04177 , 2022.[29] Y . Higuchi, K. Karube, T. Ogawa, et al. , “Hierarchical conditional end-to-end
asr with ctc and multi-granular subword units,” in Proc. ICASSP 2022 , 2022,
pp. 7797–7801.
[30] Y . Higuchi, N. Chen, Y . Fujita, et al. , “A comparative study on non-autoregressive
modelings for speech-to-text generation,” in Proc. ASRU , 2021, pp. 47–54.
[31] Y . Yang, Y . Li, and B. Du, “Improving ctc-based asr models with gated interlayer
collaboration,” arXiv preprint arXiv:2205.12462 , 2022.
[32] Y . Fujita, T. Komatsu, and Y . Kida, “Multi-sequence intermediate conditioning
for ctc-based asr,” arXiv preprint arXiv:2204.00175 , 2022.
[33] A. Graves, S. Fern ´andez, F. Gomez, et al. , “Connectionist temporal classiﬁca-
tion: Labelling unsegmented sequence data with recurrent neural networks,” in
Proc. International Conference on Machine Learning , 2006, pp. 369–376.
[34] B. Yan, C. Zhang, M. Yu, et al. , “Joint modeling of code-switched and monolin-
gual asr via conditional factorization,” in Proc. ICASSP , 2022, pp. 6412–6416.
[35] B. Yan, S. Dalmia, Y . Higuchi, et al. , “CTC alignments improve autoregressive
translation,” arXiv preprint arXiv:2210.05200 , 2022.
[36] S.-w. Yang, P.-H. Chi, Y .-S. Chuang, et al. , “SUPERB: Speech Processing Uni-
versal PERformance Benchmark,” in Proc. Interspeech , 2021, pp. 1194–1198.
[37] S. Chen, C. Wang, Z. Chen, et al. , “WavLM: Large-scale self-supervised pre-
training for full stack speech processing,” IEEE Journal of Selected Topics in
Signal Processing , pp. 1–14, 2022.
[38] D. S. Park, W. Chan, Y . Zhang, et al. , “Specaugment: A simple data augmen-
tation method for automatic speech recognition,” Proc. Interspeech , pp. 2613–
2617, 2019.
[39] T. Ko, V . Peddinti, D. Povey, et al. , “Audio augmentation for speech recognition,”
Proc. Interspeech , 2015.
[40] T. Kudo and J. Richardson, “Sentencepiece: A simple and language independent
subword tokenizer and detokenizer for neural text processing,” in Proc. EMNLP
2018 , 2018, pp. 66–71.
[41] S. Watanabe, T. Hori, S. Karita, et al. , “ESPnet: End-to-end speech processing
toolkit,” in Proc. Interspeech , 2018, pp. 2207–2211.
[42] A. Vaswani, N. Shazeer, N. Parmar, et al. , “Attention is all you need,” in Ad-
vances in Neural Information Processing Systems (NeurIPS) , 2017, pp. 5998–
6008.
[43] A. Gulati, J. Qin, C.-C. Chiu, et al. , “Conformer: Convolution-augmented Trans-
former for Speech Recognition,” in Proc. Interspeech , 2020, pp. 5036–5040.
[44] P. Guo, F. Boyer, X. Chang, et al. , “Recent developments on espnet toolkit
boosted by conformer,” in Proc. ICASSP , 2021, pp. 5874–5878.
[45] Z. Chen, A. Bapna, A. Rosenberg, et al. , “Maestro-U: Leveraging joint speech-
text representation learning for zero supervised speech asr,” arXiv preprint
arXiv:2210.10027 , 2022.
[46] C. Wang, M. Riviere, A. Lee, et al. , “V oxPopuli: A large-scale multilingual
speech corpus for representation learning, semi-supervised learning and inter-
pretation,” in Proc. ACL , Online, Aug. 2021, pp. 993–1003.
[47] V . Pratap, Q. Xu, A. Sriram, et al. , “MLS: A Large-Scale Multilingual Dataset
for Speech Research,” in Proc. Interspeech , 2020, pp. 2757–2761.
[48] A. Radford, J. W. Kim, T. Xu, et al. , “Robust speech recognition via large-scale
weak supervision,”
[49] N. A. Nystrom, M. J. Levine, R. Z. Roskies, et al. , “Bridges: A uniquely ﬂexi-
ble hpc resource for new communities and data analytics,” in Proc. of the 2015
XSEDE Conference: Scientiﬁc Advancements Enabled by Enhanced Cyberin-
frastructure , 2015, pp. 1–8.