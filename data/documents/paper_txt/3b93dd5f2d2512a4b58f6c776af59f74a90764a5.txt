arXiv:2301.09099v2  [cs.CL]  26 Jan 2023UNSUPERVISED DATA SELECTION FOR TTS:
USING ARABIC BROADCAST NEWS AS A CASE STUDY
Massa Baali1, Tomoki Hayashi2,3, Hamdy Mubarak3, Soumi Maiti4,
Shinji Watanabe4, Wassim El-Hajj5, Ahmed Ali3
1KANARI AI , California, USA,4Carnegie Mellon University, Pittsburgh, USA
3Qatar Computing Research Institute, HBKU, Doha, Qatar
2Nagoya University, Japan,3Human Dataware Lab. Co., Ltd., Japan
5American University of Beirut, Computer Science Departmen t, Beirut, Lebanon
ABSTRACT
Several high-resource Text to Speech (TTS) systems current ly pro-
duce natural, well-established human-like speech. In cont rast,
low-resource languages, including Arabic, have very limit ed TTS
systems due to the lack of resources. We propose a fully unsup er-
vised method for building TTS, including automatic data sel ection
and pre-training/ﬁne-tuning strategies for TTS training, using broad-
cast news as a case study. We show how careful selection of dat a,
yet smaller amounts, can improve the efﬁciency of TTS system
in generating more natural speech than a system trained on a b ig-
ger dataset. We adopt to propose different approaches for th e:
1) data: we applied automatic annotations using DNSMOS, aut o-
matic vowelization, and automatic speech recognition (ASR ) for
ﬁxing transcriptions’ errors; 2) model: we used transfer le arning
from high-resource language in TTS model and ﬁne-tuned it wi th
one hour broadcast recording then we used this model to guide a
FastSpeech2-based Conformer model for duration. Our objec tive
evaluation shows 3.9% character error rate (CER), while the ground
truth has 1.3% CER. As for the subjective evaluation, where 1is
bad and5is excellent, our FastSpeech2-based Conformer model
achieved a mean opinion score (MOS) of 4.4for intelligibility and
4.2for naturalness, where many annotators recognized the voic e
of the broadcaster, which proves the effectiveness of our pr oposed
unsupervised method.
Index Terms —Text-to-Speech, TTS, low resources, speech recog-
nition
1. INTRODUCTION
Great progress in deep learning and neural end-to-end appro aches
have lowered the barrier to build high quality TTS systems [1 , 2, 3].
However, training end-to-end TTS systems requires a large a mount
of high-quality paired text and audio, which are expensive a nd time
consuming to build. Most of the recent TTS systems require si zable
amounts of data; More than 260hours by professional voice actors
[4],25hours [5], 585hours constructed from 2,456speakers [1] and
24hours [2, 6]. Unlike the data collection in building ASR syst ems,
TTS needs careful recording control, since the TTS performa nce
is tied to professional speaker and high-quality sizable re cordings.
Thus where high-resource languages such as English, Japane se, and
Mandarin [1, 7, 8] has reached almost human-like performanc e in
TTS, limited attention is given to under-resource language s [9] or
resource efﬁciency of TTS [10].In [10], they proposed to use semi-supervised training fram e-
work by allowing Tacotron to utilize textual and acoustic kn owl-
edge contained in large corpora. They condition the Tacotro n en-
coder by embedding input text into word vectors. The Tacotro n
decoder was then pre-trained in the acoustic domain using an un-
paired speech corpus. In [9], authors introduce cross-ling ual trans-
fer learning for low-resource languages to build end-to-en d TTS. To
tackle input space mismatch across languages, authors prop osed a
phonetic transformation network model by mapping between s ource
and target linguistic symbols according to their pronuncia tion. An-
other approach for cross-lingual transfer learning presen ted by [11]
initializes the phoneme embeddings from scratch for low-re source
languages and discards the pre-trained phoneme embeddings . There
has been some work on data efﬁciency in TTS, researchers in [1 2]
used vector-quantization variational-Autoencoder to ext ract the un-
supervised linguistic units from large untranscribed spee ch then ﬁne-
tune it on labeled speech. Others [13] described a new unsupe rvised
speaker selection method based on clustering per-speaker a coustic
representations.
In this work, we describe a fully unsupervised framework to
build a low-resource TTS system in Arabic using broadcast ne ws
recordings. This is an interesting case study for low-resou rce TTS
as we use Arabic Aljazeera Broadcast news data from QASR cor-
pus [14] as our data source. Broadcast news recordings are wi dely
available for various languages and possibly for large numb er of
hours [15, 16]. Though such broadcast news data is not very hi gh-
quality as TTS studio recordings.
In this paper, we utilize a different data-selection strate gy, text
processing and speech synthesis method. First, in data-sel ection we
show that with expert human annotations we can select 1-hour sub-
set of high-quality data. We also show with automatic annota tions
using DNSMOS [17] we can also achieve very close performance to
expert labelled data, character error rate (CER) of 4.0% com pared
to CER of 3.9% with manual labels. Second, in text-processin g,
we show that using vowelization we can reduce CER from 32.2%
to 3.9%. Finally, in speech synthesis we show that using tran sfer
learning from high-resource language in TTS model and using this
autoregressive (AR) TTS model to guide a non-autoregressiv e (non-
AR) TTS model for duration, we can achieve naturalness score of
4.2 and intelligibility score of 4.4 using only 1 hour data. T he re-
producible recipe have been shared with the community throu gh the
ESPnet-TTS project1.
1https://github.com/espnet/espnet/tree/master/egs2/q asr_tts/tts1
2. BUILDING TTS CORPUS FROM NEWS RECORDING
As per our knowledge there is no standard TTS corpus for our lo w-
resource language Arabic. Hence, we use the MGB-2 corpus [16 ]
obtained from the Aljazeera Broadcast news channel, this da ta was
initially optimized for ASR. Collected data spans over 11 ye ars from
2004 until 2015. It contains more than 4,000 episodes from 19 differ-
ent programs covering different domains like politics, soc iety, econ-
omy, sports, science, etc. For each episode, Aljazeera prov ided the
following: ( i) audio sampled at 16KHz; ( ii) manual transcription;
and ( iii) metadata for most of the recordings. Metadata contains fol -
lowing information: program name, episode title and date, s peaker
names and topics of the episode. Also no alignment informati on
is provided with the textual transcriptions. The quality of the tran-
scription varied signiﬁcantly; the most challenge is conve rsational
programs with overlapping speech and dialectal usage; and ﬁ nally
metadata is not always available or standardized.
2.1. Speaker Identiﬁcation and Speaker Linking
Majority of metadata information appears in the beginning o f the
ﬁle. However, some of them are embedded inside the episode tr an-
scription. One of the main challenges is the inconsistency i n speaker
names, e.g. Barack Obama appeared in 9different forms ( Barack
Obama ,Barack Obama/the US President ,Barack Obama/President
of USA ,etc.). The list of guest speakers and episode topics are not
comprehensive, with many spelling mistakes in the majority of meta-
data ﬁeld names and attributes. To overcome these challenge s, we
applied several iterations of automatic parsing and extrac tion fol-
lowed by manual veriﬁcation and standardization. This data is pub-
licly available2.
2.2. Data Selection for TTS Corpus
Our next step in building the TTS corpus is selecting anchor s peak-
ers from the MGB-2 dataset. We picked recordings from two an-
chors as our main speakers, one male and one female. Intuitiv ely,
anchor speakers will be speaking clearly, and most of the tim e, their
recordings will be done in a high-quality studio with less no isy envi-
ronment. Unlike an interviewee over the phone or an audio rep orter
covering events outside the studio. By investigating some r ecord-
ings, we can identify the following six classes and only the l ast one
is our desired data:
• Background music normally happens at the beginning or at
the end of each episode;
• Wrong transcription often happens due to the fact that broa d-
cast human transcription is not verbatim, e.g. sometimes
some spoken words were dropped due to repetition, correc-
tion or rephrasing;Manual dataset classiﬁcation
• Overlap speech occurs mainly in debates and talk-shows;
• Wrong speaker label when there is a problem in the meta-data
(speaker linking results);
• Bad recording quality happens when the anchor speaker re-
ports from a noisy environment, far-ﬁeld microphone, or cal l-
ing over the phone..etc; and
•Good recording , none of the above.
2.2.1. Manual dataset classiﬁcation
To classify the good recordings we hired a professional ling uist to
listen to all segments from the selected best two anchor spea kers and
2https://arabicspeech.org/qasr_ttsclassify them into these aforementioned six-classes. The b iggest two
challenges are bad recordings (40%) and overlapped speech ( 33%).
Out of this task, we were able to manually label about one hour per
speaker, which we studied for building a TTS system. We ran ex per-
iments on these two different datasets mainly to assess the q uality of
the proposed recipe in the experimental part.
2.2.2. Automatic dataset classiﬁcation
The second approach we investigated to choose the good sampl es is
automatic data classiﬁcation because manual labelling is t ime con-
suming. We use three different methods MOSNet, wv-MOS, and
DNSMOS.
•MOSNet [18] is a deep learning-based assessment model to
predict human ratings of converted speech. They adopted the
convolutional and recurrent neural network models to build a
mean opinion score (MOS) predictor.
•wv-MOS [19] Due to the limitations of MOSNet’s ability to
detect obvious distortions they trained a modern neural net -
work model wav2vec2.0 on the same data as MOSNet. They
found that the wv-MOS was an effective predictor of subjec-
tive speech quality.
•DNSMOS [17] is a Convolutional Neural Network (CNN)
based model that is trained using the ground truth human rat-
ings obtained using ITU-T P.808 [20, 21]. It can be applied to
stack rank various Deep Noise Suppression (DNS) methods
based on MOS estimates with great accuracy.
Our input to the proposed methods are the wav ﬁles and the outp ut
is MOS score ranging from 1 to 5, with lowest score of 1 and high -
est score of 5. We considered all the MOS scores that are above
4 as excellent samples. In Table 1 we shows various challenge s in
random 10 hours for the selected two anchor speakers with man -
ual selection. Additionally, we present the labeled good se gments
from the automatic selection. We investigated the MOS score pre-
diction for each approach by analyzing the common good sampl es
chosen by the manual annotators. We found that DNSMOS & wv-
MOS achieved high correlation with human annotators rating s for
the good segments. However, such MOS rating evaluations sol ely
took into account the quality of the speech, regardless of an y tran-
scription errors. In order to consider the good samples with wrong
transcriptions and achieve our goal of building a fully unsu pervised
TTS, we used the Arabic ASR in [22] to ﬁx the spelling mistakes .
Table 1 .Details of the TTS. Segments(Seg.) and Duration (Dur.) in mi nutes
for each class
Male Female
Class # Seg. Dur. # Seg. Dur.
Background music 631 70 1,586 131
Wrong transcription 134 15 930 62
Overlapped speech 1,860 200 4,028 260
Wrong speaker 22 3 292 12
Bad recordings 2,200 240 1,312 105
Good Segments 1,200 60 1,094 70
DNSMOS 687 53 1,241 97
MOSNet 1209 70 1334 87
wv-MOS 1315 63 933 44
2.2.3. Vowelized Text
Our text was without vowelization (aka diacritization) whi ch makes
the problem very challenging. In order to diacritize a text, a complex
system that considers linguistic rules and statistics is ne eded [23].
Furthermore, there are differences between diacritizing a transcribed
speech and a normal written text such as correction, hesitat ion, and
repetition. That further needs more attention from the curr ent text di-
acritizers. Additionally, the diacritized text should mat ch the speak-
ers’ actual pronunciation of words even if they are not gramm atically
correct. In the experimental part, we will see the impact of t he vow-
elization on affecting the quality of the produced results.
3. ARCHITECTURE
In this section, we will introduce the architecture of text- to-mel mod-
els and vocoder models used in the evaluation. We employed AR
and non-AR text-to-mel models to compare the performance wi th a
small amount of training data.
3.1. Autoregressive models
We trained Tacotron2 [24] and Transformer-TTS [25] as our AR
text-to-mel model. Tacotron2, is a recurrent neural networ k (RNN)-
based sequence-to-sequence model. It consists of a bi-dire ctional
Long short-term memory (LSTM)-based encoder and a unidirec -
tional LSTM-based decoder with location sensitive attenti on [26].
After the decoder, the convolutional PostNet reﬁnes the pre dicted
sequence by predicting a residual component of the target se quence.
Transformer-TTS adopts a multi-headed self-attention mec hanism
by replacing the RNNs with the parallelizable self-attenti on struc-
ture. This enables faster and more efﬁcient training while m aintain-
ing the high perceptual quality comparable to the Tacotron2 [25]. We
additionally used the guided attention loss [3] to help the l earning of
diagonal attention weights for both AR models.
Since our training data was limited, it was challenging for t he
model to learn the alignment between the input sequence and t he
target sequence from one hour of speech. To address this issu e, at
ﬁrst, we performed pre-training on the LJSpeech dataset [27 ], which
consists of 24 hours of single-female English speech. After pre-
training, we ﬁne-tuned the pre-trained model using a small a mount
of Arabic training data. We initialized the network paramet ers except
for the token embedding layer because of the mismatch in lang uages.
3.2. Non-autoregressive models
We trained FastSpeech2 [6] based on Conformer [28] as our non -AR
text-to-mel model, which uses the Conformer block instead o f the
Transformer block. It consists of a Conformer-based encode r, a du-
ration predictor, a pitch predictor, an energy predictor, a length regu-
lator, a Conformer-based decoder, and PostNet. The duratio n predic-
tor is a convolutional network that predicts the duration of each input
token from the hidden representation of the encoder. The pit ch and
energy predictors are convolutional networks that predict pitch and
energy sequences from the encoder hidden representation, r espec-
tively. Instead of pitch and energy sequences of the target s peech,
token-averaged sequences are used to avoid over-ﬁtting [29 ]. Their
embedding is added to the encoder hidden representation. Th e length
regulator replicates each frame of the encoder hidden repre sentation
using each input token’s duration to match the time-resolut ion with
the target mel-spectrogram. Finally, the decoder converts the up-
sampled hidden representation into the target mel-spectro gram, and
PostNet reﬁnes it.
We did not perform pre-training for FastSpeech2 as we found
that the non-AR model does not require a large amount of train ing
data. As a result we train it from scratch with much smaller tr aining
data compared to the AR model.3.3. Synthesis
We used the Grifﬁn–Lim algorithm (GL) [30] and Parallel Wave -
GAN (PWG) [31] to generate speech from the predicted mel-
spectrogram. In the case of GL, the sequence of the predicted
mel-spectrogram was converted to a linear spectrogram with the
inverse mel-basis, and then GL was applied to the spectrogra m. In
the case of PWG, we trained the model from scratch with the sam e
training data as the mel-spectrogram generation networks. We used
the ground-truth of mel-spectrogram in training while usin g the
predicted one in inference.
4. EXPERIMENTS
We conducted experiments to test our proposed methodologie s with
LJSpeech English 24-hours of speech from single speaker, th en ﬁne-
tuned with one hour in Arabic. The audio quality is evaluated by
human testers. One of the common problems of E2E-TTS is that t he
generated speech sometimes includes the deletion and/or re petition
of words in the input text due to alignment errors. To address this
issue, we evaluated the word error rate (WER) and CER of gener -
ated speech using pre-trained ASR models and then automatic ally
detected the deletion and/or repetition of words. We also ex peri-
mented with the automatic data selection, as discussed in Se ction
2.2.2 to construct the TTS corpus.
4.1. Experimental Conditions
Our training process requires ﬁrst training the text-to-me l prediction
network on its own, followed by training a PWG independently on
the outputs generated by the ﬁrst network. We investigated t he per-
formance with the combinations of the following three condi tions:
Model architecture : To check the difference among the
model architectures, we compared three architectures: Tac otron2,
Transformer-TTS, and Fastspeech2, as described in Section 3. The
AR models were pre-trained3as the character-based model with the
LJSpeech dataset. We used 12,600 utterances for the trainin g and
250 utterances for the validation in pre-training. After pr e-training,
we ﬁne-tuned the model using Arabic corpus as describe in Sec tion
2. We used 25 utterances for development and 25 for testing an d
the rest for training. For non-AR models, we extracted the gr ound-
truth duration of each input token from the attention weight s of the
teacher model with teacher forced prediction [2]. We employ ed both
Tacotron2 and Transformer-TTS as the teacher model and comp ared
the performance. Then, we trained the non-AR model from scra tch
using only Arabic data, with the same split of data as the AR mo dels.
Vowelization : V owelization of the input transcription is impor-
tant to solve the mismatch between the input text and the outp ut pro-
nunciation. We performed the vowelization, as described in section
2.2.3. We compared the case of w/ and w/o vowelization to chec k
the effectiveness.
Reduction factor : The reduction factor [5] is a common pa-
rameter of text-to-mel models to decide the number of output frames
at each time step. This plays an important role in achieving s table
training, especially when using a limited amount of trainin g data.
We compared the reduction factor 1and3for each condition.
3https://zenodo.org/record/4925105
Table 2 .Objective evaluation results for the male speaker, where “R ” repre-
sents the reduction factor, “Vowel. ” represents whether to vowelize the input
text, and the name in paraphrases (·) represents the teacher model used for
the extraction of ground-truth durations.
ID Model R Vowel. WER [%] CER [%] MCD [dB]
1V Tacotron2 1✓ 34.0 14.0 10.1 ± 2.4
1 Tacotron2 1 54.4 29.8 8.5 ± 1.4
2V Tacotron2 3✓ 35.3 14.1 10.5 ± 2.9
2 Tacotron2 3 51.1 27.2 8.9 ± 1.6
3V Transformer-TTS 1✓ 20.4 9.7 8.6 ± 1.7
3 Transformer-TTS 1 45.2 21.6 9.0 ± 1.9
4V Transformer-TTS 3✓ 15.4 5.5 8.8 ± 0.9
4 Transformer-TTS 3 19.9 9.6 8.6 ± 1.7
5V FastSpeech2 (Taco2) 1✓ 9.5 4.4 8.3 ± 1.6
5 FastSpeech2 (Taco2) 1 22.8 7.0 8.5 ± 1.4
6V FastSpeech2 (Taco2) 3✓ 10.8 4.4 8.3 ± 1.1
6 FastSpeech2 (Taco2) 3 26.1 9.6 8.9 ± 1.6
7V FastSpeech2 (Trans.) 1✓ 9.1 3.9 8.8 ± 0.9
7 FastSpeech2 (Trans.) 1 53.9 32.3 9.6 ± 1.5
8V FastSpeech2 (Trans.) 3✓ 14.1 5.7 9.0 ± 1.0
8 FastSpeech2 (Trans.) 3 34.0 14.7 9.2 ± 1.1
- Ground-truth N/A N/A 3.0 1.3 N/A
Table 3 .The comparison of detailed CER for the male speaker between w /
or w/o vowelization. The model was FastSpeech2 with the Tran sformer-TTS
as the teacher model and the reduction factor was set to 1.
ID Model Sub. Ins. Del. CER [%]
7V w/ vowelization 11 2 32 3.9
7 w/o vowelization 160 45 170 32.3
4.2. Results and Analysis
4.2.1. Objective Evaluation
For objective evaluation, we reported the Mel-Cepstral Dis tortion
(MCD) [dB]. We also reported Word Error Rate (WER) and CER
on a single speaker using the large vocabulary speech recogn ition
Arabic end-to-end transformer system in [22].
The objective evaluation in Table 2 shows that vowelization im-
proved results considerably. Table 3 shows the impact of vow eliza-
tion for the FastSpeech2 models. The generated speech in the un-
vowelized text compared to the vowelized text includes a lot of dele-
tion of characters in the input text. Usually, the model with a larger
reduction factor achieves faster training convergence and easier at-
tention alignment. However, as we can see from the results pr oduced
that it reduces the quality of the predicted frames. 1V and 3V showed
reasonable performance in this particular task, but the 5V a nd 7V
outperformed the two models even without the pre-training. This re-
sult indicated that the amount of training data required for non-AR
models is much smaller than AR models.
We notice that WER for the ground-truth is very low compared
to any other representative test set. This can be owed to the f act that
anchor speaker speaks clearly and recording setup is very go od.
To check the quality of the automatic labeling done by MOSNet ,
wv-MOS, and DNSMOS (Section 2.2.2) vs. manual labeling pro-
vided by the annotators, we trained a FastSpeech2 model with the
ﬁne-tuned Transformers as a teacher model. Table 4 shows a co m-
parison between the efﬁciency of the four models trained on d iffer-
ent samples, which shows that the manual labeling slightly o utper-
formed the automatic labeling. However the DNSMOS shows a ve ry
promising results as its slightly worse than the manual sele ction.
For checking the efﬁciency of the data selection and the perf or-
mance of the mechanism presented, we trained a FastSpeech2 m odel
with the ﬁne-tuned Tacotron2 as a teacher model for both the m ale
and the female speaker dataset. Table 5 shows a comparison be tween
the results of each dataset using the same model. This shows t hat the
male speaker dataset has higher quality.Table 4 .Comparison between the manual and the automatic labeling re -
sults for the male speaker. The model was FastSpeech2 with th e Transformers
as the teacher model. The reduction factor was set to 1 and vow elization was
performed.
ID Model WER [%] CER [%] MCD [dB]
10 MOSNet 22.0 10.0 12.6 ± 1.1
10V wv-MOS 18.0 7.0 10.9 ± 1.0
11 DNSMOS 11.0 4.0 11.2 ± 0.65
7V w/ manual labels 9.13 3.9 8.8 ± 0.9
Table 5 .Objective evlaution using FastSpeech2 with the Tacotron2 a s the
teacher model with vowelization.
ID Dataset WER [%] CER [%] MCD [dB]
5V Male 9.5 4.4 8.3 ± 1.6
11V Female 13.0 7.0 7.9 ± 1.3
Table 6 .Subjective evaluation results with 95% conﬁdence interval ,
where reduction factor “R”, intelligibility “Int. ” and nat uralness
“Nat. ”.
ID Model R Vowel. Int. Nat.
7V FastSpeech2 (Trans.) 1✓ 4.1 ± 0.06 4.0 ± 0.06
7 FastSpeech2 (Trans.) 1 3.5 ± 0.08 3.2 ± 0.08
8V FastSpeech2 (Trans.) 3✓ 3.9 ± 0.07 3.8 ± 0.07
12 FastSpeech2 (Trans.) w/ PWG 1✓ 4.4 ± 0.06 4.2 ± 0.06
- Ground-truth N/A N/A 4.9 ± 0.05 4.9 ± 0.05
4.2.2. Subjective Evaluation
Finally, we conducted the subjective evaluation using MOS o n nat-
uralness and intelligibility. We compared the following fo ur models
trained on the male dataset: 7V , 7, 8V , and 7V with PWG. For a
reference, we also used ground-truth samples in the evaluat ion. We
used 100 sentences of evaluation data for the subjective eva luation.
Each subject evaluated 100 samples of each model (in total 40 0 sam-
ples) and rated the intelligibility and naturalness of each sample on
a 5-point scale: 5 for excellent, 4 for good, 3 for fair, 2 for p oor, and
1 for bad. The number of subjects was ten.
Table 6 shows that with reduction factor 1, vowelization sig nif-
icantly improved both intelligibility and naturalness, re vealing the
effectiveness of vowelization. We can conﬁrm that a larger r educ-
tion factor did not improve the performance, although it acc elerated
the training speed. Finally, the result with PWG showed that ap-
plying the neural vocoder brought a signiﬁcant improvement of the
naturalness and the intelligibility. We can also conclude f rom the
high results presented in the ground-truth that people are v ery famil-
iar with the tone of the speakers in the broadcast news.
5. CONCLUSION
This paper introduced a method for building TTS in an unsuper vised
way, including data selection and pre-training/ﬁne-tunin g strategies
for TTS training, using Arabic broadcast news as a case study . We
explained the pipeline that involves data collection, adap tation of
AR models, training of non-AR models, and training PWG. Our a p-
proach resulted in MOS of 4.4/5 for intelligibility (almost excellent)
and 4.2/5 for naturalness (almost very good). For future wor k, we
plan to automate the process for all the speakers in the QASR [ 14]
corpus. Finally, we also consider applying our pipeline on a nother
language, such as the BBC in the MGB-1.
6. REFERENCES
[1] H. Zen, V . Dang, R. Clark, Y . Zhang, Ron J Weiss, Y . Jia,
Z. Chen, and Y . Wu, “LibriTTS: A corpus derived from Lib-
rispeech for text-to-speech,” Proc. Interspeech 2019 .
[2] Y . Ren, Y . Ruan, X. Tan, T. Qin, S. Zhao, Z. Zhao, and T. Liu,
“Fastspeech: Fast, robust and controllable text to speech, ”Ad-
vances in neural information processing systems , 2019.
[3] H. Tachibana, K. Uenoyama, and S. Aihara, “Efﬁciently tr ain-
able text-to-speech system based on deep convolutional net -
works with guided attention,” in ICASSP , 2018.
[4] Je. Donahue, S. Dieleman, M. Bi´ nkowski, E. Elsen, and K. Si-
monyan, “End-to-end adversarial text-to-speech,” Interna-
tional Conference on Learning Representations , 2020.
[5] Y . Wang, R. Skerry-Ryan, D. Stanton, Y . Wu, R. Weiss,
N. Jaitly, Z. Yang, Y . Xiao, Z. Chen, S. Bengio, et al.,
“Tacotron: Towards end-to-end speech synthesis,” Proc. In-
terspeech 2017 .
[6] Y . Ren, C. Hu, T. Qin, S. Zhao, Z. Zhao, and T. Liu, “Fast-
speech 2: Fast and high-quality end-to-end text-to-speech ,”In-
ternational Conference on Learning Representations , 2020.
[7] R. Sonobe, S. Takamichi, and H. Saruwatari, “JSUT corpus :
free large-scale japanese speech corpus for end-to-end spe ech
synthesis,” arXiv:1711.00354 , 2017.
[8] T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watan -
abe, T. Toda, K. Takeda, Y . Zhang, and X. Tan, “Espnet-tts:
Uniﬁed, reproducible, and integratable open source end-to -end
text-to-speech toolkit,” in ICASSP , 2020.
[9] T. Tu, Y . Chen, C. Yeh, and H. Lee, “End-to-end text-to-sp eech
for low-resource languages by cross-lingual transfer lear ning,”
Proc. Interspeech 2019 .
[10] Y . Chung, Y . Wang, W. Hsu, Y . Zhang, and R. Skerry-Ryan,
“Semi-supervised training for improving data efﬁciency in
end-to-end speech synthesis,” in ICASSP , 2019.
[11] J. Xu, X. Tan, Y . Ren, T. Qin, J. Li, S. Zhao, and T. Y . Liu, “ Lr-
speech: Extremely low-resource speech synthesis and recog ni-
tion,” in Proceedings of the 26th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining , 2020.
[12] H. Zhang and Y . Lin, “Unsupervised learning for sequenc e-
to-sequence text-to-speech for low-resource languages,” Proc.
Interspeech 2020 .
[13] P. Gallegos, J. Williams, J. Rownicka, and S. King, “An u n-
supervised method to select a speaker subset from large mult i-
speaker speech synthesis datasets,” Interspeech , 2020.
[14] H. Mubarak, A. Hussein, S. Chowdhury, and A. Ali, “QASR:
QCRI Aljazeera speech resource–a large scale annotated Ara -
bic speech corpus,” ACL, 2021.
[15] P. Bell, M. Gales, T. Hain, J. Kilgour, P. Lanchantin, X. Liu,
A. McParland, S. Renals, Os. Saz, M. Wester, et al., “The MGB
challenge: Evaluating multi-genre Broadcast media recogn i-
tion,” in ASRU , 2015.[16] A. Ali, P. Bell, J. Glass, Y . Messaoui, H. Mubarak, S. Ren als,
and Y . Zhang, “The MGB-2 challenge: Arabic multi-dialect
broadcast media recognition,” in SLT, 2016.
[17] C. K. Reddy, V . Gopal, and R. Cutler, “Dnsmos: A non-
intrusive perceptual objective speech quality metric to ev aluate
noise suppressors,” in ICASSP 2021 .
[18] C. Lo, S. Fu, W. Huang, X. Wang, J. Yamagishi, Y . Tsao, and
H. Wang, “Mosnet: Deep learning based objective assessment
for voice conversion,” in Interspeech 2019 .
[19] P. Andreev, A. Alanov, O. Ivanov, and D. Vetrov, “Hiﬁ++: a
uniﬁed framework for neural vocoding, bandwidth extension
and speech enhancement,” arXiv:2203.13086 , 2022.
[20] B. Naderi and R. Cutler, “An open source implementation
of itu-t recommendation p. 808 with validation,” Proc. Inter-
speech 2020 .
[21] C. K. Reddy, V . Gopal, R. Cutler, E. Beyrami, R. Cheng,
H. Dubey, S. Matusevych, R. Aichner, A. Aazami, S. Braun,
et al., “The interspeech 2020 deep noise suppression challe nge:
Datasets, subjective testing framework, and challenge res ults,”
Proc. Interspeech 2020 .
[22] A. Hussein, S. Watanabe, and A. Ali, “Arabic speech reco gni-
tion by end-to-end, modular systems and human,” Computer
Speech & Language , 2021.
[23] A. Abdelali, K. Darwish, N. Durrani, and H. Mubarak,
“Farasa: A fast and furious segmenter for Arabic,” in The
North American chapter of the association for computationa l
linguistics: Demonstrations , 2016.
[24] J. Shen, R. Pang, R. Weiss, M. Schuster, N. Jaitly, Z. Yan g,
Z. Chen, Y . Zhang, Y . Wang, R. Skerrv-Ryan, et al., “Natural
TTS synthesis by conditioning wavenet on mel spectrogram
predictions,” in ICASSP , 2018.
[25] N. Li, S. Liu, Y . Liu, S. Zhao, and M. Liu, “Neural speech
synthesis with transformer network,” in AAAI , 2019.
[26] J. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and Y . Ben -
gio, “Attention-based models for speech recognition,” NIPS ,
2015.
[27] K. Ito and L. Johnson, “The lj speech dataset,” Online:
https://keithito. com/LJ-Speech-Dataset , 2017.
[28] P. Guo, F. Boyer, X. Chang, T. Hayashi, Y . Higuchi, H. In-
aguma, N. Kamo, C. Li, D. Garcia-Romero, J. Shi, et al., “Re-
cent developments on ESPnet toolkit boosted by conformer,”
ICASSP , 2021.
[29] A. Ła´ ncucki, “Fastpitch: Parallel text-to-speech wi th pitch pre-
diction,” ICASSP , 2021.
[30] N. Perraudin, P. Balazs, and P. L. Søndergaard, “A fast g rifﬁn-
lim algorithm,” in 2013 IEEE Workshop on Applications of
Signal Processing to Audio and Acoustics .
[31] R. Yamamoto, E. Song, and J. Kim, “Parallel wavegan: A
fast waveform generation model based on generative adversa r-
ial networks with multi-resolution spectrogram,” in ICASSP ,
2020.