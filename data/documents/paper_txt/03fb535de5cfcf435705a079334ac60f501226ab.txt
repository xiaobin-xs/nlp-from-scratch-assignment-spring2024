Reverse-Engineering Decoding Strategies Given
Blackbox Access to a Language Generation System
Daphne Ippolito∗
dei@google.comNicholas Carlini*
ncarlini@google.comKatherine Lee*
katherinelee@google.com
Milad Nasr*
miladnasr@google.comYun William Yu†
ywyu@math.toronto.edu
Abstract
Neural language models are increasingly deployed
into APIs and websites that allow a user to pass
in a prompt and receive generated text. Many of
these systems do not reveal generation parameters.
In this paper, we present methods to reverse-
engineer the decoding method used to generate
text (i.e., top- kor nucleus sampling). Our ability
to discover which decoding strategy was used has
implications for detecting generated text. Addi-
tionally, the process of discovering the decoding
strategy can reveal biases caused by selecting de-
coding settings which severely truncate a model’s
predicted distributions. We perform our attack on
several families of open-source language models,
as well as on production systems (e.g., ChatGPT).
1 Introduction
Language models are increasingly being incorporated
into web applications and other user-facing tools.1
These applications typically do not provide direct
access to the underlying language model or the
decoding configuration used for generation. In this
paper, we show how even in this blackbox setting, it
is possible to identify the decoding strategy employed
for generation. We consider the case where one only
has access to a system that inputs a prompt and out-
puts a generated response. We present algorithms for
distinguishing the two most popular decoding strate-
gies, top- kand nucleus sampling (a.k.a. top- p), and
estimating their respective hyperparameters ( kandp).
The choice of decoding strategy—the algorithm
used to sample text from a language model—has a
profound impact on the randomness of generated text,
introducing biases toward some word choices. For ex-
ample, when OpenAI’s ChatGPT,2a chatbot built with
large language models, is repeatedly passed a prompt
asking it to report the outcome of rolling a twenty-
∗Google Deepmind, †University of Toronto
1E.g., see https://gpt3demo.com/ for a list of such
apps.
2https://openai.com/blog/chatgpt/sided die, we found that it only returns 14 of the 20
options, even though all should be equally likely.
Prior work has shown that knowing the decoding
method makes it easier to detect whether a writing
sample was generated by a language model or
else was human-written (Ippolito et al., 2020). As
generated text proliferates on the web, in student
homework, and elsewhere, this disambiguation is
becoming increasingly important.
Concurrent work to ours by Naseh et al. (2023) has
developed similar strategies for detecting decoding
strategy from a blackbox API: however, they focus
more on identifying hybrid decoding strategies (includ-
ing beam search), whereas we focus more on prompt
engineering to produce close-to-uniform token distri-
butions that reduce the number of queries needed. Our
proposed methods complement but are not compara-
ble to those of Tay et al. (2020). Their method trains
classifiers that input a generated text sequence and
output a prediction for the decoding strategy used to
generate it. In contrast, our method interacts with an
API and does not require any data or ML training.
2 Background
Neural language models are not inherently generative.
A causal language model fθtakes as input a
sequence of tokens x1,...,x t−1and outputs a score
for each possible next token xt, computing the
a likelihood score for each token in the vocabu-
lary, which can be transformed into a probability
distribution by applying a softmax such that
Prob(xt|x1,...,x t−1)∼fθ(x1,...,x t−1).
Adecoding method takes this probability
distribution as input and samples a particular token
to output. The simplest algorithm is argmax decoding
(also known as ‘greedy decoding’), where the most
likely next token is outputted. Argmax is rarely used
in practice because (1) only one generation can be
produced for any given prompt, and (2) generations
with argmax tend to be repetitive and low-quality.
Most commonly used decoding algorithms arearXiv:2309.04858v1  [cs.LG]  9 Sep 2023
based on random sampling: a token is chosen with
probability proportional to the likelihood assigned
to it by the model. Whereas argmax sampling has
too little randomness, purely random sampling over
the full distribution can have too much, leading to
text that is too erratic and prone to errors. Thus, it is
common to modify the distribution to reduce entropy
before sampling from it.
In this short paper, we focus on two popular
strategies researchers have developed for decoding:
top-ksampling (Fan et al., 2018) and top- psampling
(Holtzman et al., 2019) (also known as nucleus
sampling). Top- ksampling involves the implementer
picking a fixed hyperparemter kthen only ever
sampling from the kmost likely items by assigning all
other items a score of 0 before applying the softmax.
Top-psampling involves the implementer picking
a fixed hyperparamter p. Then at each step tof
generation, a ktis selected such that the ktmost likely
vocabulary items cover pproportion of the total prob-
ability mass in the distribution. More precisely, let the
notation x(l)refer to the lth most likely token in the
distribution predicted at step t. We set ktto the first
value for whichPkt
l=1Prob(xt=xl|x1,...,x t−1)≥p.
Then, the distribution is truncated to the ktmost likely
tokens, as described above for top- k.
Other common methods like beam search and
temperature annealing are omitted in the interest
of space (cf. Zarrieß et al. (2021) and Wiher et al.
(2022)). Temperature annealing simply modifies the
probability distributions of the output tokens, so the
methods in this manuscript can be easily generalized
(and indeed were in the concurrent work of Naseh
et al. (2023)). Beam search is a bit more complicated,
as tokens are not chosen independently of previous
tokens; instead, multiple candidate token paths are
retained. As such, it would be necessary to generate
more than a single word for each prompt, which is
the primary interrogative tool we use here.
3 Method
3.1 Threat Model
We assume black-box, query-only access to the system
Gen:m7→rwhich takes as input a prompt string m
and outputs a textual response r; without loss of gen-
erality, we assume that the response ris exactly one
token long. The adversary can input arbitrary prompts
and observe the output response. In most of our exper-
iments, we assume Genpasses minto the language
model without any modification, then generates a con-
tinuation using an unknown decoding strategy. How-ever, in some cases, such as for ChatGPT, the system
might modify the provided prompt, m, such as by
prepending few-shot examples, before passing it to
the language model. Still, we assume that the causal
language model can be repeatedly queried by a fixed
prompt m′, even if modified from the original m.
The adversary’s attack objective is to determine the
decoding strategy employed by Gen, outputting either
topk ortopp , as well as the value for either pork.
3.2 Intuition for Method
We begin with the intuition of our attack. Suppose
we were given a prompt m, such that the output of
Gen(m)is equally likely to be any item from a set of
vocabulary items Vm⊆V. For example, the prompt
“List of capital English letters,
chosen uniformly at random: ” ought to
result in the model emitting each of the 26 letters
of the alphabet with equal probability. However,
suppose that when we repeatedly prompt the model in
this way, it only ever emits 10 different letters. What
could cause this?
One explanation could be that our prompt does
not actually induce a uniform probability distribution
over each of the 26 letters, and in fact that the model
assigns (nearly) zero probability mass to the 11th
most likely token. Suppose we know for a fact the
prompt does induce a near-uniform distribution on
all publicly-available language models: then the
more likely explanation would be that the sampling
algorithm itself truncated this distribution—either
with top- kor top- psampling. By measuring what
fraction of the words we would expect to get
generated actually do get generated for prompts with
known output distributions, we can estimate values for
kandpand distinguish between these two techniques.
3.3 Estimating k
Suppose, for a given prompt m, we call Gen(m),
nnumber of times, each time keeping just the first
token of the output. We can trivially lower bound k
by observing the number of unique items in a set of re-
sponses. As napproaches ∞, allkallowed responses
will be observed. To achieve a compute-efficient
attack, our goal is to estimate kwhile keeping nas
small as possible. Appendix A gives theoretical ac-
curacy/runtime estimates for this approach by posing
it as the coupon collector problem (Pólya, 1930).
In practice, we use Algorithm 1 (see Appendix),
which repeatedly estimates a lower bound for kusing
two different prompts m1andm2for increasing
numbers of trials until (1) the two estimates match
and (2) the x(k)token appears at least twice in both
generations (to prevent spurious matching).
3.4 Estimating p
In this paper, we set a goal of determining pto
within 0.05of the true value. We can upper bound
pby constructing a prompt that yields a known,
computable distribution over a set of vocab items
Vm. Then to attack a system, we repeatedly sample
with the prompt, and count how many of those items
are generated. Let’s call this value k.3We estimate
pas the sum of the probabilities of the kmost likely
tokens in the known distribution over items in Vm.
Because our guessed distributions are not perfect,
instead of relying on just one prompt for our estimate,
we instead average over two upper bounds of p
derived from two different prompts. Although our
experiments here use only two prompts, increased
precision is achievable by using additional prompts.
The detailed algorithm can be found in the Appendix.
3.5 Distinguishing Top- kfrom Top- p
To distinguish between top- kand top- p, we need
only reject the hypothesis that top- kis used. It turns
out that we can simply reuse Algorithm 1 because
we already built in a measure of concordance in
thekpredictions. If the two prompts used as input
to Algorithm 1 continue to yield very different
predictions of kno matter how many samples are
taken, we can reject the hypothesis of top- kbeing
used. For rejecting top- k, we found it useful to start
with two prompts with radically different distributions;
it suffices to choose prompts that with very differently
sizedVm, such as A DVERBS and M ONTHS .
Although we did not explore it in this short paper,
we could in theory also reject top- pby looking at how
closely the pestimates from different prompts match.
This may prove helpful if we wish to determine
that neither top- por top- kis being used, but is
unnecessary for simply disambiguating the two.
Prompt Selection In addition to the distributional
properties described above, we also need our prompts
to have the property that the first space-separated
word in the output of Gen(m)is in-fact a word in the
vocabulary. Since we often do not know which vo-
cabulary was used by the model we are attacking, we
choose prompts which yield distributions over words
which are likely to be tokens in all models trained on
3This is a slight abuse of notation since we used kearlier for
top-k, but in both cases, this value corresponds precisely to the
number of unique tokens seen.Table 1: Prompts used for top- kand (top) top- p(bottom)
estimation on open-source models. The first two prompts
include randomly selected exemplars (shown in blue). For
MONTHS , Ramadan is included as the 13th month.
Name Prompt |Vm|
NOUNS List of nouns chosen completely
randomly: dog, slash, altar8,432
ADVERBS List of adverbs chosen completely
randomly: formally, blatantly, sadly504
MONTHS She came to visit in the month of 13
DATES The accident occurred on March 31
0 100 200 300 400 500
True k02468101214161820222426Error (true k - predicted k)
 Blackbox model
gpt2-xl
bloom-3b
Figure 1: Error in top- kestimation for 500 GPT-2 XL-
and 500 bloom-3b-based systems. Error for both models
is very low in common settings ( k< 100).
Table 2: Performance at kestimation over 100 systems
withkvalues randomly chosen between 1 to 500.
Model Acc Acc ±5Avg Error
GPT-2 Base 28% 76% 1.3
GPT-2 XL 44% 80% 0.9
BLOOM-3B 0% 71% 2.3
pythia-2.7b 22% 81% 1.1
webtext4. Table 1 shows all the prompts used in all
experiments except for those we used on ChatGPT
(which had to be longer), and Appendix C gives more
details on prompt selection (including for ChatGPT).
4 Experiments
We conduct experiments on four language models
where we can set the decoding strategy: GPT-2
Base and XL (Radford et al., 2019), GPT-3 Davinci
(Brown et al., 2020), BLOOM 3B (Scao et al., 2022),
and Pythia 2.7B (Biderman et al., 2023).
4.1 Predicting k
We used two prompt templates for estimating top- k:
NOUNS andADVERBS (see Table 1), each with 16
randomly selected exemplars. We build an evaluation
set of 100 systems, each with a kselected uniform
4Other prompts may be needed for attacking code models.
pythia-2.7bbloom-3bgpt2
gpt2-xl
text-davinci-0010.00.20.40.60.81.0Acc±0.05Known Dist
gpt2-base
text-davinci-001Figure 2: Performance at pestimation over 100 systems
withpvalues ranging from 0.0 to 1.0, when the known
distributions are computed using GPT-2 base (orange), and
GPT-3 Davinci likelihoods (blue). Using Davinci as the
known model leads to a better attack on GPT-3 models,
but a worse one on all other models. RMSE in Table 6.
Figure 3: Mismatch between the known distributions and
the distributions of the language model underlying the
blackbox system lead to increased error. The matched
estimates still have a slight systematic upward bias because
we use the upper bounds for pin our algorithm.
randomly to be between 1 and 500. Table 2 shows the
accuracy of our approach on this evaluation set.5We
see that while our method is not so great at guessing
kperfectly, on average its guesses are between 0.9
and 2.3 off (depending on the underlying model).
In Figure 1, we plot accuracy as a function of true
kfor GPT-2 XL. This plot reveals that our method
is especially effective at predicting kfork<300, and
accuracy deteriorates for higher k. The vast majority
of applications use kwell within this range, and it is
simple to adjust for larger kby increasing the max
number of iterations parameter.
4.2 Predicting p
We build an evaluation set of 100 systems, each with a
randomly assigned pbetween 0 and 1. Table 1 shows
the prompts used for top- pestimation: MONTHS
andDATES . For each prompt, we need to compute
a known distribution over the next word. We experi-
ment with using both GPT-2 Base and GPT-3 Davinci
5GPT-3 is omitted because the API does not expose top- k.
0.0 0.2 0.4 0.6 0.8 1.0
Blackbox system's p value0204060Ratio of predicted ks
Blackbox system: bloom-3bFigure 4: We plot the ratio between the kestimated using
ADVERBS and using MONTHS , for systems actually
using top- p. Except when pis near its extreme values, the
prompt with larger Vmresults in more generated words.
for this. For GPT-2, we compute the distribution
directly; for GPT-3, we estimate it by running 1,000
trials with full random sampling. Figure 2 shows
our method’s accuracy at predicting within 0.05 of
the true pvalue. Figure 3 shows two limitations: (1)
our estimates are worse when there is significant mis-
match between Gen’s distributions and our known
distributions; and (2) the minimum pour method
can predict is 
Prob(x(1)
1)+Prob(x(1)
2)
/2, reducing
accuracy for low pvalues. Further research is needed
into the design of prompts which induce consistent
distributions over many families of language models.
4.3 Distinguishing Top- kand Top- p
To evaluate our ability to distinguish between top- k
and top- p, we conduct the following experiment. We
take 10 systems with pvalues ranging from 0.0 to
1.0 and find the chance we misclassify each system
as using top- k. Figure 4 reports the results of this
experiment. We see that it would be fairly easy to dis-
tinguish the two methods by thresholding the ratio of
thekvalues returned by the two prompts. Note that at
the extreme values of kandp, the method are indistin-
guishable. Top- kwithk=1 and top- pwithp=0.0 are
both the same as argmax; top- kwirhk=|V|and top- p
withp=1.0 are both the same as full random sampling.
4.4 ChatGPT
We cannot repeat the exact same experiments with
ChatGPT because (1) it does not use the exact
prompt passed to the UI as the language model
input, instead preprocessing it into a conversation
format; and (2) the rate limiting prevents us from
easily conducting many trials. We instead employ
four conversational-format prompts (see Table A5).
For the known distribution, we try out empirical
distributions from five different versions of GPT-3,
Prompt nEst.p
MONTHS CHAT 200 0.84
DATES CHAT 125 0.74
D20C HAT 115 0.79
D100C HAT 500 0.86
Table 3: The values of pestimated for ChatGPT using
different prompts, where nis the number of samples taken.
and take the one with lowest total variation distance
from ChatGPT’s output distribution. Table 3 shows
thepestimated using each prompt. Averaging
these, we get p=0.81. This estimate could be further
narrowed down by incorporating more prompts,
though of course we cannot validate this number due
to opacity of the ChatGPT proprietary system.
4.5 Room for Improvement
All of the estimates reported in this section could be
improved with additional queries to the model For
bothpandkestimation, we average over the estimates
from just two prompts, but using more prompts would
lead to better estimates. In addition, to improve top- k
estimation for larger k, one can increase the minimum
number of times the least frequent items needs to be
seen before the sampling loop terminates; in this pa-
per, we set that value to 2. Our methods could also be
further improved by in-depth investigation of prompts
which consistently produce close-to-uniform distri-
butions across different families of language models.
Finally, while our methods do not currently address
the case where temperature annealing is used in
conjunction with top- kor top- p, extending them to
support this setting should be straightforward. Tem-
perature followed by top- kis still top- k, and should
be detectable via our methods. Temperature followed
by top- pis trickier, because we no longer have a
known distribution. However, this combination can
be detected by comparing the empirical distribution
against a set of known distributions for common
models; if the distribution does not match any of
them, then we can conclude that either it is not using
any known model, or that other distribution shaping
such as temperature has been applied.
5 Limitations
Our method is limited to identifying when top- psam-
pling or top- ksampling is used. We do not attempt to
detect other decoding strategies which other systems
might use. Additionally, there is no guarantee that a
system would use a single decoding strategy—it is
possible that different prompts may trigger differentdecoding strategies, or that A/B testing results in dif-
ferent users seeing different decoding strategies. Our
ChatGPT results were computed by two different au-
thors on separate OpenAI accounts. Also, we have no
guarantees that the decoding strategy is not changed
over time. Some of our ChatGPT results were com-
puted using the December 15, 2022 release while oth-
ers were computed using the January 9, 2023 release.
Additionally, the biases in distributions that we
see here could have other underlying reasons; for
example, changes in the data can result in very
different true distributions.
Furthermore, under the hood, an API might not
be generating a new random generation each time an
identical prompt is passed in. Either random seeds
might be getting re-used, or generations could be
retrieved from a cache. In both cases, the generations
might look like argmax sampling. It’s also conceiv-
able that certain combinations of fixed models could
look like top- k/p. For example, if a query is randomly
routed to one of a series of sservers, each serving
a different model, we might interpret the decoding
strategy to be top- keven if each server is using
argmax. In these cases, an approach more like that
proposed by Tay et al. (2020), where classification of
decoding strategy is made based on a long generated
sequence (rather than single token system predictions,
as in our approach), might be more effective.
For top- p(though not top- k), we require access
to an underlying distribution that approximates the
model used. This is not an issue for open source mod-
els or models with API access that allows specifying
the decoding strategy, but it does limit the applicability
of our method to newer proprietary models. It may
be possible to empirically determine distributions
for carefully engineered prompts, but future work is
needed for reverse engineering fully closed models.
6 Conclusion
Our attack shows that with even a little work, it
is possible to reverse-engineer common decoding
strategies. Although we have focused here only on
top-pand top- ksampling, these approaches generalize
readily to other common methodologies when the
output probability distributions are well-approximated.
Along with other recent work on reverse-engineering
other parts of a language generation system (Zhang
and Ippolito, 2023), it seems is infeasible to hide
inference implementation details given black-box
access to the system.
References
Stella Biderman, Hailey Schoelkopf, Quentin Anthony,
Herbie Bradley, Kyle O’Brien, Eric Hallahan, Mo-
hammad Aflah Khan, Shivanshu Purohit, USVSN Sai
Prashanth, Edward Raff, et al. 2023. Pythia: A suite
for analyzing large language models across training and
scaling. arXiv preprint arXiv:2304.01373 .
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot
learners. Advances in neural information processing
systems , 33:1877–1901.
Angela Fan, Mike Lewis, and Yann Dauphin. 2018.
Hierarchical neural story generation. In Proceedings
of the 56th Annual Meeting of the Association for
Computational Linguistics (V olume 1: Long Papers) ,
pages 889–898.
Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and
Yejin Choi. 2019. The curious case of neural text
degeneration. In International Conference on Learning
Representations .
Daphne Ippolito, Daniel Duckworth, Chris Callison-Burch,
and Douglas Eck. 2020. Automatic detection of
generated text is easiest when humans are fooled. In
Proceedings of the 58th Annual Meeting of the Associ-
ation for Computational Linguistics , pages 1808–1822.
Ali Naseh, Kalpesh Krishna, Mohit Iyyer, and Amir
Houmansadr. 2023. On the risks of stealing the
decoding algorithms of language models. arXiv
preprint arXiv:2303.04729 .
George Pólya. 1930. Eine wahrscheinlichkeitsaufgabe
in der kundenwerbung. Zeitschrift Angewandte
Mathematik und Mechanik , 10(1):96–97.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, Ilya Sutskever, et al. 2019. Language
models are unsupervised multitask learners. OpenAI
blog, 1(8):9.
Teven Le Scao, Angela Fan, Christopher Akiki, Ellie
Pavlick, Suzana Ili ´c, Daniel Hesslow, Roman Castagné,
Alexandra Sasha Luccioni, François Yvon, Matthias
Gallé, et al. 2022. Bloom: A 176b-parameter open-
access multilingual language model. arXiv preprint
arXiv:2211.05100 .
Yi Tay, Dara Bahri, Che Zheng, Clifford Brunk, Donald
Metzler, and Andrew Tomkins. 2020. Reverse engineer-
ing configurations of neural text generation models. In
Proceedings of the 58th Annual Meeting of the Asso-
ciation for Computational Linguistics , pages 275–279,
Online. Association for Computational Linguistics.
Gian Wiher, Clara Meister, and Ryan Cotterell. 2022.
On decoding strategies for neural text generators.
Transactions of the Association for Computational
Linguistics , 10:997–1012.Sina Zarrieß, Henrik V oigt, and Simeon Schüz. 2021.
Decoding methods in neural language generation: a
survey. Information , 12(9):355.
Yiming Zhang and Daphne Ippolito. 2023. Prompts
should not be seen as secrets: Systematically measuring
prompt extraction attack success. arXiv preprint
arXiv:2307.06865 .
A Algorithm for Estimating k
Algorithm 1 Algorithm for estimating k.
Given a system Gen:m7→rthat takes an input prompt mand outputs a single response token r, letm1andm2be two prompts
that with high probability return responses from large ( ≫k) sets of different sizes—e.g. m1returns random nouns and m2returns
random adverbs.
function ESTIMATE K(Prompts m1,m2;Gen:m7→r)
samples1 ← [[],samples2 ← [[]
while max number iterations not reached do // we set max iterations=32
for{1..100}do // at most 3200 samples generated
samples1.insert (Gen(m1))
samples2.insert (Gen(m2))
end for
k1← [# unique items in samples1
k2← [# unique items in samples2
minSamples ← [(samples1(k1)>1and
samples2(k2)>1) // Boolean testing all items appear twice
ifk1=k2and minSamples then
break
end if
end while
return ⌊(k1+k2)/2⌋ // guesses average if convergence not reached
end function
As we mentioned in the main paper text, suppose, for a given prompt m, we call Gen(m),nnumber of
times, each time keeping just the first token of the output. We can trivially lower bound kby observing the
number of unique items in a set of responses. As napproaches ∞, allkallowed responses will be observed.
Since this is infeasible, the adversary’s goal is to estimate kwhile keeping nas small as possible.
It is easy to see that the ideal prompt mis one that gives responses that are perfectly uniform over the entire
vocabulary V. In the uniform case, we are left with the standard coupon collector problem (Pólya, 1930). We
would recover the exact value of kwith probability at least 1−1
kby setting n>2klogk. Unfortunately, such
a prompt is exceedingly difficult to engineer (see Appendix D).
It turns out we can do almost as well without needing full uniformity. The key building block for our attack
is the construction of an mthat distributes substantial probability mass onto a subset Vm⊆Vof the token space.
We require that for any k<|Vm|, we have Prob 
Gen(m)=x(k)
≥1
ck, for some small constant c. Put in plain
language, we want to ensure that for any number of tokens kthe distribution might be truncated at, the least
likely token that can be generated is no more than ctimes less likely to appear than if the distribution were
truly uniform. If n≥2cklog(ck), then with probability at least 1−1
ck, our prediction is exactly correct. This
result is far from tight, but follows easily from coupon collector on a uniformly random set of size ck.
In practice, we use Algorithm 1, which repeatedly estimates a lower bound for kusing two different prompts
m1andm2for increasing numbers of trials until (1) the two estimates match and (2) the x(k)token appears
at least twice in both generations (to prevent spurious matching). In such a case, the expected number of trials
nis approximately bounded above by 2cklog(ck)via coupon collector6.
B Algorithm for Estimating p
Our goal is to estimate pto within a factor of ϵ. This would be trivial to do if we could construct a prompt
mthat is uniform over a subset Vm⊆Vof size at least1
ϵ. Then estimating pwould be equivalent to estimating
top-kfork≈p
ϵbecause each unique token seen implies a probability mass of ϵ.
It is impossible to design a prompt which yields a perfectly uniform distribution. However, although
uniformity is desirable, for top- pestimation, it is more important that the distribution of Gen(m)is known, i.e.,
we have access to the underlying language model fθ. Ifkdistinct tokens appear in the p-truncated distribution,
6Aside: the constant 2 that appears in the expected number of trials is due to requiring that the kth most likely token appears at
least twice. However, it is unrelated to the constant 2that appears in the bound in the previous paragraph, which is chosen to ensure
the1
ckfailure probability.
Algorithm 2 Algorithm for estimating p.
Consider a language model fθ:(m,r)→Rthat scores a prompt/response pair and a system Gen:m7→rthat takes an input prompt
mand outputs a single response token rusingfθand top- psampling. Let m1andm2be two prompts that return responses from
known distributions over relatively small sets ( |Vm|around 10-40)—e.g. m1returns random months and m2returns random dates
within the month of March.
function ESTIMATE P(Prompts m1,m2;Gen:m7→r,fθ)
p1← [HELPER (m1,Gen,fθ)
p2← [HELPER (m2,Gen,fθ)
return (p1+p2)/2
end function
function HELPER (Prompt m;Gen,known LM fθ)
baseProbs ← [[] // Will store known probability distribution
forv∈Vmdo //Vmis the subset of tokens we consider
baseProbs.insert (Pfθ(r=v|m)) // Probabilities using full random sampling
end for
Sort baseProbs from largest to smallest.
baseProbs.insert(P
v∈V−V mProbfθ(r=v|m) // Summed probabilities of all out-of-set tokens
samples ← [[]
for{1..N}do
samples.insert (Gen(m))
end for
l← [num unique items in samples
p← [Pl
i=1baseProbs [i]
return p
end function
then (using the same notation as above), we can bound pas:
k−1X
l=1Prob fθ(x(l))<p≤kX
l=1Prob fθ(x(l)).
Thus, given a known distribution, the top- preverse engineering problem reduces to top- k.
Even if we do not know exactly the underlying model for a blackbox system, we can construct prompts
that appear to often return distributions close to a family of known distributions. Then the error in estimating
pis just determined by how far off our guess of distribution is from the true underlying one. Note that to
ensure robustness against an imperfectly guessed distribution, we estimate pusing the sum of the klargest
in-vocabulary probabilities, rather than trying to actually match the probabilities for the unique items sampled.
This turns out to be important when prompts including exemplars are used, as the exemplars often create a
bias in the tokens returned, but the overall drop-off in probabilities of most to least likely tokens tends to be
more consistent. However, for distribution matching, we use the actual distributions over tokens.
In this paper, we set a goal of determining pto within ϵ=0.05and construct two prompts with almost-known
distributions over k=13 andk=31. Because our guessed distributions are not perfect, instead of relying on
a single distribution to bound our estimate, we instead average over the two upper bounds of pderived from
the different prompts and return that as our guess. Additionally, as kis small for both prompts, instead of using
the stopping criterion of Algorithm 1, for each prompt, we always generate 3000 samples. This means that
with very high likelihood, we correctly return all possible items from the prompt’s vocabulary. Algorithm 2
gives our implementation.
RS: 1 RS: 2 RS: 4 RS: 8 RS: 16 RS: 32 P: 26
Exemplars0.00.51.01.52.02.5entropyDigits
RS: 1 RS: 2 RS: 4 RS: 8 RS: 16 RS: 32 P: 26
Exemplars0123ABCFigure 5: For each prompt style ( DIGITS orABC ), we prompted with either [1, 2, 4, 8, 16, 32] exemplars selected
randomly (RS), or with a random permutation of all expected outcomes (P). We ran three trials for each number of
exemplars and generated the next word 5,000 times per trial. The majority of next-word generated were within the
vocabulary, however, sometimes they were not, in those cases, we discarded that output. Missing bars indicate that there
weren’t enough generated next-words that were in the vocabulary to compute entropy.
C Prompt Selection
This Appendix gives more details on selecting good prompts for the decoding strategy detection task.
D Challenges
We encountered many challenges in selecting appropriate prompts. Our initial aim was to find prompts that
induced an as-close-to-uniform distribution over the next token as possible. In addition to the prompts decided
on for our main experiments (Table 1), we tried prompts meant to elicit a uniform distribution over digits, letters,
dice rolls, and alphanumeric characters. For some of these prompt styles, the main difficulty was in getting the
language model to assign higher probability to the expected outputs for the prompt than to unexpected outputs.
For example, a prompt designed to elicit random digits would result in “and” being a more likely next token
than several of the digits. For other prompts, the distribution was not as random as we would have expected.
If exemplars were involved, even if they were chosen completely randomly, the model would try to follow
any patterns observed in the exemplars. For example, if a prompt containing randomly selected exemplars
of digits happened to end with “2 4 6”, then “8” would be by far the most likely next token. Our difficulty
here conforms with prior work that has shown that language models have significant biases toward certain
numbers and words, even in settings where there should not be such bias.
Table 4: Prompts showcasing the sensitivity of models to different exemplar choices. The exemplars, shown in blue,
can be varied in order and count.
Name Prompt v2 |Vp|
DIGITS Digits: 4, 3, 2 10
ABC Letters: E, F, P 26
Figure 5 shows the variance in output distributions for two exemplar-based prompts, DIGITS andABC (Table
4), across different numbers of exemplars and different random selection of exemplars. The DIGITS prompt is ex-
pected to output digits [0-9] with equal likelihood, and the random letters prompt is expected to output the letters
[A-Z] with equal likelihood. While with enough exemplars, the DIGITS prompt yielded consistently high entropy
(i.e., close to uniform-random) distributions, the ABCprompt did not consistently improve with more exemplars.
In the end, we decided to avoid these prompts, and others which were too dependent on choice of exemplars.
For prompts to be used in top- pestimation, one additional challenge is that ideally the prompt should yield
a similar distribution when inputted to all popular language models. As discussed in the paper, our estimates
forpare worse when there is a greater mismatch between the known distribution used for top- pestimation
and the true distribution of the language model underlying the blackbox system being attacked. Figure 6 shows
gpt2 gpt2-xl bloom-3b pythia-2.7b text-davinci-002
Likelihood0.000.010.020.030.040.05"The accident occurred on March"
gpt2 gpt2-xl bloom-3b pythia-2.7b text-davinci-002
Likelihood0.00.10.20.30.4"She came to visit in the month of"Figure 6: The likelihoods for the digits {1-31} given the prompt “The accident occurred on March" (left) and for {January
thru December + Ramadan} given the prompt “She arrived in the month of” (right), ordered from most to least likely.
The sum probability of the remaining items in Vis shown in red.
Table 5: Prompts uses to estimate the pvalue for ChatGPT.
Name Prompt |Vm|
MONTHS CHAT write one word for the rest of this sentence:
“She came to visit in the month of”13
DATES CHAT write one word for the rest of this sentence:
“The accident occured on March”31
D20C HAT write one number for the rest of this sentence:
“I rolled a D20 and the outcome was”20
D100C HAT Could you roll me a D100? We’re playing
D&D. Answer with just the roll value and
nothing else.100
the known distributions for the two prompts we used in top- pestimation, across several different models. We
see that some models have much spikier distributions than others. The best approach (and the one we used
to attack ChatGPT) is to choose the known distribution to use for top- pestimation by keeping around a database
of distributions from a bunch of different models, and then comparing the output distribution from the blackbox
system to each distribution in the database. We can then choose to estimate pusing the known distribution
with the lowest relative entropy with the blackbox’s one.
D.1 Chosen Prompt Details
Here we describe the actual prompts used in our experiments. For the NOUNS andADVERBS prompts, we
assumed access to the GPT-2 vocabulary and used Spacy ( en_core_web_sm ) to identify all tokens in the
vocabulary corresponding to nouns and adverbs. In all experiments with these prompts, we used 16 randomly se-
lected exemplars from these lists. An example prompt for NOUNS is: “List of nouns chosen completely randomly:
negativity diarrhea problems eloqu money aspect vertex fraternity stone breast skies pushes probabilities ink
north creditor”. In our experiments estimating top- k, for each system being evaluated, we varied the random seed,
resulting in a slightly different prompt. We did this to avoid any systematic biases resulting from always using
the same choice of exemplars. For the non-exemplar-based prompts, we did not assume vocabulary access and
instead relied on the expectation that letters, digits, and common words are present in most model vocabularies.
As mentioned in the main paper, different prompts were needed to attack ChatGPT than for the experiments
on open-source models because ChatGPT expects its inputs to be in a conversation format and it does not offer
control over the number of words generated (without careful prompt design, it tends to return tens to hundreds
of words). Table 5 gives the prompts used to attack ChatGPT.
E Scientific Artifacts
We use the following language models in our research:
•BLOOM 3B : This model was released by BigScience under the RAIL License v1.0 with the goal
to “to enable public research on large language models” (Scao et al., 2022). It can be downloaded at
https://huggingface.co/bigscience/bloom .
•Pythia 2.7B : This model was released by EleutherAI under the MIT License with the goal of enabling
research on “interpretability analysis and scaling laws” (Biderman et al., 2023). It can be downloaded
athttps://github.com/EleutherAI/pythia .
•GPT-2 base and XL : These models were released by OpenAI under the MIT license with the
goal of fostering language model research (Radford et al., 2019). They can be downloaded at
https://huggingface.co/gpt2 .
•ChatGPT and GPT-3 model family : These models are only available via OpenAI’s API or through
OpenAI’s web interface. Our experiments with them fall under OpenAI’s research policy, found at
https://openai.com/api/policies/sharing-publication/#research-policy .
We chose these models evaluate on because (1) we wanted to evaluate our method on a wide range of
independently trained models using different paradigms and training dataset choices. For example, though
we conduct all our experiments using English prompts, we can observe the impact of BLOOM being trained on
multilingual data, in that for the MONTHS prompt, BLOOM puts significant probability-mass on non-English
month names, which could affect our pestimates for BLOOM models.
F Computational Resources
Preliminary experiments were run in Google Colab using a Pro membership, which gave access to one Tesla
T4. Subsequent experiments were running on a Google Cloud machine with 8 Tesla V100s. No more than
100 hours were spent running computation on this machine, which has a cost of $17 per hour.
G Additional Results
Table 6 gives the numbers used in Figure 2 in the main paper, as well as the root mean-square error between
the true and estimated pvalues.
Table 6: Performance at pestimation across 100 estimations with pvalues random from 0 to 1. On the left, GPT-2 Base
was used to compute known distributions, and on the right GPT-3 was used to compute the known distributions.
GPT-2 Base GPT-3 Davinci v1
Model Acc±.05 RMSE Acc±.05 RMSE
GPT-2 Base 0.93 0.03 0.08 0.19
GPT-2 XL 0.82 0.04 0.07 0.21
Davinci v1 0.23 0.14 0.51 0.06
BLOOM-3B 0.77 0.04 0.07 0.22
pythia-2.7b 0.88 0.03 0.07 0.20