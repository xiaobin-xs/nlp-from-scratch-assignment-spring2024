arXiv:2309.09510v1  [eess.AS]  18 Sep 2023DYNAMIC-SUPERB: TOWARDS A DYNAMIC, COLLABORATIVE, AND COM PREHENSIVE
INSTRUCTION-TUNING BENCHMARK FOR SPEECH
Chien-yu Huang1, Ke-Han Lu∗1, Shih-Heng Wang∗1, Chi-Yuan Hsiao†1, Chun-Yi Kuan†1, Haibin Wu†1
Siddhant Arora§2, Kai-Wei Chang§1, Jiatong Shi2, Yifan Peng2, Roshan Sharma2, Shinji Watanabe2
Bhiksha Ramakrishnan2,3, Shady Shehata3, Hung-yi Lee1
1National Taiwan University, Taiwan,2Carnegie Mellon University, USA
3Mohamed bin Zayed University of Artiﬁcial Intelligence, Un ited Arab Emirates
ABSTRACT
Text language models have shown remarkable zero-shot capability
in generalizing to unseen tasks when provided with well-for mulated
instructions. However, existing studies in speech process ing primar-
ily focus on limited or speciﬁc tasks. Moreover, the lack of s tan-
dardized benchmarks hinders a fair comparison across diffe rent ap-
proaches. Thus, we present Dynamic-SUPERB, a benchmark de-
signed for building universal speech models capable of leve raging
instruction tuning to perform multiple tasks in a zero-shot fashion.
To achieve comprehensive coverage of diverse speech tasks a nd har-
ness instruction tuning, we invite the community to collabo rate and
contribute, facilitating the dynamic growth of the benchmark. To ini-
tiate, Dynamic-SUPERB features 55 evaluation instances by com-
bining 33 tasks and 22 datasets. This spans a broad spectrum o f di-
mensions, providing a comprehensive platform for evaluati on. Ad-
ditionally, we propose several approaches to establish ben chmark
baselines. These include the utilization of speech models, text lan-
guage models, and the multimodal encoder. Evaluation resul ts in-
dicate that while these baselines perform reasonably on see n tasks,
they struggle with unseen ones. We also conducted an ablatio n study
to assess the robustness and seek improvements in the perfor mance.
We release all materials to the public and welcome researche rs to
collaborate on the project, advancing technologies in the ﬁ eld to-
gether1.
Index Terms —self-supervised learning, instruction tuning,
benchmark
1. INTRODUCTION
Self-supervised learning (SSL) signiﬁcantly improves the scalabil-
ity of machine learning models and their ability to generali ze across
a wide range of tasks [1–4]. When applying an SSL model to a
downstream task, a common approach is to build a task-depend ent
network on top of the SSL model and ﬁne-tune it with task-spec iﬁc
data. However, this approach demands that users construct a unique
model for each task. As downstream tasks increase, this proc ess
becomes more resource-intensive and time-consuming. To ad dress
these issues, several parameter-efﬁcient tuning approach es have thus
gained popularity, such as prompting [5–13] and adapters [1 4–17].
Particularly, in natural language processing (NLP), instruction tun-
inginvolves ﬁne-tuning language models (LM) on datasets where
the tasks are deﬁned by speciﬁc instructions [18]. This appr oach
∗co-second authors.†co-third authors.§co-fourth authors.
1https://github.com/dynamic-superb/dynamic-superbconsiderably ampliﬁes the zero-shot learning capacity of LMs, en-
abling them to efﬁciently handle unseen tasks. However, it r emains
largely unexplored in the speech-processing ﬁeld, where th e aspects
are even more abundant and diverse. While there are existing stud-
ies like SPECTRON [19] and AudioPaLM [20] that focus on joint ly
processing text and speech, they are designed for speciﬁc ta sks. On
the other hand, AudioGPT [21], though driven by text instruc tions,
is limited to a pre-deﬁned set of tasks.
This paper presents Dynamic-SUPERB, the ﬁrst collaborativ e
benchmark for instruction-tuning speech models. Although previous
benchmarks provide evaluations on several aspects, they ar estatic
[22–29]. In contrast, Dynamic-SUPERB encourages the commu -
nity to contribute a broader range of tasks so that the task va riations
aredynamically extended, aiming for a more comprehensive assess-
ment. For the ﬁrst version, we gathered over 20 publicly avai lable
datasets and transformed them into a diverse set of tasks. Th ese tasks
span 6 dimensions: content, speaker, semantics, degradati on, par-
alinguistics, and audio (non-speech). In Dynamic-SUPERB, each
task is basically composed of three parts: (I) text instruct ions, (II)
speech utterances, and (III) text labels. Following this, t he system
receives both the text instruction and speech utterances as input, and
then functions based on the instruction. For example, given the in-
struction “Identify the emotion conveyed in the utterance” , the model
performs emotion recognition and outputs the label “Happy” textu-
ally. For simplicity in evaluation, we currently focus on cl assiﬁ-
cation tasks, such as intent classiﬁcation and speaker veri ﬁcation,
and leave generative ones for future collaboration with the commu-
nity. To enhance user engagement, we offer detailed documen tation
and establish a clear process for submitting new tasks to Dyn amic-
SUPERB. All submitted tasks are subject to a review before in clu-
sion. Reviews mainly focus on technical accuracy and comple teness,
ensure clarity of the proposal, and determine if the task apt ly evalu-
ates its intended objectives.
We propose ﬁve approaches to establish baselines in Dynamic -
SUPERB. We integrated text embeddings from BERT [30] into th e
generative spoken language model (GSLM) [31], enabling ope ra-
tions on both speech and text. We also adapted Whisper [32], w hich
was primarily designed for speech recognition involving bo th speech
and text, to instruction tuning. Besides modifying speech m odels, we
integrated speech representations from either Whisper or I mageBind
[33], into LLaMA [34,35], a prevalent large language model ( LLM).
We also utilize Whisper (ASR) and ChatGPT [36] to build a con-
catenative system (ASR-ChatGPT). Due to the lack of instruc tion-
based tasks in the ﬁeld, we developed Dynamic-SUPERB-Train us-
ing academic resources as a preliminary training set for the base-
lines. Its overlap with Dynamic-SUPERB differentiates tas ks into
seen andunseen categories, assessing model generalizability. Eval-
uation results indicate that no single model dominates acro ss all
tasks. ASR-ChatGPT excels in some semantic tasks but underp er-
forms with speaker and paralinguistic tasks. Speech models that
integrate text struggle with unseen tasks due to their limit ed com-
prehension of text instructions. Conversely, combining sp eech rep-
resentations with a text language model surpassed other met hods.
2. DYNAMIC-SUPERB
2.1. Objectives
Dynamic-SUPERB is committed to offering a comprehensive ev al-
uation of universal speech models and facilitating advance ments in
this ﬁeld. We believe that instruction tuning is a pivotal st ep to-
wards universal speech models. While we included a wide rang e of
tasks to kickstart the benchmark, there are always numerous innova-
tive and creative applications in speech processing. Unlik e previous
benchmarks with a ﬁxed task set [22–29], Dynamic-SUPERB see ks
to dynamically expand its task variety through community co llabo-
ration, just as its name indicates. We offer a straightforwa rd pipeline
for easy user engagement and have established a review guide line
outlining how the community can contribute to Dynamic-SUPE RB.
2.2. Terminologies
We collected several datasets and used them in a variety of ta sks. For
example, we can utilize LibriSpeech [37] for speaker veriﬁc ation
(multi-speaker dataset), and spoken term detection (trans cription
available), to name just a few applications. Therefore, in D ynamic-
SUPERB, a dataset can correspond to multiple tasks, and a tas k may
cover several datasets. In this paper, a task is the speciﬁc type of
processing or operation to be carried out, such as “speaker v eriﬁca-
tion” or “intent classiﬁcation”. It is a general category th at deﬁnes
the kind of work or operation being done, without specifying the
dataset on which it is performed. Meanwhile, an instance refers to
the speciﬁc combination of a task and a dataset. For example, per-
forming “speaker veriﬁcation” on the “LibriSpeech” datase t would
be an instance. This term describes the exact pairing of the t ask and
dataset we evaluate or discuss.
2.3. Tasks & datasets
Generally, tasks can be categorized as either discriminati ve or gener-
ative. Discriminative tasks aim to sort data into predeﬁned class sets.
In contrast, generative tasks involve producing sequences of tokens
or signals of variable lengths, making them inherently more chal-
lenging. As an initial launch to simplify the evaluation pro cess, we
only consider classiﬁcation tasks in the current version of Dynamic-
SUPERB. These tasks assess the capabilities of models acros s var-
ious dimensions, including content ( CNT ), speaker ( SPK ), seman-
tics ( SEM ), degradation ( DEG ), and paralinguistics ( PRL ). Besides,
we added audio-processing tasks ( AUD ) to investigate how baseline
models extend beyond speech processing. Table 1 outlines th e tasks
in Dynamic-SUPERB by the above-mentioned 6 dimensions.
Given the lack of instruction-tuning data in the ﬁeld, we col -
lected Dynamic-SUPERB-Train to train baselines. It shares some
tasks with Dynamic-SUPERB, termed seen tasks , but uses different
datasets for instances. Due to the space limit, we cannot lis t all tasks
in detail. The full collection of tasks is available on the Dy namic-
SUPERB ofﬁcial page1.Table 1 : Statistics of Dynamic-SUPERB.
Attribute Status CNT SPK SEM DEG PRL AUD
Datasets - 5 3 3 6 7 3
TasksSeen 3 2 2 4 3 0
Unseen 2 1 2 8 3 3
InstancesSeen 9 3 2 6 4 0
Unseen 2 2 4 13 3 7
2.4. Task format
Each task consists of at least three components: speech utte rance, in-
struction, and label. We sourced it from widely-used corpor a such as
LibriSpeech [37], LJSpeech [38], and VCTK [39]. Depending o n the
task speciﬁcation, there may be more than one utterance. For exam-
ple, in speaker veriﬁcation, two utterances are involved to determine
whether they are produced by the same speaker. For instructi ons, we
chose to use text format rather than spoken format. Spoken in struc-
tions, with varying content, speaker characteristics, and prosody, are
more complex than text-based ones. Text instructions serve as an in-
termediary, bridging text and spoken instruction tuning. T o generate
various instructions, we initially created some basic ones manually
and then used ChatGPT to rephrase them and generate addition al
variations. This results in each task containing approxima tely 10 to
30 different types of instructions.
A standard classiﬁcation model produces a distribution ove r all
potential labels. For instance, an emotion classiﬁer assig ns proba-
bilities to each emotion category, including happy, angry, and sad.
The method is simple and efﬁcient, but it constrains the adap tability
of the model. Adding a new emotion, like “tired,” would requi re re-
training the model, let alone adapting it for entirely diffe rent tasks.
To tackle this issue, we believe that a universal speech mode l should
produce outputs generatively. Consequently, in Dynamic-SUPERB,
all labels are represented textually. To explore generalizability on
unseen tasks, all baseline models are designed to respond in a gen-
erative manner. That is, without any constraints, a model ca n pro-
duce any arbitrary response. For example, a model might resp ond
with synonyms instead of the exact desired answer, complica ting the
evaluation process. To address this, we specify the availab le options
directly in the instruction. Speciﬁcally, we append the phr ase “The
answer could be A, B, or C” to the instruction, where A, B, and C
represent the candidate answers. If “A” is the correct answe r, then
only the response “A” is considered correct. Responses like “The
answer is A” or synonyms of “A” are considered incorrect.
2.5. Benchmark extendibility
Dynamic-SUPERB is a benchmark designed with forward-think ing
principles. As detailed in Sec. 2.1, it has the capability to dynam-
ically expand its task coverage. In most conventional bench marks,
adding a new task is challenging because it requires users to undergo
re-training speciﬁc to the new task. However, Dynamic-SUPE RB
leverages instruction tuning, enabling a single model to ha ndle mul-
tiple tasks without explicit ﬁne-tuning . This feature signiﬁcantly sim-
pliﬁes the process when new tasks are integrated, highlight ing the
extendibility of the benchmark.
Contrasting many existing benchmarks that focus on static
evaluations, Dynamic-SUPERB adopts a more dynamic and ﬂex-
ible design. Recognizing the ever-changing landscape of sp eech-
processing tasks, we believe that benchmarks should evolve along-
side the innovations and advances in research. Though Dynam ic-
SUPERB may not encompass every potential speech-processin g task
at present, its real value lies in its design that encourages growth and
the incorporation of emerging tasks. This inclusive framew ork not
only welcomes but thrives on community collaboration, faci litating
the introduction of innovative tasks and methods. Such exte ndibility
ensures that Dynamic-SUPERB remains relevant and effectiv e in
the fast-paced world of speech-processing research.
3. BASELINE FRAMEWORKS
While instruction tuning is a promising method for achievin g uni-
versal speech models, there is no model that has been speciﬁc ally
designed to do so. Therefore, based on existing models, we pr o-
pose ﬁve approaches to instruction tuning and serve them as t he
baseline models. These models were directly trained with Dy namic-
SUPERB-Train and tested on Dynamic-SUPERB.
3.1. BERT-GSLM
GSLM [31] comprises three components: a discrete feature ex tractor
(utilizing HuBERT and k-means clustering), a unit language model
(uLM), and a speech synthesizer. The ﬁrst two components are used
here. To adapt the GSLM for instruction tuning, we aim to enha nce
the uLM to handle both input and output in textual form. For th e in-
put, we employ BERT [30] to derive contextualized represent ations
of text tokens. These are then projected into the GSLM embedd ing
space using a linear layer. For the output, originally, the u LM pro-
duced a probability distribution exclusively over speech t okens. We
expanded the token set by incorporating text tokens, allowi ng the
uLM to produce a distribution that includes both speech and t ext
tokens. During training, feature extractors (HuBERT & BERT ) re-
mained ﬁxed, while the uLM and the projection layer were trai nable.
3.2. Whisper
Whisper [32] is a large-scale ASR model, but its encoder capt ures
a wealth of information beyond mere content [40]. The decode r
transcribes based on speech features and previously genera ted texts,
making it potentially suitable for our scenario. We add the i nstruc-
tion to the text input of the decoder, enabling it to autoregr essively
generate the desired output conditioned on the instruction .
In the original Whisper implementation, several special to kens
were used to signify speciﬁc functions, and we also adhered t o
this for instruction tuning. Text instructions are appende d after the
PREV token, followed by the start of transcription token, a lan-
guage tag token, and a timestamp disabling token. Whisper th en
auto-regressively generates a response based on these toke ns and the
encoder-extracted speech features. During training, we em ployed
Whisper-medium, and both the encoder and decoder were updat ed.
3.3. Multi-modal large language model
LLMs have demonstrated exceptional proﬁciency in instruct ion tun-
ing tasks within NLP, prompting our choice of LLaMA [34,35] h ere.
Speciﬁcally, we used ImageBind-LLM [41], a multi-modal var iant
of LLaMA. ImageBind [33] is a multi-modal encoder that embed s
several data types, such as images and audio, into a uniﬁed sp ace.
Speech samples are converted into utterance-level feature s and sub-
sequently integrated with LLaMA through the zero-initiali zed injec-
tion mechanism [41, 42]. We employed LLaMA-Adapter [42, 43]Table 2 : Average accuracy (%) of the proposed baselines on seen
tasks in Dynamic-SUPERB across different dimensions. Entr ies
marked with “-” indicate an empty task set.
Model CNT SPK SEM DEG PRL AUD
BERT-GSLM 66.3 45.0 47.1 68.2 52.7 -
Whisper 95.3 55.6 54.9 71.1 49.4 -
ImageBind-LLM 64.3 42.1 47.6 78.7 59.8 -
Whisper-LLM 77.6 92.3 55.5 91.0 66.3 -
ASR-ChatGPT - - - - - -
Random 49.9 54.1 41.0 45.9 67.1 -
Table 3 : Average accuracy (%) of the proposed baselines on unseen
tasks in Dynamic-SUPERB across different dimensions.
Model CNT SPK SEM DEG PRL AUD
BERT-GSLM 0.0 32.8 5.3 41.6 12.6 0.0
Whisper 14.4 58.0 13.8 55.4 8.5 0.8
ImageBind-LLM 15.7 45.4 24.7 47.6 20.6 35.7
Whisper-LLM 8.7 60.6 20.9 59.0 6.6 15.9
ASR-ChatGPT 65.0 40.3 70.0 43.5 22.9 9.8
Random 11.8 50.2 33.1 43.1 21.0 23.4
for computational efﬁciency. During training, ImageBind r emained
ﬁxed, and only adapter modules in LLaMA were updated.
While ImageBind-LLM shows promise, we observed that its
utterance-level encoding overlooks the temporal nuances o f speech.
Therefore we introduced Whisper-LLM to address this limita tion.
We substituted ImageBind with the Whisper encoder to better cap-
ture temporal information. The training process was simila r to that
of ImageBind-LLM, with the exception of an added downsampli ng
step to reduce the sequence length of speech features.
3.4. ASR-ChatGPT
The above models require re-training. We also adopted a conc atena-
tive framework using pre-trained models, as demonstrated i n a pre-
vious study [44]. Speciﬁcally, we used Whisper to transcrib e the
speech and then merge it with the instruction to guide ChatGP T in
responding. However, we found responses from ChatGPT vary. For
example, an answer “A” might be elaborated as “The answer is A .”
To ensure ChatGPT adheres to our requirements, we added a pro mpt:
“Please select one label from the provided options and respo nd with
that label only.” Empirically, this constrains ChatGPT eff ectively.
4. RESULTS
In this section, we present a preliminary study of the baseli ne mod-
els. Speciﬁcally, we ﬁrst discuss how the models perform on s een
tasks (Sec. 4.1 & Sec. 4.2), and then examine their capabilit y of gen-
eralization towards unseen tasks (Sec. 4.3).
4.1. Seen tasks
Table 2 presents the evaluation results for each baseline on seen tasks
in Dynamic-SUPERB. Due to space constraints, we report the a ver-
age accuracy for each dimension. Detailed accuracy for each task
is available on our website1. We also included the random baseline
for comparison, which was obtained by randomly selecting an swers
from the label distribution of each task. Here, we discuss th e perfor-
mance on seen tasks , which were used in both training and testing.
ASR-ChatGPT is included for comparison (Table 3), even thou gh
the term “seen tasks” does not apply to it. The audio (AUD) dim en-
sion is not discussed here because there are no seen tasks.
Overall, GSLM performed poorly. While the best performance
of GSLM was in the degradation dimension (DEG), its accuracy only
surpassed that of ASR-ChatGPT. We believe this is likely bec ause
GSLM has fewer parameters than other models, and using discr ete
speech tokens may discard crucial information required for these
tasks. Whisper showed an outstanding accuracy in the conten t di-
mension (CNT). This is probably because Whisper is primaril y an
ASR model, contributing to its high accuracy in processing c on-
tent information. ImageBind-LLM had a slightly better perf ormance
than GSLM. Despite using utterance-level embeddings, it ac hieved
much higher accuracy than the random baseline in CNT, which w e
believed would require the use of temporal information. Thi s sug-
gests that, as of now, some tasks in this dimension do not nece s-
sitate detailed temporal information. We hope the communit y will
contribute more diverse tasks to enrich it. Whisper-LLM dom inated
in most dimensions except CNT. Particularly, it achieved ve ry high
accuracy in the speaker (SPK) and degradation (DEG) dimensi ons.
The performance difference between Whisper and Whisper-LL M
suggests that while the Whisper encoder provides abundant i nforma-
tion, the way we utilize this information signiﬁcantly impa cts perfor-
mance (comparing the Whisper decoder to LLaMA). ASR-ChatGP T
excelled in the semantics dimension (SEM) and had a much high er
accuracy than all the other baselines. Conversely, it faile d in those
involving several speech characteristics (SPK, DEG, and PR L) be-
cause the ASR process discarded the essential information.
4.2. Seen & unseen instructions
For seen tasks, despite being utilized during the training s tage, cer-
tain instructions were reserved exclusively for the testin g. We then
analyzed how baseline models performed from the view of seen
and unseen instructions. Table 4 lists the performance comp arison
based on seen/unseen instructions. The audio dimension (AU D) is
excluded as it was not utilized during training. All the base lines
followed the same trend. In CNT, SPK, and SEM, there was lit-
tle difference between the seen and unseen instructions. Fo r DEG
and PRL, the unseen instructions had a greater impact, leadi ng to
decreased accuracies. In the two dimensions, there was a rel atively
serious imbalance between the seen and unseen instructions , so the
average accuracy was inﬂuenced much by a speciﬁc task that mo d-
els did not perform well on. In our preliminary study, we ende avored
to provide as many instructions as possible for each task. Ho wever,
in real-world scenarios, there can be hundreds or even thous ands of
ways to formulate instructions speciﬁc to a task. Achieving such di-
versity is unlikely with just a small group of researchers or solely
with tools like ChatGPT. This underscores the importance of com-
munity collaboration to diversify and enhance the benchmar k.
4.3. Unseen tasks
Table 3 reveals a signiﬁcant performance gap between seen an d un-
seen tasks across all models. While baseline models perform ed rea-
sonably on seen tasks, their performance declined on unseen tasks.
In typical classiﬁcation tasks, a random guess serves as the baseline
for the lowest performance, but this is not the case here. Alt hough
the instructions explicitly outlined available options, t he model re-
sponded in a generative manner. It could produce answers tha t wereTable 4 : Average accuracy (%) across different dimensions based on
seen/unseen instructions. Only seen tasks were evaluated.
Model Instruction CNT SPK SEM DEG PRL
BERT-GSLMSeen 66.5 44.2 47.1 68.2 69.3
Unseen 65.9 45.4 47.1 60.6 54.6
WhisperSeen 95.4 55.3 54.6 71.0 65.8
Unseen 95.0 55.4 55.6 50.5 49.6
ImageBind-LLMSeen 64.1 42.9 47.1 78.6 75.8
Unseen 64.8 41.6 49.0 62.8 59.7
Whisper-LLMSeen 77.3 92.5 55.0 91.1 84.7
Unseen 78.1 92.1 56.6 84.9 66.4
not among the given options, leading to a performance that wa s
even worse than what a random guess would achieve. BERT-GSLM
and Whisper struggled to select from the provided options, w hich
is evidenced by their accuracy falling below the expected ra ndom
baseline. Notably, BERT-GSLM had a very low accuracy of 0.0%
on unseen tasks in CNT. On the other hand, ImageBind-LLM and
Whisper-LLM seemed to make selections that appeared almost ran-
dom from the instructions, which resulted in an accuracy tha t hov-
ered around the random baseline. These indicate that while L LMs
might not always perform a speciﬁc task as instructed, they e xhibit
an understanding of how options are presented in natural lan guage.
This proﬁciency likely stems from their pre-training on lar ge-scale
text data. Conversely, speech models, without text pre-tra ining and
trained on limited data, could not achieve the same proﬁcien cy.
Additionally, we observed that most models underperformed in
AUD. This can be attributed both to its absence during the tra ining
and the intrinsic distinction between speech and audio sign als. Con-
versely, ImageBind-LLM exhibited superior accuracy becau se of its
pre-training on several data modalities, including audio.
From our analysis of seen/unseen instructions (Sec. 4.2) an d
tasks (this section), we surmise that these models could not gen-
uinely perform speciﬁc tasks based on the semantic informat ion in
the instructions. Instead, they often performed tasks by re cogniz-
ing speciﬁc patterns in the instructions, such as a bag-of-w ords ap-
proach. Within a task, despite variations in instructions, they might
possess a similar bag of words due to their design aimed at spe ci-
fying the same task. On the other hand, different tasks have u nique
instructions (and patterns), and the model might fail if the se were
not used in training. This explains why these models demonst rated
notable performance on seen tasks with unseen instructions but fal-
tered in generalizing effectively to unseen tasks. The inst ructions for
unseen tasks have very different patterns than those for see n tasks.
5. CONCLUSIONS
Instruction tuning is increasingly popular for enabling ze ro-shot
applications in NLP, but it remains underexplored in speech pro-
cessing. This paper presents Dynamic-SUPERB, the ﬁrst dyna mic
and collaborative benchmark for instruction tuning in spee ch mod-
els, offering a comprehensive exploration across diverse d imensions.
Five different approaches are proposed and tested on the ben chmark,
showing encouraging results on seen tasks. However, their p erfor-
mance on unseen tasks highlights the need for continued rese arch
in this ﬁeld. We open-source all the materials to lower the ba rrier
for reproduction, benchmarking, and analysis of instructi on tuning
in speech processing. We welcome researchers to join our act ive
community and drive the research frontier together.
6. REFERENCES
[1] Aaron van den Oord, Yazhe Li, and Oriol Vinyals, “Rep-
resentation learning with contrastive predictive coding, ”
arXiv:1807.03748 , 2018.
[2] Alexei Baevski et al., “wav2vec 2.0: A framework for self -
supervised learning of speech representations,” NeurIPS , 2020.
[3] Wei-Ning Hsu et al., “Hubert: Self-supervised speech re p-
resentation learning by masked prediction of hidden units, ”
IEEE/ACM Transactions on Audio, Speech, and Language
Processing , 2021.
[4] Abdelrahman Mohamed et al., “Self-supervised speech re pre-
sentation learning: A review,” IEEE Journal of Selected Topics
in Signal Processing , 2022.
[5] Kai-Wei Chang et al., “An Exploration of Prompt Tuning
on Generative Spoken Language Model for Speech Processing
Tasks,” in INTERSPEECH , 2022.
[6] Kai-Wei Chang et al., “Speechprompt v2: Prompt tuning fo r
speech classiﬁcation tasks,” arXiv:2303.00733 , 2023.
[7] Haibin Wu et al., “Speechgen: Unlocking the generative p ower
of speech language models with prompts,” arXiv:2306.02207 ,
2023.
[8] Xiang Lisa Li and Percy Liang, “Preﬁx-tuning: Optimizin g
continuous prompts for generation,” in ACL, 2021.
[9] Xiao Liu et al., “P-tuning: Prompt tuning can be comparab le
to ﬁne-tuning across scales and tasks,” in ACL, 2022.
[10] Xiao Liu et al., “Gpt understands, too,” AI Open , 2023.
[11] Brian Lester, Rami Al-Rfou, and Noah Constant, “The pow er
of scale for parameter-efﬁcient prompt tuning,” in EMNLP ,
2021.
[12] Taylor Shin et al., “Autoprompt: Eliciting knowledge f rom
language models with automatically generated prompts,” in
EMNLP , 2020.
[13] Puyuan Peng et al., “Prompting the Hidden Talent of Web-
Scale Speech Models for Zero-Shot Task Generalization,” in
INTERSPEECH , 2023.
[14] Zih-Ching Chen et al., “Exploring efﬁcient-tuning met hods in
self-supervised speech models,” in SLT, 2023.
[15] Yuan Gong et al., “Listen, think, and understand,”
arXiv:2305.10790 , 2023.
[16] Chin-Lun Fu et al., “Adapterbias: Parameter-efﬁcient token-
dependent representation shift for adapters in nlp tasks,” in
NAACL 2022 , 2022.
[17] Ruidan He et al., “On the effectiveness of adapter-base d tuning
for pretrained language model adaptation,” in ACL, 2021.
[18] Jason Wei et al., “Finetuned language models are zero-s hot
learners,” in ICLR , 2021.
[19] Eliya Nachmani et al., “Lms with a voice: Spoken languag e
modeling beyond speech tokens,” arXiv:2305.15255 , 2023.
[20] Paul K Rubenstein et al., “Audiopalm: A large language m odel
that can speak and listen,” arXiv:2306.12925 , 2023.
[21] Rongjie Huang et al., “Audiogpt: Understanding and
generating speech, music, sound, and talking head,”
arXiv:2304.12995 , 2023.[22] Tu Anh Nguyen et al., “The zero resource speech benchmar k
2021: Metrics and baselines for unsupervised spoken langua ge
modeling,” arXiv:2011.11588 , 2020.
[23] Shu wen Yang et al., “SUPERB: Speech Processing Univers al
PERformance Benchmark,” in INTERSPEECH , 2021.
[24] Suwon Shon et al., “Slue: New benchmark tasks for spo-
ken language understanding evaluation on natural speech,” in
ICASSP , 2022.
[25] Sol` ene Evain et al., “ LeBenchmark: A Reproducible Fra me-
work for Assessing Self-Supervised Representation Learni ng
from Speech,” in Interspeech , 2021.
[26] Sanchit Gandhi, Patrick V on Platen, and Alexander M Rus h,
“Esb: A benchmark for multi-domain end-to-end speech
recognition,” arXiv:2210.13352 , 2022.
[27] Alexis Conneau et al., “Fleurs: Few-shot learning eval uation
of universal representations of speech,” in SLT, 2023.
[28] Jiatong Shi et al., “Ml-superb: Multilingual speech un iversal
performance benchmark,” arXiv:2305.10615 , 2023.
[29] Guan-Ting Lin et al., “On the utility of self-supervise d models
for prosody-related tasks,” in SLT, 2023.
[30] Jacob Devlin et al., “BERT: Pre-training of deep bidire ctional
transformers for language understanding,” in NAACL , 2019.
[31] Kushal Lakhotia et al., “On generative spoken language model-
ing from raw audio,” Transactions of the Association for Com-
putational Linguistics , 2021.
[32] Alec Radford et al., “Robust speech recognition via lar ge-scale
weak supervision,” in ICML , 2023.
[33] Rohit Girdhar et al., “Imagebind: One embedding space t o
bind them all,” in CVPR , 2023.
[34] Hugo Touvron et al., “Llama: Open and efﬁcient foundati on
language models,” arXiv:2302.13971 , 2023.
[35] Hugo Touvron et al., “Llama 2: Open foundation and ﬁne-
tuned chat models,” arXiv:2307.09288 , 2023.
[36] OpenAI, “Chatgpt (august 3 version),” Large language m odel,
2023.
[37] Vassil Panayotov et al., “Librispeech: an asr corpus ba sed on
public domain audio books,” in ICASSP , 2015.
[38] Keith Ito and Linda Johnson, “The lj speech dataset,”
https://keithito.com/LJ-Speech-Dataset/ ,
2017.
[39] Junichi Yamagishi, Christophe Veaux, and Kirsten MacD on-
ald, “CSTR VCTK Corpus: English multi-speaker corpus for
CSTR voice cloning toolkit (version 0.92),” 2019.
[40] Yuan Gong et al., “Whisper-AT: Noise-Robust Automatic
Speech Recognizers are Also Strong General Audio Event Tag-
gers,” in INTERSPEECH , 2023.
[41] Jiaming Han et al., “Imagebind-llm: Multi-modality in struc-
tion tuning,” arXiv:2309.03905 , 2023.
[42] Peng Gao et al., “Llama-adapter v2: Parameter-efﬁcien t visual
instruction model,” arXiv:2304.15010 , 2023.
[43] Renrui Zhang et al., “Llama-adapter: Efﬁcient ﬁne-tun ing of
language models with zero-init attention,” arXiv:2303.16199 ,
2023.
[44] Mutian He and Philip N. Garner, “Can ChatGPT Detect Inte nt?
Evaluating Large Language Models for Spoken Language Un-
derstanding,” in INTERSPEECH , 2023.