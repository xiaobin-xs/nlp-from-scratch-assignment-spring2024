Learning to Speak from Text: Zero-Shot Multilingual Text-to-Speech with
Unsupervised Text Pretraining
Takaaki Saeki1,Soumi Maiti2,Xinjian Li2,Shinji Watanabe2,
Shinnosuke Takamichi1andHiroshi Saruwatari1
1The University of Tokyo, Japan
2Carnegie Mellon University, USA
takaaki saeki@ipc.i.u-tokyo.ac.jp, {smaiti, swatanab }@andrew.cmu.edu
Abstract
While neural text-to-speech (TTS) has achieved
human-like natural synthetic speech, multilingual
TTS systems are limited to resource-rich languages
due to the need for paired text and studio-quality
audio data. This paper proposes a method for zero-
shot multilingual TTS using text-only data for the
target language. The use of text-only data allows
the development of TTS systems for low-resource
languages for which only textual resources are
available, making TTS accessible to thousands of
languages. Inspired by the strong cross-lingual
transferability of multilingual language models, our
framework first performs masked language model
pretraining with multilingual text-only data. Then
we train this model with a paired data in a super-
vised manner, while freezing a language-aware em-
bedding layer. This allows inference even for lan-
guages not included in the paired data but present in
the text-only data. Evaluation results demonstrate
highly intelligible zero-shot TTS with a character
error rate of less than 12% for an unseen language.
1 Introduction
Recent advances in end-to-end neural text-to-speech synthe-
sis (TTS) [Liet al. , 2019b; Kim et al. , 2021 ]have yielded
significant improvements in naturalness and speech qual-
ity. However, the data-intensive nature and the requirement
of paired text and studio-quality audio data have limited
multilingual TTS systems to resource-rich languages, which
are small portions of the more than 6,000 languages in the
world [Jr, 2005 ]. To address the limitation, current research in
multilingual TTS aims not only to exploit resource-rich lan-
guages [Zenet al. , 2012; Li and Zen, 2016 ]but also to build
models for low-resource languages [Prakash et al. , 2019 ].
Previous work has addressed low-resource TTS by using
untranscribed speech data with vector-quantized variational
autoencoder (VQ-V AE) [Zhang and Lin, 2020 ]or automatic
speech recognition (ASR) models [Niet al. , 2022 ]. Another
study [Saeki et al. , 2022b ]has built a massively multilin-
gual TTS model jointly using paired TTS, paired ASR, un-
paired speech, and unpaired text data. However, these ap-
proaches still rely on speech data for the target languages and
Paired data for  
language AText data for
language CMultilingual TTS
TrainingSynthesis
Paired data for  
language B
Figure 1: Our concept. We aim to build TTS model on languages for
which only text data is available, to support low-resource languages.
face the challenge of data collection, when audio recordings
for these languages are hard to obtain. In this study, we fo-
cus on the use of a text-only data for multilingual TTS as
shown in Fig. 1. Previous research [Wu and Dredze, 2019;
Pires et al. , 2019 ]has shown the strong cross-lingual trans-
ferability of multilingual language models such as multilin-
gual BERT [Devlin et al. , 2019 ]in natural language pro-
cessing (NLP) tasks. By leveraging multilingual pretrain-
ing, the model can generalize to other languages, even if
it has never seen the target data in those languages. Our
work applies the framework of multilingual masked language
model (MLM) pretraining to TTS, with the goal of achieving
zero-shot cross-lingual transfer of pronunciation and prosody.
Zero-shot TTS using text data enables the development of
TTS systems for languages where only textual resources are
available, which potentially opens up TTS to thousands of
languages [Ebrahimi and Kann, 2021; Li et al. , 2022 ].
In this paper, we propose a multilingual TTS framework
that leverages unsupervised text pretraining. Fig. 2 illus-
trates the proposed framework. We use a typical end-to-end
TTS architecture consisting of token embedding, encoder,
and decoder. Our model also has a language-aware embed-
ding layer, which includes the token embedding layer, a lan-
guage embedding layer, and a bottleneck layer. As shown
in Fig. 2(a), we first pretrain the language-aware embedding
layer and the encoder of the TTS model with multilingual text
data. We then fine-tune the encoder and decoder of the TTS
model with paired data, while the language-aware embedding
layer is frozen, as illustrated in Fig. 2(b). This allows zero-
shot TTS for a language not included in the paired data but
present in the text data, as shown on the right in Fig. 2(c).
Our contributions are as follows. 1) We propose a zero-shotarXiv:2301.12596v3  [eess.AS]  27 May 2023
Bytes or IP A<Mask>Language-aware
embedding layerEncoder
Bytes or IP ALanguage-aware
embedding layerEncoderDecoder
Bytes or IP ALanguage-aware
embedding layerEncoderDecoder(a) Unsupervised multilingual text pretraining
 (b) Supervised learning with paired data
(c) Inferencede ru
hu ...
Textes de
ru hu
en sk
ro  ...
Text
es
Bytes or IP ALanguage-aware
embedding layerEncoderDecoder
de ru
hu ...
Multilingual TTS for seen languages Zero-shot TTS for unseen languageFreeze
Text TextFigure 2: Proposed framework. (a) We perform MLM pretraining on
multilingual text data and then (b) train TTS model on paired data
with frozen language-aware embedding layer. (c) Zero-shot TTS is
performed with language IDs that are not included in paired data.
multilingual TTS framework that achieves highly intelligible
TTS for an unseen language, resulting in a character error rate
of less than 12%. 2) Our method also improves TTS for seen
languages, resulting in byte-based models without grapheme-
to-phoneme (G2P) modules that outperform the phoneme-
based baselines. 3) Our ablation studies provide additional
insights, including the effectiveness of the frozen language-
aware embedding layer. The experiments were conducted on
public datasets and the implementation is available1. We en-
courage readers to listen to our audio samples2.
2 Method
Our model has a typical neural TTS model architecture con-
sisting of token embedding, encoder, and decoder. First, we
use MLM pretraining with multilingual text data to learn
cross-lingual representations. Then we perform supervised
learning with paired data to learn the mapping from linguis-
tic features to speech features. The model performs inference
even for languages that are not present in the paired data.
2.1 Unsupervised Multilingual Text Pretraining
Fig. 2(a) illustrates the unsupervised pretraining method. It
uses multilingual text data consisting of languages that are
not included in the paired data. Let X= (xn‚ààV|n=
1,¬∑¬∑¬∑, N)denote the input text token sequence of length N,
where Vdenotes a vocabulary constructed for pretraining.
1https://github.com/Takaaki-Saeki/zm-text-tts
2https://takaaki-saeki.github.io/zm-tts-text demoWe define Dtextas the text dataset. Let Ltextdenote the set
of language IDs included in Dtext. First, the masked token
sequence Xmand a language ID ltext‚ààLtextare fed to the
model. Let the token embedding sequence and language em-
bedding be Zm= (zm
n‚ààRd|n= 1,¬∑¬∑¬∑, N)andel‚ààRd,
respectively. The embedding layers output Zmandelas:
Zm=Embed (Xm;Œ∏T), el=Embed (ltext;Œ∏L),(1)
where Œ∏TandŒ∏Ldenote the model parameters of the to-
ken embedding and language embedding layers, respectively.
Then the token and language embeddings obtained in Eq. (1)
are added and fed to a bottleneck layer to project them into a
hidden input vector. Let Hin= (hin,n‚ààRd|n= 1,¬∑¬∑¬∑, N)
andHout= (hout,n‚ààRd|n= 1,¬∑¬∑¬∑, N)denote hidden
vectors in the encoder input and output, respectively. Then
the conditional probability p(X|X‚àíŒ†)is computed as:
Hin=Bottleneck (Zm+el;Œ∏B), (2)
Hout=Encoder (Hin;Œ∏E), (3)
p(X|X‚àíŒ†) =Softmax (PredictionNet (Hout;Œ∏P)),(4)
where Œ∏B,Œ∏E,Œ∏Pdenote the model parameters of the bot-
tleneck layer, the encoder and a prediction network, respec-
tively. In Eq. (4), Softmax (¬∑)denotes a softmax function. We
define the network with the model parameters {Œ∏B, Œ∏T, Œ∏L}
aslanguage-aware embedding layer , which jointly embeds
the token sequence Xand the language ID ltextas in Eq. (1)
and (2). Let Œ† = ( œÄk‚ààN|k= 1,¬∑¬∑¬∑, K)be the indexes
of the masked tokens of length K. With the probability com-
puted in Eq. (4), the training objective can be defined as:
Lmlm=1
KKX
k=1logp(xœÄk|Xm),
{ÀÜŒ∏E,ÀÜŒ∏B,ÀÜŒ∏T,ÀÜŒ∏L}= arg min
Œ∏E,Œ∏B,Œ∏T,Œ∏LLmlm.(5)
We use UTF-8 bytes or International Phonetic Alphabet
(IPA) symbols for the input token sequence X. For each to-
ken type, the vocabulary Vis constructed from Dtext, which
includes a start/end of sentence token ([ SOS/EOS ]). We ex-
tracted International IPA sequences using an open-source
toolkit3. To obtain the masked token Xm, we use the same
masking ratio and category as in the original BERT pre-
training [Devlin et al. , 2019 ]for each token type. Randomly,
12 % of the tokens are replaced with the [ MASK ] token, and
1.5 % of them are replaced with random tokens. Also, 1.5 %
of the tokens are left unchanged and Lmlmis computed as in
Eq. (5) for those 15 % of tokens that have indices Œ†.
2.2 Supervised Learning with Paired Data
Fig. 2(b) illustrates the supervised learning of the TTS model
with paired data. We define the paired data and the set of lan-
guage IDs as Dpaired andLpaired , respectively. Note that we
assume Lpaired‚äÇLtext. Let Y= (yt‚ààRD|t= 1,¬∑¬∑¬∑, T)
denote the speech feature sequence with the length of T.
We first initialize the model parameters {Œ∏E, Œ∏B, Œ∏T, Œ∏L}with
3https://github.com/espeak-ng/espeak-ng
those obtained in the pretraining described in ¬ß 2.1. Let Œ∏D
denote the model parameter of the decoder. The speech fea-
tures are predicted with teacher forcing as:
Hout=Encoder (Bottleneck (Z+el)), (6)
ÀÜY=Decoder (Hout, Y;Œ∏D), (7)
where Zis the unmasked token embedding sequence. Note
that the unmasked token sequence is used in Eq. (6), while
the masked token sequence is used in Eq. (2) Let Ltts(ÀÜY , Y)
denote the training objective of the TTS model. Then we
consider two types of schemes.
Updating language-aware embedding layer We only
freeze the parameter of the language embedding layer Œ∏L
while updating the rest of the parameters. Therefore the train-
able model parameters can be written as
{ÀÜŒ∏D,ÀÜŒ∏E,ÀÜŒ∏B,ÀÜŒ∏T}= arg min
Œ∏D,Œ∏E,Œ∏B,Œ∏TLtts(ÀÜY , Y). (8)
Previous work has confirmed that multilingual BERT has
high cross-lingual transferability for various NLP tasks [Wu
and Dredze, 2019 ]. This scheme corresponds to a simple fine-
tuning of BERT [Wu and Dredze, 2019 ], which updates all
the parameters during training for the downstream tasks4.
Freezing language-aware embedding layer We freeze the
bottleneck layer and the token embedding layer along with
the language embedding, updating the encoder and decoder.
The training process can be written as
{ÀÜŒ∏D,ÀÜŒ∏E}= arg min
Œ∏D,Œ∏ELtts(ÀÜY , Y). (9)
In contrast to the scheme represented in Eq. (8), the scheme in
Eq. (9) preserves the parameters of the language-aware em-
bedding layer to facilitate cross-lingual transfer. In the evalu-
ation, we use the scheme formulated in Eq. (9), except for the
ablation study in ¬ß 3.4.
2.3 Inference
LetLsyndenote the set of language IDs used for inference.
The text token sequence Xand the language ID lsyn‚ààLsyn
are fed to the model as in Eq. (1), and the encoder output is
predicted as in Eq. (6). Unlike Eq. (7), the speech features are
predicted as:
ÀÜY=Decoder (Hout;Œ∏D). (10)
The output waveform is obtained by feeding the predicted
features ÀÜYto a pretrained neural vocoder.
Fig. 2(c) illustrates the inference process. The left and right
sides of the figure show the typical multilingual TTS and our
zero-shot TTS. Previous work [Liet al. , 2019a ]has typically
assumed seen languages, and the inference is performed with
the language IDs Lseen‚äÇLpaired . However, it is challenging
to perform TTS for unseen languages Lunseen‚à©Lpaired =‚àÖ.
While other work [Saeki et al. , 2022b ]has built a massively
multilingual TTS model that even achieves zero-shot TTS
from ASR data, it uses paired data for the target languages.
4We freeze the language embedding layer to address the mis-
match between language embedding of seen and unseen languages.Our work attempts to only use the linguistic knowledge to im-
prove the zero-shot TTS. Thus, the inference process is writ-
ten as L‚Ä≤
unseen ‚à©Lpaired =‚àÖandL‚Ä≤
unseen ‚äÇLtext. In the
evaluation, we denote the inference with Lunseen andL‚Ä≤
unseen
asFully zero-shot TTS andText-seen zero-shot TTS , respec-
tively. Fully zero-shot TTS performs zero-shot TTS without
pretraining as in the IPA-based previous method [Staib et al. ,
2020 ], which is the baseline method in our evaluations.
2.4 Model Architecture
Our model is an autoregressive TTS model based on Trans-
former TTS [Liet al. , 2019b ], which has also been used in
the previous work on byte-based multilingual TTS [Heet al. ,
2021 ]. During the supervised learning described in ¬ß 2.2 and
inference described in ¬ß 2, we use x-vector [Snyder et al. ,
2018 ]for the speaker embedding and add it to the encoder
output through a projection layer. During supervised learn-
ing, we use the average x-vectors computed from the training
data. For evaluation purposes, we perform zero-shot synthe-
sis with the average x-vector from the test data of the target
language and feed it to the model. Note that we also conduct
the evaluation with x-vectors from seen languages.
For the bottleneck layer with Œ∏B, we use a residual network
consisting of Layer Normalization [Baet al. , 2016 ], down
projection, ReLU [Nair and Hinton, 2010 ], and up projection
with the residual connection, which is used in previous work
on language adaptation [Bapna and Firat, 2019 ].
3 Experimental Evaluations
3.1 Experimental Setting
Dataset
We carried out all the evaluations with publicly available
datasets. Table 1 shows the sizes of the data for each lan-
guage. For the unsupervised text pretraining described in
¬ß 2.1, we used transcripts from V oxPopuli [Wang et al. ,
2021 ], M-AILABS [Munich Artificial Intelligence Labora-
tories GmbH, 2017 ], and CSS10 [Park and Mulc, 2019 ], re-
sulting in a total of about 2.8 GB of spoken text across 19
languages. We used CSS10 for the supervised learning de-
scribed in ¬ß 2.2, and we selected seven European languages
as the seen languages, with Spanish as the unseen language.
The paired data consisted of one speaker per language. It
should be noted that Spanish is not actually a low-resource
language, but we chose to use it for evaluation purposes in
order to 1) compare our zero-shot TTS methods with the ora-
cle methods using the paired data for the target language and
2) ensure a sufficient number of evaluators for the subjective
evaluation. We used 5 and 100 utterances as dev and test sets,
respectively, with the remaining data used for training.
Training Details
The sampling rate was set to 16 kHz. An 80-dimension
of mel filter bank, 1024 samples of FFT length, and 256
samples of frame shit were used for speech analysis. For
the pretraining described in ¬ß 2.1, we trained the model for
1.2M iterations using the Noam optimizer [Vaswani et al. ,
2017 ]with the learning rate and warm-up step set to 1.0
and 10000, respectively. For the TTS model described in
Languages Code Text-only dataPaired data
Text Audio
Seen languages for evaluation Lseen
German de 359MB 0.73MB 16.13h
French fr 372MB 0.94MB 19.15h
Dutch nl 336MB 0.75MB 14.10h
Finnish fi 308MB 0.47MB 21.36h
Hungarian hu 104MB 0.51MB 10.53h
Russian ru 4.9MB 1.5MB 10.00h
Greek el 0.39MB 0.39MB 4.13h
Unseen language for evaluation Lunseen
Spanish es 345MB 0.0MB (1.2MB) 0.00h (23.81h)
Languages not included in CSS10
English en 338MB
Estonian et 87MB
Croatian hr 2.0MB
Italian it 334MB
Lithuanian lt 89MB
Polish pl 102MB
Romanian ro 67MB
Slovak sk 94MB
Slovenian sl 81MB
Table 1: Amount of text-only and paired data for each language.
Parentheses indicate amount of original data in CSS10.
¬ß 2.4, we used a 6-block Transformer encoder [Vaswani et
al., 2017 ]and a 6-block Transformer decoder, with a post-
net consisting of five convolutional layers with a kernel size
of five. The attention dimension and the number of atten-
tion heads were set to 512 and 8, respectively. For the bot-
tleneck layer described in ¬ß 2.4, we set the hidden dimen-
sion after the down projection to 256. The PredictionNet in
Eq. (4) consisted of a linear layer, a GELU activation func-
tion [Hendrycks and Gimpel, 2016 ], Layer Normalization,
and a linear layer with the hidden dimension of 512. We
also used guided attention loss [Tachibana et al. , 2018 ]to
improve the training efficiency. For the supervised learn-
ing described in ¬ß 2.2, we trained the models for 2.47M it-
erations (200 epochs). The Noam optimizer was used with
the warm-up step of 50000. For the neural vocoder, we
trained HiFi-GAN [Kong et al. , 2020 ]for 2M iterations
with LibriTTS [Zen et al. , 2019 ], VCTK [Veaux et al. ,
2017 ], and CSS10. For the x-vector described in ¬ß 2.4, we
used a model trained on V oxCeleb1 and V oxCeleb2 [Na-
grani et al. , 2017 ]published in SpeechBrain [Ravanelli et
al., 2021 ]. We used ESPnet2-TTS [Watanabe et al. , 2018;
Hayashi et al. , 2021 ]for the implementation.
Baselines
We developed baseline models without the pretraining.
Seen language Monolingual: We trained a model for each
language independently. Our preliminary study found that
Transformer TTS was unstable5and could not synthesize in-
telligible speech in the monolingual condition due to the lack
of training data. Therefore, we used Tacotron2 [Shen et al. ,
2018 ]only for the monolingual models, as in the original pa-
per of the dataset [Park and Mulc, 2019 ].Multilingual w/o
LIDs: We trained a multilingual Transformer TTS model us-
ing the paired data shown in Table 1 without language IDs
5The original paper [Liet al. , 2019b ]also reports the instability.(LIDs). Multilingual w/ LIDs: We trained a multilingual
Transformer TTS model with the paired data of the unseen
language. It also used the language IDs.
Unseen language We compared Fully zero-shot TTS and
Text-seen zero-shot TTS defined in ¬ß 2.3. In Oracle , we used
theMonolingual andMultilingual w/ LIDs , which used the
paired data of the unseen language. In Fully zero-shot TTS ,
we used Multilingual w/o LIDs to synthesize speech from text
tokens in the unseen language. This method corresponds to
the conventional multilingual TTS model using bytes [Heet
al., 2021 ]or IPA symbols [Staib et al. , 2020 ].
Evaluation Metrics
To objectively measure the synthetic speech quality, we used
mel cepstral distortion (MCD) [Fukada et al. , 1992 ]with
the mel cepstrum dimension set to 25. We also evaluated
the intelligibility using CERs computed with a multilingual
ASR model [Radford et al. , 2022 ]. We used a pretrained
large model that is publicly available6. To evaluate the nat-
uralness, we carried out listening tests to calculate five-scale
mean opinion scores (MOS) of synthesized speech for each
method. Forty native speakers were recruited through Ama-
zon Mechanical Turk [Paolacci et al. , 2010 ]for each of the
tests. Furthermore, we leveraged a publicly available auto-
matic MOS (AMOS) prediction model [Saeki et al. , 2022a ]
to evaluate the naturalness. Note that the model was trained
on English and Chinese datasets, but previous work [Seki et
al., 2022 ]has reported that it also showed a correlation coef-
ficient higher than 0.8 for another language (Japanese).
3.2 Evaluation Results on Seen Languages
We evaluated our framework on the seen languages included
in the paired data, as defined in ¬ß 2.3. Table 2 lists the results
in MCD and CER. Lower values are better for both metrics.
As we can see, the byte-based or IPA-based models with the
proposed multilingual pretraining performed the best across
all languages and metrics. Among the baselines, byte-based
monolingual and multilingual models tended to have higher
MCD and CER than IPA-based models, and failed to synthe-
size intelligible speech in some languages. For example, the
baseline byte-based models showed the high CER values for
French, which has a deep orthography, meaning that a single
character has different pronunciations depending on the con-
text. We observed that our method improved the byte-based
models and they outperformed the IPA-based baseline models
for all the metrics and languages. It is worth noting that the
proposed byte-based models even outperformed the proposed
IPA-based models except for el and ru. These results suggest
that our framework is effective in building a TTS model for
languages without G2P modules.
3.3 Evaluation Results on Unseen Language
We evaluated our method on zero-shot TTS for the unseen
language defined in ¬ß 2.3. As described in ¬ß 2.4, we first
used the x-vector from the es speaker to compute the MCD.
Table 3 lists the results. The baseline models showed the
CERs of over 40% and MCDs of over 10.0. However, our
6https://github.com/openai/whisper
Methodde fr ru fi hu nl el
MCD CER MCD CER MCD CER MCD CER MCD CER MCD CER MCD CER
Natural - 2.75 - 4.52 - 2.12 - 4.73 - 4.86 - 6.22 - 7.14
Baseline (Monolingual)
Bytes monolingual 7.70 8.61 11.76 91.82 11.43 >100 8.33 56.03 10.22 93.05 7.49 15.33 10.20 85.98
IPA monolingual 7.38 4.07 8.96 17.86 11.89 25.30 7.23 27.62 7.59 24.62 7.80 19.20 8.16 21.79
Baseline (Multilingual)
Bytes multilingual w/o LIDs 7.68 37.46 8.71 41.35 9.38 45.92 6.26 29.19 6.48 33.82 8.46 46.33 7.64 36.24
Bytes multilingual w/ LIDs 6.51 13.19 10.84 55.79 12.89 >100 6.78 27.22 9.09 42.97 8.47 39.37 7.25 23.56
IPA multilingual w/o LIDs 6.31 10.64 7.44 20.86 8.10 35.32 5.53 19.56 5.59 14.03 7.76 34.49 6.90 19.33
IPA multilingual w/ LIDs 6.16 9.76 6.88 14.97 7.63 23.54 5.17 10.63 5.28 9.11 6.95 19.48 6.90 16.97
Proposed (Unsupervised text pretraining)
Bytes multilingual 5.65 3.79 6.48 7.15 7.38 10.62 4.99 5.28 5.01 6.05 6.52 13.74 6.57 11.75
IPA multilingual 5.88 5.52 6.61 7.72 7.25 15.85 5.18 8.62 5.30 7.37 7.00 14.42 6.53 11.06
Table 2: Evaluation results for seen languages. Bold indicates best scores in baseline and proposed methods.
Methodes
es x-vector fr x-vector
MCD CER CER
Natural - 2.71 2.71
Oracle
Bytes monolingual 8.65 10.70 -
IPA monolingual 8.47 5.28 -
IPA multilingual 6.20 5.32 6.99
Baseline (Fully zero-shot TTS)
Bytes multilingual 11.22 64.07 66.45
IPA multilingual 10.75 44.75 44.37
Proposed (Text-seen zero-shot TTS)
Bytes multilingual 9.05 18.27 13.74
IPA multilingual 9.44 11.69 13.33
Table 3: Evaluation results for unseen language.
proposed text preraining improved the metrics, resulting in
CERs of less than half for both byte and IPA-based methods.
Also, in contrast to the results for the seen languages, the
IPA-based model outperformed the byte-based one in terms
of CER. Compared with the oracle case with the paired data
of the unseen language, our proposed zero-shot TTS showed
higher MCD and CER but achieved only 1% difference in
CER compared to the oracle byte-based monolingual model.
These results demonstrate the effectiveness of our method in
achieving intelligible zero-shot TTS for the unseen language.
To investigate the case where the target speaker informa-
tion is completely unavailable, we also used the x-vector from
a seen language. We chose the fr speaker because es and fr
are both categorized as Western Romance in Glottolog [Ham-
marstr ¬®omet al. , 2021 ]. Table 3 lists the results. Note that
this case does not have the MCD results, since a different
speaker than the ground-truth speech was used. We can see
that the unsupervised text pretraining also improved the zero-
shot performance when using the x-vector from the fr speaker.
In the proposed byte-based model, the cross-lingual x-vector
showed the lower CER. This might result from that the es
x-vector was not present in the training data whereas the fr
x-vector was present in the training data.
(a) Token embedding ùëç(b) Encoder inputs ùêª!"Figure 3: Visualization of token and language embedding. Pairs of
similar languages (es‚Äìfr and de‚Äìnl) are overlapping in token embed-
ding space, while output of bottleneck layer separates them.
3.4 Ablation Study
To further evaluate our method, we conducted several abla-
tion studies. Table 4 lists the results. Bytes multilingual rep-
resents the byte-based proposed method in the evaluation of
¬ß 3.2 and 3.3. Note that it used the frozen language-aware
embedding layer as formulated in Eq. (9). Some additional
studies of our method are also presented in the Appendix.
InW/o bottleneck layer , we excluded the bottleneck layer
and simply added the token and language embedding to ob-
tain the encoder input in Eq. (2). We found that removing
the bottleneck layer led to a performance drop in all the lan-
guages and metrics, with an average increase of 0.53 in MCD
and 4.16% in CER. The largest increase was observed in the
unseen language, with an increase of 1.21 in MCD. This sug-
gests that the bottleneck layer, which projects the token and
language embedding into the hidden input text representation
with nonlinear dimensionality reduction, is effective in im-
proving the generalization for zero-shot TTS.
We also evaluated the effect of including language IDs in
the proposed method by comparing it with a version that ex-
cluded language IDs, referred to as W/o language ID . It cor-
responds to a simple multilingual BERT pretraining [Wu and
Dredze, 2019 ]that uses only text tokens across different lan-
guages. We observed that the use of language IDs led to an
MethodSeen UnseenAvg.de fr ru fi es
MCD CER MCD CER MCD CER MCD CER MCD CER MCD CER
Bytes multilingual 5.65 3.79 6.48 7.15 7.38 10.62 4.99 5.28 9.05 18.27 6.46 9.58
W/o bottleneck layer 6.06 5.01 7.15 9.09 7.71 28.52 5.33 6.47 10.26 24.01 6.99 13.74
W/o language ID 6.07 5.09 7.09 9.99 7.77 22.58 5.23 6.99 10.45 32.70 6.96 14.06
W/o initializing encoder 5.59 3.75 6.52 9.31 7.12 16.47 4.86 5.03 9.02 21.91 6.42 11.85
Updating language-aware embedding layer 6.05 6.22 6.75 6.93 7.46 11.42 5.16 8.00 9.48 17.21 6.75 10.62
Table 4: Ablation studies on training and model configurations. Bold indicates best metrics on average (Avg.).
11.522.533.544.5
de (MOS)fr (MOS)
de (AMOS)
fr (AMOS)
ru (AMOS)
fi (AMOS)
hu (AMOS)
nl (AMOS)
el (AMOS)
Avg (AMOS)11.522.533.544.5
defrNaturalBaseline (IPA monolingual)Baseline (Bytes multilingual w/o LIDs)Baseline (IPA multilingual w/o LIDs)Baseline (IPA multilingual w/ LIDs)Proposed (Bytes multilingual)Proposed (IPA multilingual)
Figure 4: MOS and AMOS results for seen languages. Error bars in MOS results represent 95% confidence intervals.
Methodde hu
MCD CER MCD CER
Natural - 2.75 - 2.12
Oracle
IPA monolingual 7.38 4.07 7.59 24.62
IPA multilingual 6.16 9.76 5.28 9.11
Baseline (Fully zero-shot TTS)
IPA multilingual 10.31 38.75 9.93 52.62
Proposed (Text-seen zero-shot TTS)
Bytes multilingual 10.00 28.01 9.40 50.11
Table 5: Analysis on different unseen languages.
average improvement of 0.5 MCD and 4.48% CER, indicat-
ing the effectiveness of our approach in using language IDs.
InW/o initializing encoder , we did not initialize the en-
coder Œ∏Ebefore the supervised leaning described in ¬ß 2.2.
Instead, we only initialized the parameters Œ∏T,Œ∏L, and Œ∏B
with the parameters pretrained in ¬ß 2.1. Through this eval-
uation, we investigated whether the performance gain with
our method resulted from the initialization of the language-
aware embedding layer or the encoder. We observed that W/o
initializing encoder resulted in an improvement of 0.04 in
MCD and only a 2.27% increase in CER on average, sug-
gesting that our method benefits more from the pretraining of
the language-aware embedding layer than from the encoder.
InUpdating language-aware embedding layer , we updated
the language-aware embedding layer during supervised learn-
ing, as formulated in Eq. (8). We observed that freezing the
language-aware embedding layer led to better performance
for most languages and metrics, resulting in an average dif-
ference of 0.29 in MCD and 1.04% in CER.
NaturalOracle (IPA monolingual)Oracle (IPA multilingual)Baseline (IPA multilingual)Proposed (Bytes multilingual)Proposed (IPA multilingual)
es (MOS)es (AMOS)11.522.533.544.5
0.5560.444es (AB test)p-value: 0.011Figure 5: MOS, AMOS, and AB test results for unseen language.
Error bars in MOS results represent 95% confidence intervals.
3.5 Dependency on Unseen Languages
We conducted evaluations on the zero-shot TTS for different
unseen languages. The eight European languages included in
the paired data are composed of Indo-European and Uralic
language families defined in Glottolog [Hammarstr ¬®omet al. ,
2021 ]. In this evaluation, we selected de and hu from each
of the families. During supervised learning in ¬ß 2.2, we ex-
cluded the paired data for each of de and hu and instead in-
cluded the paired data for es. Table 5 lists the results. We
chose the IPA-based baseline method, which had shown bet-
ter results in ¬ß 3.3. We observed that the pretraining im-
proved the CER by around 10% and MCD by around 0.3 for
de. However, the improvement in CER for hu was limited
to 2%, while the MCD was improved by around 0.5. These
results suggest that the performance of our zero-shot TTS is
language dependent, as observed in previous work on cross-
lingual transfer for NLP tasks [Wu and Dredze, 2019 ].
Fig. 3 visualize the token embedding Zand encoder in-
putsHinaveraged on each utterance. We used a t-distributed
stochastic neighbor embeddings (t-SNE) [der Maaten and
Hinton, 2008 ]. We observed overlaps in the token embed-
ding for (es, fr) and (de, nl), which are classified as Western
Romance and West Germanic in Glottolog, respectively. The
encoder inputs are separated in the embedding space for each
language. The results in Table 5 and the visualization sug-
gest that the cross-lingual transfer works better when simi-
lar languages sharing the token embedding space are present
during supervised learning. However, for languages with dis-
tinct token and language embeddings, the cross-lingual trans-
ferability might be limited. We leave the further analysis on
language dependencies as a topic for future research.
3.6 Subjective Evaluations on Naturalness
We conducted evaluations on naturalness as described in
¬ß 3.1. Fig. 4 shows the results for seen languages. Note
that we conducted the listening tests for de and fr. For each
language, either of the proposed methods showed the high-
est MOS, while we did not observe any significant differ-
ence between the proposed methods and the best baseline
method, which was the IPA-based multilingual model with
LIDs. To further validate our results, we also evaluated the
naturalness with an AMOS prediction model, as shown in
Fig. 4. We observed that the either of the proposed meth-
ods showed the highest scores in all the languages. On aver-
age, the byte-based and IPA-based proposed models showed
2.89 and 2.84, respectively, while the best baseline method
obtained 2.837. Additionally, we observed that the byte-based
proposed model often scored higher than the IPA-based pro-
posed models, which is consistent with the results in Table 2.
Fig. 5 shows the results for unseen languages. The ora-
cle methods had the highest MOS of 3.76 and 3.96, and the
baseline zero-shot method had the lowest MOS of 3.29. The
proposed methods outperformed the baseline method, and the
byte- and IPA-based models had the MOS of 3.44 and 3.32,
respectively. The AMOS results were consistent with the lis-
tening test results, with the proposed zero-shot TTS methods
outperforming the baseline method. In this evaluation, the
proposed byte-based model scored 3.21 on the AMOS, while
the oracle IPA-based model scored 3.20. To further validate
the results, we conducted a preference AB test on naturalness
with 25 rators. As shown in Fig. 5, our byte-based model
significantly outperformed the baseline IPA-based model.
4 Related Work
Multilingual TTS While previous work on multilingual
TTS has primarily focused on resource-rich languages [Zen
et al. , 2012; Li and Zen, 2016 ], there is growing interest
in developing TTS models on low-resource languages. Sev-
eral studies have explored the input tokens shared across lan-
guages such as bytes [Liet al. , 2019a; He et al. , 2021 ], IPA
symbols [Gutkin, 2017 ], and articulatory features [Lux and
Vu, 2022 ], to transfer knowledge from resource-rich to low-
resource languages. Grapheme tokens can eliminate the per-
7The AMOS tended to be lower than the MOS. While the MOS
prediction model has a high correlation, it may produce errors in
predicting absolute values, as reported in previous work [Saeki et al. ,
2022a ]. The relative relationships are more reliable in the AMOS.language G2P knowledge, and previous work has built a byte-
based TTS model for around 40 languages [Heet al. , 2021 ].
There has been work using the phonological features derived
from IPA to achieve the zero-shot TTS [Staib et al. , 2020 ].
Our framework achieves the zero-shot cross-lingual transfer
with bytes by leveraging multilingual text pretraining. There
have been studies on using untranscribed speech data for low-
resource scenarios by leveraging VQ-V AE [Zhang and Lin,
2020 ]or an ASR model [Ren et al. , 2019; Ni et al. , 2022 ].
Other work [Saeki et al. , 2022b ]has trained a massively
multilingual TTS using paired TTS, paired ASR, unpaired
speech, and unpaired text data. While it also performs text-
only training as in our work, it still uses the paired speech-text
data of the target languages. Our framework is simple and
scalable, while pioneering a novel paradigm with the zero-
shot TTS approach that relies only on text data.
Cross-lingual representation learning for NLP There
have been studies on learning cross-lingual representations
that can be applied to various NLP tasks in different lan-
guages [Gouws et al. , 2015; Ruder et al. , 2019 ]. Recent
work has highlighted the strong cross-lingual transferability
of multilingual BERT [Devlin et al. , 2019 ], which has been
observed to perform surprisingly well when transferred to
other languages [Wu and Dredze, 2019; Conneau and Lam-
ple, 2019 ]. Building on this, our work leverages multilingual
MLM pretraining for TTS, which improves byte-based TTS
models without G2P knowledge and achieves zero-shot TTS.
Language model pretraining for TTS Previous research
has explored self-supervised text pretraining techniques for
TTS. BERT models have been used to extract contextual
embeddings and enhance the prosody of TTS [Hayashi et
al., 2019; Xu et al. , 2021 ]. Other studies have used
phonemes jointly with graphemes [Jiaet al. , 2021 ]or sub-
phonemes [Zhang et al. , 2022 ]as the inputs of the MLM pre-
training. Our work proposes multilingual MLM pretraining
for TTS using text tokens shared across languages, rather than
focusing on monolingual pretraining.
5 Conclusions
We presented a multilingual TTS framework that leverages
unsupervised text pretraining. Our framework achieved
highly intelligible zero-shot TTS for an unseen language, re-
sulting in a CER of less than 12%. It also improved the
TTS for seen languages, with byte-based models without G2P
modules outperforming the IPA-based baselines. Our abla-
tion studies provided additional insights, including the effec-
tiveness of the frozen language embedding layer.
Limitations and future work Our proposed framework
has limitations. The performance gap remains between the
oracle models and our zero-shot TTS models in terms of in-
telligibility, speech quality, and naturalness, as seen in the
evaluation in ¬ß 3.3 and ¬ß 3.6. Further studies are needed to
improve our zero-shot TTS. Our framework also has a limi-
tation with language dependency, as the results in ¬ß 3.5 sug-
gest that this dependency is caused by the presence of similar
languages during supervised learning. Our future work will
focus on studying this language dependency further and de-
veloping a method that performs better for various languages.
Acknowledgments
Part of this work was supported by JSPS KAKENHI Grant
Number 21H05054, 22H03639, and 22J12040. This work
used the Bridges system [Nystrom et al. , 2015 ], which is sup-
ported by NSF award number ACI-1445606, at the Pittsburgh
Supercomputing Center. We would like to thank the research
teams at Google through the internship of the first author for
providing various insights on this topic.
References
[Baet al. , 2016 ]J. L. Ba, J. R. Kiros, and G. E Hinton. Layer
normalization. arXiv preprint arXiv:1607.06450 , 2016.
[BaÀún¬¥onet al. , 2020 ]M. Ba Àún¬¥on, P. Chen, B. Haddow,
K. Heafield, H. Hoang, M. Espl `a-Gomis, M. L Forcada,
A. Kamran, F. Kirefu, P. Koehn, et al. Paracrawl: Web-
scale acquisition of parallel corpora. In Proc. ACL , pages
4555‚Äì4567, 2020.
[Bapna and Firat, 2019 ]A. Bapna and O. Firat. Simple, scal-
able adaptation for neural machine translation. In Proc.
EMNLP-IJCNLP , pages 1538‚Äì1548, 2019.
[Conneau and Lample, 2019 ]A. Conneau and G. Lample.
Cross-lingual language model pretraining. In Proc.
NeurIPS , pages 7059‚Äì7069, 2019.
[der Maaten and Hinton, 2008 ]L. Van der Maaten and
G. Hinton. Visualizing data using t-sne. JMLR ,
9(11):2579‚Äì2605, 2008.
[Devlin et al. , 2019 ]J. Devlin, M.-W. Chang, K. Lee, and
K. Toutanova. BERT: Pre-training of deep bidirectional
transformers for language understanding. In Proc. NAACL ,
pages 4171‚Äì4186, 2019.
[Ebrahimi and Kann, 2021 ]A. Ebrahimi and K. Kann. How
to adapt your pretrained multilingual model to 1600 lan-
guages. In Proc. ACL-IJCNLP , pages 4555‚Äì4567, 2021.
[Fukada et al. , 1992 ]T. Fukada, K. Tokuda, T. Kobayashi,
and S. Imai. An adaptive algorithm for mel-cepstral anal-
ysis of speech. In Proc. ICASSP , pages 137‚Äì140, 1992.
[Gouws et al. , 2015 ]S. Gouws, Y . Bengio, and G. Corrado.
Bilbowa: Fast bilingual distributed representations with-
out word alignments. In Proc. ICML , pages 748‚Äì756,
2015.
[Gutkin, 2017 ]A. Gutkin. Uniform multilingual multi-
speaker acoustic model for statistical parametric speech
synthesis of low-resourced languages. In Proc. Inter-
speech , pages 2183‚Äì2187, 2017.
[Hammarstr ¬®omet al. , 2021 ]H. Hammarstr ¬®om, R. Forkel,
M. Haspelmath, and S. Bank. Glottolog 4.5. Max Planck
Institute for the Science of Human History , 2021.
[Hayashi et al. , 2019 ]T. Hayashi, S. Watanabe, T. Toda,
K. Takeda, S. Toshniwal, and K. Livescu. Pre-trained
text embeddings for enhanced text-to-speech synthesis. In
Proc. Interspeech , pages 4430‚Äì4434, 2019.
[Hayashi et al. , 2021 ]T. Hayashi, R. Yamamoto,
T. Yoshimura, P. Wu, J. Shi, T. Saeki, Y . Ju, Y . Ya-
suda, S. Takamichi, and S. Watanabe. Espnet2-tts:Extending the edge of tts research. arXiv preprint
arXiv:2110.07840 , 2021.
[Heet al. , 2021 ]M. He, J. Yang, L. He, and F. K
Soong. Multilingual byte2speech models for scal-
able low-resource speech synthesis. arXiv preprint
arXiv:2103.03541 , 2021.
[Hendrycks and Gimpel, 2016 ]D. Hendrycks and K. Gim-
pel. Gaussian error linear units (gelus). arXiv preprint
arXiv:1606.08415 , 2016.
[Jiaet al. , 2021 ]Y . Jia, H. Zen, J. Shen, Y . Zhang, and
Y . Wu. PnG BERT: Augmented BERT on phonemes
and graphemes for neural TTS. arXiv preprint
arXiv:2103.15060 , 2021.
[Jr, 2005 ]R. G Gordon Jr. Ethnologue, languages of the
world. https://www.ethnologue.com/, 2005. Accessed:
2023-05-27.
[Kim et al. , 2021 ]J. Kim, J. Kong, and J. Son. Conditional
variational autoencoder with adversarial learning for end-
to-end text-to-speech. In Proc. ICML , pages 5530‚Äì5540,
2021.
[Kong et al. , 2020 ]J. Kong, J. Kim, and J. Bae. HiFi-GAN:
Generative adversarial networks for efficient and high fi-
delity speech synthesis. Proc. NeurIPS , 33:17022‚Äì17033,
2020.
[Li and Zen, 2016 ]B. Li and H. Zen. Multi-language multi-
speaker acoustic modeling for LSTM-RNN based statis-
tical parametric speech synthesis. In Proc. Interspeech ,
pages 2468‚Äì2472, 2016.
[Liet al. , 2019a ]B. Li, Y . Zhang, T. Sainath, Y . Wu, and
W. Chan. Bytes are all you need: End-to-end multilin-
gual speech recognition and synthesis with bytes. In Proc.
ICASSP , pages 5621‚Äì5625, 2019.
[Liet al. , 2019b ]N. Li, S. Liu, Y . Liu, S. Zhao, and M. Liu.
Neural speech synthesis with Transformer network. In
Proc. AAAI , pages 6706‚Äì6713, 2019.
[Liet al. , 2022 ]X. Li, F. Metze, D. R Mortensen, A. W
Black, and S. Watanabe. ASR2K: Speech recognition
for around 2000 languages without audio. arXiv preprint
arXiv:2209.02842 , 2022.
[Lux and Vu, 2022 ]F. Lux and T. Vu. Language-agnostic
meta-learning for low-resource text-to-speech with artic-
ulatory features. In Proc. ACL , pages 6858‚Äì6868, 2022.
[Munich Artificial Intelligence Laboratories GmbH, 2017 ]
Munich Artificial Intelligence Laboratories GmbH.
The M-AILABS speech dataset. https://www.caito.de/
2019/01/the-m-ailabs-speech-dataset/, 2017. Accessed:
2023-05-27.
[Nagrani et al. , 2017 ]A. Nagrani, J. S. Chung, and A. Zis-
serman. V oxCeleb: A large-scale speaker identification
dataset. In Proc. Interspeech , pages 2616‚Äì2620, 2017.
[Nair and Hinton, 2010 ]V . Nair and G. E Hinton. Rectified
linear units improve restricted boltzmann machines. In
Proc. ICML , 2010.
[Niet al. , 2022 ]J. Ni, L. Wang, H. Gao, K. Qian, Y . Zhang,
S. Chang, and M. Hasegawa-Johnson. Unsupervised
text-to-speech synthesis by unsupervised automatic speech
recognition. In Proc. Interspeech , pages 461‚Äì465, 2022.
[Nystrom et al. , 2015 ]N. A Nystrom, M. J Levine, R. Z
Roskies, and J Ray Scott. Bridges: a uniquely flexible
hpc resource for new communities and data analytics. In
Proc. XSEDE , pages 1‚Äì8, 2015.
[Paolacci et al. , 2010 ]G. Paolacci, J. Chandler, and P. G
Ipeirotis. Running experiments on amazon mechanical
turk. Judgment and Decision making , 5(5):411‚Äì419, 2010.
[Park and Mulc, 2019 ]K. Park and T. Mulc. CSS10: A col-
lection of single speaker speech datasets for 10 languages.
Proc. Interspeech , pages 1566‚Äì1570, 2019.
[Pires et al. , 2019 ]T. Pires, E. Schlinger, and D. Garrette.
How multilingual is multilingual BERT? In Proc. ACL ,
pages 4996‚Äì5001, 2019.
[Prakash et al. , 2019 ]A. Prakash, A L. Thomas, S Umesh,
and H. A Murthy. Building multilingual end-to-end speech
synthesisers for Indian languages. In Proc. SSW , pages
194‚Äì199, 2019.
[Radford et al. , 2022 ]A. Radford, J. W Kim, T. Xu,
G. Brockman, C. McLeavey, and I. Sutskever. Robust
speech recognition via large-scale weak supervision. arXiv
preprint arXiv:2212.04356 , 2022.
[Ravanelli et al. , 2021 ]M. Ravanelli, T. Parcollet,
P. Plantinga, A. Rouhe, S. Cornell, L. Lugosch,
C. Subakan, N. Dawalatabad, A. Heba, J. Zhong,
et al. SpeechBrain: A general-purpose speech toolkit.
arXiv preprint arXiv:2106.04624 , 2021.
[Renet al. , 2019 ]Y . Ren, X. Tan, T. Qin, S. Zhao, Z. Zhao,
and T.-Y . Liu. Almost unsupervised text to speech and au-
tomatic speech recognition. In Proc. ICML , pages 5410‚Äì
5419, 2019.
[Ruder et al. , 2019 ]S. Ruder, I. Vuli ¬¥c, and A. S√∏gaard. A
survey of cross-lingual word embedding models. JAIR ,
65:569‚Äì631, 2019.
[Saeki et al. , 2022a ]T. Saeki, D. Xin, W. Nakata, T. Ko-
riyama, S. Takamichi, and H. Saruwatari. UTMOS:
UTokyo-SaruLab system for V oiceMOS Challenge 2022.
InProc. Interspeech , pages 4521‚Äì4525, 2022.
[Saeki et al. , 2022b ]T. Saeki, H. Zen, Z. Chen, N. Morioka,
G. Wang, Y . Zhang, A. Bapna, A. Rosenberg, and B. Ram-
abhadran. Virtuoso: Massive multilingual speech-text
joint semi-supervised learning for text-to-speech. arXiv
preprint arXiv:2210.15447 , 2022.
[Seki et al. , 2022 ]K. Seki, S. Takamichi, T. Saeki, and
H. Saruwatari. Text-to-speech synthesis from dark data
with evaluation-in-the-loop data selection. arXiv preprint
arXiv:2210.14850 , 2022.
[Shen et al. , 2018 ]J. Shen, R. Pang, R. J Weiss, M. Schus-
ter, N. Jaitly, Z. Yang, Z. Chen, Y . Zhang, Y . Wang,
RJ Skerrv-Ryan, et al. Natural TTS synthesis by condi-
tioning WaveNet on mel spectrogram predictions. In Proc.
ICASSP , pages 4779‚Äì4783, 2018.[Snyder et al. , 2018 ]D. Snyder, D. Garcia-Romero, G. Sell,
D. Povey, and S. Khudanpur. X-vectors: Robust dnn em-
beddings for speaker recognition. In Proc. ICASSP , pages
5329‚Äì5333, 2018.
[Staib et al. , 2020 ]M. Staib, T. H. Teh, A. Torresquintero,
D. S R. Mohan, L. Foglianti, R. Lenain, and J. Gao.
Phonological features for 0-shot multilingual speech syn-
thesis. In Proc. Interspeech , pages 2942‚Äì2946, 2020.
[Tachibana et al. , 2018 ]H. Tachibana, K. Uenoyama, and
S. Aihara. Efficiently trainable text-to-speech system
based on deep convolutional networks with guided atten-
tion. In Proc. ICASSP , pages 4784‚Äì4788, 2018.
[Vaswani et al. , 2017 ]A. Vaswani, N. Shazeer, N. Parmar,
J. Uszkoreit, L. Jones, A. N Gomez, ≈Å. Kaiser, and I. Polo-
sukhin. Attention is all you need. In Proc. NeurIPS , vol-
ume 30, 2017.
[Veaux et al. , 2017 ]C. Veaux, J. Yamagishi, K. MacDonald,
et al. CSTR VCTK corpus: English multi-speaker corpus
for CSTR voice cloning toolkit. University of Edinburgh.
The Centre for Speech Technology Research (CSTR) , 2017.
[Wang et al. , 2021 ]C. Wang, M. Riviere, A. Lee, A. Wu,
C. Talnikar, D. Haziza, M. Williamson, J. Pino, and
E. Dupoux. V oxPopuli: A large-scale multilingual speech
corpus for representation learning, semi-supervised learn-
ing and interpretation. In Proc. ACL , pages 993‚Äì1003,
2021.
[Watanabe et al. , 2018 ]S. Watanabe, T. Hori, S. Karita,
T. Hayashi, J. Nishitoba, Y . Unno, N.-E. Y . Soplin, J. Hey-
mann, M. Wiesner, N. Chen, et al. ESPnet: End-to-end
speech processing toolkit. Proc. Interspeech , pages 2207‚Äì
2211, 2018.
[Wu and Dredze, 2019 ]S. Wu and M. Dredze. Beto, Bentz,
Becas: The surprising cross-lingual effectiveness of
BERT. In Proc. EMNLP-IJCNLP , pages 833‚Äì844, 2019.
[Xuet al. , 2021 ]G. Xu, W. Song, Z. Zhang, C. Zhang,
X. He, and B. Zhou. Improving prosody modelling with
cross-utterance BERT embeddings for end-to-end speech
synthesis. In Proc. ICASSP , pages 6079‚Äì6083, 2021.
[Zenet al. , 2012 ]H. Zen, N. Braunschweiler, S. Buchholz,
M. JF Gales, K. Knill, S. Krstulovic, and J. Latorre. Sta-
tistical parametric speech synthesis based on speaker and
language factorization. TASLP , 20(6):1713‚Äì1724, 2012.
[Zenet al. , 2019 ]H. Zen, V . Dang, R. Clark, Y . Zhang, R. J
Weiss, Y . Jia, Z. Chen, and Y . Wu. LibriTTS: A corpus
derived from LibriSpeech for text-to-speech. In Proc. In-
terspeech , pages 1526‚Äì1530, 2019.
[Zhang and Lin, 2020 ]H. Zhang and Y . Lin. Unsupervised
learning for sequence-to-sequence text-to-speech for low-
resource languages. Proc. Interspeech , pages 3161‚Äì3165,
2020.
[Zhang et al. , 2022 ]G. Zhang, K. Song, X. Tan, D. Tan,
Y . Yan, Y . Liu, G. Wang, W. Zhou, T. Qin, T. Lee,
et al. Mixed-Phoneme BERT: Improving BERT with
mixed phoneme and sup-phoneme representations for text
to speech. In Proc. Interspeech , pages 456‚Äì460, 2022.
MethodSeen UnseenAvg.de fr ru fi es
MCD CER MCD CER MCD CER MCD CER MCD CER MCD CER
Spoken Text 5.65 3.79 6.48 7.15 7.38 10.62 4.99 5.28 9.05 18.27 6.46 9.58
Written Text 5.81 4.55 6.94 9.10 7.61 21.24 5.22 12.73 9.50 18.44 6.76 12.52
Spoken+Written Text 5.54 3.72 6.34 7.51 7.07 15.33 4.96 5.44 8.82 17.48 6.35 10.04
Table 6: Comparison of text data domain for unsupervised text pretraining.
MethodSeen UnseenAvg.de fr ru fi es
MCD CER MCD CER MCD CER MCD CER MCD CER MCD CER
Residual layer 5.65 3.79 6.48 7.15 7.38 10.62 4.99 5.28 9.05 18.27 6.46 9.58
Transformer encoder 5.77 4.63 6.36 6.61 7.17 11.25 4.90 6.50 8.89 14.15 6.44 9.97
Table 7: Comparison of bottleneck layer architecture.
A Text data for pretraining
We investigated the effect of different types of text data
used in pretraining Dtexton the performance of our method.
As described in ¬ß 3.1, we used spoken texts from V oxPop-
uli, M-AILABS, and CSS10 in the evaluations presented in
¬ß 3. However, we can acquire a large amount of text data
from datasets designed for NLP or from web-crawled text
resources. These data typically consist of written texts and
cover a wide range of domains. To investigate the effective-
ness of using written texts for our multilingual TTS, we used
ParaCrawl [BaÀún¬¥onet al. , 2020 ], a web-crawled text dataset
built for machine translation, for Dtextduring the unsuper-
vised pretraining described in ¬ß 2.1. For the investigation,
we randomly sampled the same amount of texts as in Table 1
for each language. We then trained our model using the fol-
lowing three different cases. 1) Spoken Text : Only using the
spoken text for pretraining as in the previous evaluations, 2)
Written Text : Only using the text data from ParaCrawl, and
3)Spoken+Written Text : We combined the text data in Spo-
ken Text andWritten Text . We used the byte-based proposed
model presented in ¬ß 3.2 and ¬ß 3.3. Table 6 lists the results.
We observed that Spoken Text outperformed Written Text
in all the metrics and languages, resulting in an average dif-
ference of 0.3 in MCD and 2.94% in CER. These results
demonstrate the effectiveness of using spoken text for pre-
training. Spoken+Written Text showed on average 0.11 lower
MCD and 0.46% higher CER compared to Spoken Text . How-
ever, for the unseen language, Spoken+Written Text outper-
formed Spoken Text in MCD and CER. These results suggest
that adding written text data can improve the generalization
of our TTS models for the zero-shot scenarios.
B Architecture of bottleneck layer
As described in ¬ß 2.4, we used the residual layer for our bot-
tleneck layer. Also, we demonstrated the effectiveness of
the residula bottleneck layer for both seen and unseen lan-
guages in the evaluation presented in ¬ß 3.4. In this section, we
explored an alternative architecture for the bottleneck layer.
We conducted experiments using a single-layer Transformerencoder as the bottleneck layer (referred to as Transformer-
encoder ), comparing it with the original residual layer de-
tailed in ¬ß 2.4 (referred to as Residual layer ). Table 7 lists the
results.
For the seen languages, the superior performance between
the residual layer and the transformer encoder varied, depend-
ing on the specific language and evaluation metrics. However,
for the unseen language, the Transformer encoder showed
higher performance, achieving an improvement of 4.12 in
CER. Looking at the average scores across all languages,
the Transformer encoder had a slightly lower MCD, while
the CER was reduced by 0.39 when using the residual layer.
These results suggest that the use of a deeper layer can im-
prove the generalizability of the proposed model. Neverthe-
less, the overall performance of both models remains compa-
rable in terms of average metrics.
C Effect of excluding some languages from
paired data.
In this section, we have deliberately excluded several lan-
guages from the paired data used for the supervised learning
described in ¬ß 2.2 in order to study their impact. As shown
in Table 1, the paired data originally included the languages
de, fr, nl, fi, hu, ru, and el. As a comparison case, we first ex-
cluded fr, which belongs to Italic languages as es according to
Glottolog [Hammarstr ¬®omet al. , 2021 ]. We also removed de
and nl, which belong to Germanic languages, from the paired
data. Consequently, in the comparison case, supervised learn-
ing was performed only with fi, ru, hu, and el. Table 8 lists
the results. Original corresponds to the case shown in Ta-
ble 1, while Excluded denotes the case where only fi, ru, hu,
and el were used.
The three languages listed on the left-hand side of Table 8
(ru, hu, fi) represent the seen languages in both cases. In-
terestingly, the Original scenario generally outperformed the
Excluded scenario for these languages. These results indi-
cate that in the context of multilingual TTS training, perfor-
mance can potentially be improved by including a wider va-
riety of languages rather than restricting to similar languages.
Methodru hu fi de fr es
MCD CER MCD CER MCD CER MCD CER MCD CER MCD CER
Original (de, fr, nl, fi, hu, ru, el) 7.38 10.62 5.01 6.05 4.99 5.28 5.65 3.79 6.48 7.15 9.05 18.27
Excluded (fi, hu, ru, el) 7.00 11.11 5.32 6.92 4.98 5.46 10.39 34.11 10.90 49.65 10.00 24.80
Table 8: Effects of excluding languages from paired data.
(a) Baseline (Bytes)(b) Proposed(Bytes)
Figure 6: Cross-attention maps obtained from byte-based baseline
and proposed methods, which correspond to first attention head in
fifth and sixth layers of decoder.
This suggests the effectiveness of massively multilingual TTS
training, as supported by previous work [Saeki et al. , 2022b ].
The two languages (de, fr) were included in the paired data
forOriginal , but were excluded from the paired data for Ex-
cluded . We observed that the zero-shot setting resulted in a
decrease in performance. Comparing the de results in Table 5
with those in Table 8, we confirmed that excluding fr, nl, and
es resulted in a 6.1% performance degradation in CER. Also,
es was absent from the paired data in both the Original and
Excluded scenarios. An examination of the es results revealed
a 6.53% increase in CER for the Excluded scenario. These
results demonstrate the importance of including linguistically
similar languages in the paired data.
D Observation of cross-attention map.
In this section, we show the cross-attention maps obtained
during inference from both the byte-based proposed and the
baseline methods defined in ¬ß 3.1. In both cases, an utterance
sampled from an unseen language (Spanish) was used. Fig. 6
shows the cross-attention maps corresponding to the first at-
tention head in the fifth and sixth layers of the decoder. It
should be noted that we have cross-attention maps for other
heads and layers. The results shown in Fig. 6 are derived from
attention maps that have a diagonal shape in higher layers forboth the baseline and the proposed methods. The top part of
Fig. 6 represents results from the fifth layer, while the bottom
part corresponds to results from the sixth layer. The left side
of the figure shows the results of the baseline method, while
the right side shows the results of the proposed method.
We observe that the fifth layer of the baseline model shows
a discontinuity in the attention map, which leads to instabil-
ity of the linguistic content. Conversely, in the fifth layer of
the proposed model, the attention map is significantly more
continuous than in the baseline method. These results sug-
gest that our unsupervised text pretraining can improve cross-
attention in the absence of paired speech-text data. The re-
sults are also reflected in the intelligibility difference between
the baseline and the proposed methods presented in ¬ß 3.3.