Balancing Exploration and Exploitation in
Hierarchical Reinforcement Learning via Latent
Landmark Graphs
Qingyang Zhang1,2, Yiming Yang1, Jingqing Ruan1,2, Xuantang Xiong1,3, Dengpeng Xing1,3,∗, and Bo Xu1,2,3
1Institute of Automation, Chinese Academy of Sciences, Beijing, China
2School of Future Technology, University of Chinese Academy of Sciences, Beijing, China
3School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China
{zhangqingyang2019, yangyiming2019, ruanjingqing2019, xiongxuantang2021,
dengpeng.xing, xubo }@ia.ac.cn
Abstract —Goal-Conditioned Hierarchical Reinforcement
Learning (GCHRL) is a promising paradigm to address the
exploration-exploitation dilemma in reinforcement learning. It
decomposes the source task into subgoal conditional subtasks and
conducts exploration and exploitation in the subgoal space. The
effectiveness of GCHRL heavily relies on subgoal representation
functions and subgoal selection strategy. However, existing
works often overlook the temporal coherence in GCHRL
when learning latent subgoal representations and lack an
efficient subgoal selection strategy that balances exploration and
exploitation. This paper proposes HIerarchical reinforcement
learning via dynamically building Latent Landmark graphs
(HILL) to overcome these limitations. HILL learns latent
subgoal representations that satisfy temporal coherence using
a contrastive representation learning objective. Based on these
representations, HILL dynamically builds latent landmark
graphs and employs a novelty measure on nodes and a
utility measure on edges. Finally, HILL develops a subgoal
selection strategy that balances exploration and exploitation
by jointly considering both measures. Experimental results
demonstrate that HILL outperforms state-of-the-art baselines
on continuous control tasks with sparse rewards in sample
efficiency and asymptotic performance1. Our code is available
at https://github.com/papercode2022/HILL.
Index Terms —Hierarchical Reinforcement Learning, Explo-
ration and Exploitation, Representation Learning, Landmark
Graph, Contrastive Learning
I. I NTRODUCTION
Balancing exploration and exploitation is a one of the
major challenges in reinforcement learning. Goal-Conditioned
Hierarchical Reinforcement Learning (GCHRL) [1, 2] is a
promising paradigm that leverages task decomposition and
temporal abstraction to address this challenge. GCHRL typ-
ically consists of two hierarchical levels [3], where a high-
level controller periodically decomposes the source task into
subgoal conditional subtasks, and a low-level controller learns
This work is supported in part by National Key R&D Program of China
(No.2022ZD0116405), in part by the Program for National Nature Science
Foundation of China (62073324), and in part by the Strategic Priority Research
Program of the Chinese Academy of Sciences (No.XDA27030300).
*Corresponding author.
1Our paper has been accepted by the conference of International Joint
Conference on Neural Networks (IJCNN) 2023.to complete those subtasks by reaching the subgoals. The sub-
goal sequence helps compress the exploration space, thereby
reducing the exploration difficulty. It also enables efficient ex-
ploitation by selecting subgoals with higher value estimations.
A proper subgoal representation function is crucial for effec-
tive GCHRL. Early efforts manually identify bottleneck states
as subgoals [4, 5], which require task-specific knowledge.
Recent approaches automatically learn subgoal representations
in an end-to-end manner along with bi-level policies [6, 7, 8],
making them more generic. However, they often overlook the
temporal coherence in GCHRL. Temporal coherence refers to
the hierarchical relationship in time between different levels
of control in a system. The high-level controller operates
at a slower timescale and focuses on radical changes in
environmental states associated with the completion of the
source task, while the low-level controller operates at a quicker
timescale and focuses on modest changes in environmental
states associated with completing subtasks. Approaches [9, 10]
that consider the temporal abstraction perspective have shown
great promise in improving exploration efficiency in GCHRL.
The subgoal selection strategy is another crucial compo-
nent of GCHRL. The balance of exploration and exploitation
and the final performance are significantly impacted. If the
high-level controller always selects subgoals that have ever
yielded high rewards, it may miss out on potentially greater
rewards. One line of work uses neural networks trained with
environmental rewards as subgoal selection strategies [5, 6, 8],
which enable efficient exploitation but often suffer from weak
exploration. Another line implements high-level policies as
planners, achieving efficient exploitation. They build environ-
ment graphs or trees as world descriptors based on state tran-
sitions, and subgoals are generated by planning on these world
descriptors [11, 12]. However, this line typically builds graphs
in state space, where the complexity of graph construction
grows exponentially with the dimension of the state.
This paper proposes a novel method to balance exploration
and exploitation in HIerarchical reinforcement learning via
dynamically building Latent Landmark graphs (HILL). HILL
introduces a negative-power contrastive representation learning
objective to train a subgoal representation function that con-arXiv:2307.12063v1  [cs.LG]  22 Jul 2023
siders temporal coherence. The learned representations serve
as possible subgoals, and landmarks are identified to maximize
coverage of the explored latent space. Latent landmark graphs
are built based on these landmarks, and two measures are
defined on the graphs: a novelty measure upon nodes to
encourage exploring novel landmarks and a utility measure
upon edges to estimate the benefits of landmark transitions to
task completion. HILL balances exploration and exploitation
by simultaneously considering both measures and choosing
the most valuable landmark as the subgoal. Empirical results
demonstrate that HILL outperforms state-of-the-art (SOTA)
baselines in numerous challenging continuous control tasks
with sparse rewards based on the MuJoCo simulator [13].
Furthermore, we conduct visualization analyses and ablation
studies to verify the significance of the various components of
HILL. We highlight the main contributions below:
•We introduce a contrastive representation learning objec-
tive to train latent subgoal representations that comply
with the temporal coherence in GCHRL.
•We propose a latent landmark graph structure built on
learned subgoal representations. Based on the graphs, we
present a subgoal selection strategy that well tackles the
exploration-exploitation dilemma in GCHRL.
•Empirical results demonstrate the superiority of our
method over SOTA baselines in numerous continuous
control tasks with sparse rewards.
II. R ELATED WORK
Subgoal Representation Learning. Learning effective sub-
goal representations remains a major challenge in GCHRL.
Previous works either pre-define bottleneck states as sub-
goals [4, 5] or use external rewards as supervision to train
the high-level policy to generate subgoals [6, 14, 15]. The
former requires task-specific knowledge and lacks general-
ization, while the latter results in challenging exploration
and low training efficiency. Recent works use variational
autoencoders [16, 17] to extract low-dimensional features from
observations. However, such features may include redundant
information. Instead, it is more effective to focus on the key as-
pects of variation that are essential for decision-making. Ghosh
et al. [18] learn actionable representations that emphasize state
factors inducing significant differences in corresponding ac-
tions and de-emphasize irrelevant features. Two recent studies
[9, 10] use slow feature analysis methods [19] to extract useful
features as subgoals. However, they use L2norm as a distance
estimation when calculating triplet loss, which may lead to
degenerate distributions [20]. In contrast, our method uses a
negative-power contrastive representation learning objective to
learn informative and robust subgoal representations.
Explore and Exploit in GCHRL. Efficient exploration in
GCHRL can be achieved through a variety of methods, such as
scheduling between intrinsic and extrinsic task policies [21],
restricting the high-level action space to reduce exploration
space [8], designing transition-based intrinsic rewards [22], or
using curiosity-driven intrinsic rewards [23]. However, these
methods rarely consider exploitation efficiency after sufficientexploration. Efficient exploitation in GCHRL is typically
achieved through planning [24, 25], where high-level graphical
planners have shown great promise [11]. Some works use
previously seen states in a replay buffer as graph nodes to
generate subgoals through graph search. For example, Shang
et al. [26] propose a method that unsupervisedly discovers
the world graph and integrates it to accelerate GCHRL, while
Jin et al. [27] use an attention mechanism to select subgoals
based on the graphs. However, these methods construct graphs
in the state space, where the graphs can be challenging to be
built in high-dimensional state spaces. While Zhang et al. [28]
leverage an auto-encoder with reachability constraints to learn
a latent space and generate latent landmarks through clustering
in this space, they ultimately decode the cluster centroids as
landmarks, suggesting that they still plan in the state space.
In contrast, our method builds graphs in the learned latent
space, making it more generic and adaptable to state spaces
of varying dimensions.
III. P RELIMINARIES
A. Goal-Conditioned Hierarchical Reinforcement Learning
A finite-horizon, subgoal-augmented Markov Decision Pro-
cess can be described as a tuple ⟨S,G,A,P,R, γ⟩, where S
is a state space, Gis a subgoal space, Ais an action space,
P:S×A×S → [0,1]is an environmental transition function,
R:S × A → Ris a reward function, and γ∈[0,1)is a
discount factor. We consider a two-level GCHRL framework,
which comprises a high-level policy πθh(g|s)over subgoals
given states and a low-level policy πθl(a|s, g)over actions
given states and subgoals. The policy πθhoperates at a slower
timescale and samples a subgoal gt∈ G when t≡0
(mod c), for fixed c. The policy πθhis trained to optimize
the expected cumulative discounted environmental rewards
Eπθh,πθl[P∞
t=0γtR(st, at)]. The policy πθlselects an action
at∼πθl(·|st, gt)at every time step and is intrinsically re-
warded with rl
t(gt, φ(st)) =−D(gt, φ(st))to reach gt, where
Dis a distance function. Following previous work [10], we
employ L2distance as Dto provide dense non-zero rewards,
thus accelerating low-level policy learning. The policy πθlis
trained to optimize the expected cumulative intrinsic rewards
EπθlhPci+c−1
t=cirl
ti
, i= 0,1,2, . . ..
B. Universal Value Function Approximator
We use an off-policy algorithm Soft Actor-Critic (SAC) [29]
as the base RL optimizer of both levels. The low-level critic
is implemented as a Universal Value Function Approximator
(UVFA) [30], which extends the concept of value function
approximation to include both states and subgoals as inputs.
Theoretically, a sufficiently expressive UVFA can identify
the underlying structure across states and subgoals, and thus
generalize to any subgoal.
We define a pseudo-discount function σ:S→[0,1], which
takes the double role of state-dependent discounting, and of
soft termination, in the sense that σ(s) = 0 if and only if
sis a terminal state according to the subgoal g. UVFAs can
then be formalized as V(s, g)≈Vg,π∗
θl(s), where π∗
θlis the
Fig. 1. An overview of HILL. HILL learns subgoal representations through a negative-power contrastive learning objective and selects subgoals by building
latent landmark graphs. HILL identifies landmarks that maximize the explored latent space’s coverage and constructs latent landmark graphs based on them.
HILL then measures novelties of nodes to encourage exploring novel landmarks and utilities of edges to estimate the benefits of transitions between landmarks
to task completion. Finally, HILL uses a balanced subgoal selection strategy to tackle the exploration-exploitation dilemma by jointly considering both measures
and selecting the most valuable landmark as the next subgoal. The representation function and bi-level policies are trained jointly online.
optimal low-level policy and Vg,πθlis a general value function
representing the expected cumulative pseudo-discounted future
pseudo-return for each gand any policy πθl:
Vg,πθl(s) =E∞X
t=0rl
t(g, φ(st))tY
k=0σ(sk)s0=s
. (1)
V(s, g)is usually implemented by a neural network for better
generalization and is trained by the Bellman equation in a
bootstrapping way.
IV. M ETHOD
We introduce HILL in detail in this section, and its main
structure is depicted in Figure 1.
A. Contrastive Subgoal Representation Learning
We define a subgoal representation function φ:S →Rd
which abstracts states s∈ S to latent representations z∈ Z.
Assuming the high-level controller selects a subgoal every c
time steps. According to the temporal coherence in GCHRL,
the representations of high-level adjacent states (e.g., φ(st)
andφ(st+c)) are distinguishable, while those of low-level ad-
jacent states (e.g., φ(st)andφ(st+1)) are relatively similar. To
trainφ, we define a negative-power contrastive representation
learning objective as follows:
Lc(st, st+1, st+c) =||φ(st)−φ(st+1)||2
2
+β·1
||φ(st)−φ(st+c)||n
2+ϵ,(2)
where n≥1,βis a scaling factor, and ϵ > 0is a
small constant to avoid a zero denominator. The function
φabstracts the high-dimensional state space into a low-
dimensional information-intensive latent space (i.e., subgoal
space), significantly reducing the exploration difficulty.The function φis trained with the bi-level policies and value
functions jointly. To alleviate the non-stationarity caused by
dynamically updated representations z, we adopt a regulariza-
tion function Lrsimilar to HESS [10] to restrict the change
of representations that already well-fit Equation 2:
Lr(st) =||φ(st)−φold(st)||2. (3)
We maintain a replay buffer Bpto record the newest losses of
the sampled triplets calculated by Equation 2. When updating
φ, we sample the top ktriplets with the smallest losses and
calculate their Lrto regularize its update. The overall subgoal
representation learning loss is:
Lφ(s) =Est∼Bl[Lc(st, st+1, st+c)] +Es′
t∼Bp[Lr(s′
t)],(4)
where Blis a replay buffer containing historical episodes.
B. Building Latent Landmark Graphs
At time steps t≡0(mod c), we build a latent landmark
graph by randomly sampling Kstates from Bl, abstracting
them into representations using φ, and store the state and its
representation in pairs into a temporary buffer Bt. We then
build a latent landmark graph that balances subgoal selection
for effective exploration-exploitation trade-offs. Every time a
new graph is built, Btis reset.
1) Novelty-guided Exploration Based on Nodes: Sampling
Nodes. We use the Farthest Point Sampling (FPS) [31] method
to select a collection of representations from Btthat maximize
the coverage of the latent space explored. These representa-
tions, called landmarks and denoted as Lt, serve as nodes of
the latent landmark graph and optional subgoals. We also add
the representation of the task goal that the agent receives at
the start of the current episode, as well as the representation
of the current state, to Lt. A single landmark is denoted as
l∈ Lt.
Novelty Measure upon Nodes. To identify novel landmarks
that can guide the agent to states it has rarely visited before,
we utilize the count-based method [32] and define a novelty
measure Nfor any representation (including l∈ L t). This
measure estimates the expected discounted future occupancies
of representations starting from a given state si:
N(zi) =X
T ∈B l⌊(T−i)/c⌋X
j=0γjη(zi+jc), (5)
where zi=φ(si),Tis the length of episode Tandηis a hash
table [33] that records the historical cumulative visit counts of
representations. By leveraging the visit history of past episodes
inBl,Nrealizes a long-term estimate of novelty. A landmark
lwith smaller N(l)is considered more worth exploring. We
update the hash table ηat the end of each episode.
2) Utility-guided Exploitation Based on Edges: Connect-
ing Nodes into Edges. We create two edges directed in reverse
orders between each pair of latent landmarks li, lj∈ L t.
Specifically, an edge wi,jis defined from litolj.
Utility Measure upon Edges. We define a utility measure
Ufor any wi,j, which estimates the benefits of transitioning
from litoljin completing the source task:
U(wi,j) =Esx∈Bl[I(φ(sx) =li)V(sx, lj)], (6)
where Iis an indicator function and Vis the low-level
UVFA. The measure Udepends on the accuracy of V.
However, learning a UVFA that accurately estimates Vposes
special challenges. The agent encounters limited combinations
of states and subgoals (s, g), which can lead to unreliable
estimates for unseen combinations. To address this issue, we
consider that, starting from a point (i.e., a representation) in
the latent space, representations within the neighborhood of
this point are more likely to be visited, and thus, Vprovides
accurate estimates. Therefore, we obtain Ufor non-adjacent
landmarks by applying the Bellman-Ford [34] method:
U(wi,f) = max
lj[U(wi,j) +U(wj,f)]
= max
lj,...,lv[U(wi,j) +v−1X
x=jU(wx,x+1) +U(wv,f)],
(7)
where wi,fandwj,fare the edges of non-adjacent landmarks
(li, lf)and(lj, lf), respectively, and wi,j,wx,x+1andwv,fare
the edges of adjacent landmarks (li, lj),(lx, lx+1)and(lv, lf),
respectively. A landmark ljwith greater U(wi,j)is more
worth exploiting. Moreover, we accelerate the convergence
ofVusing Hindsight Experience Replay (HER) [35]. HER
smartly generates more feedback for the agent by replacing
unachievable subgoals with achieved ones in the near future,
thus accelerating the training process.
C. Balance Exploration and Exploitation
We tackle the exploration-exploitation dilemma by selecting
a landmark from Ltthat well-balances NandUas theAlgorithm 1 HILL algorithm
Initialize :φ,πθh,πθlandp.
1:fori= 1...num episodes do
2: fort= 0...T−1do
3: ift≡0(mod c)then
4: Generate a random float value q∈[0.0,1.0].
5: if2q−1≤pthen
6: Build a latent landmark graph.
7: Select gtwith the strategy in Equation 9.
8: else
9: Sample gtusing πθh.
10: end if
11: Update θhusing SAC.
12: end if
13: rt, st+1←execute at∼πθl(·|st, gt).
14: end for
15: ifi≡0(mod 100)then
16: Update φusing Equation 4.
17: Update pto the newest average train success rate.
18: end if
19: Update θlandφusing SAC.
20:end for
21:return φ,πθhandπθl.
subgoal. When mapped to the [0,1]distribution, U(wi,j)can
be regarded as an incremental probability of success as it
approximates the expected benefits of transitioning from lito
lj. The distribution is as follows:
P(U(wi,j)) =eU(wi,j)
PY
y=1eU(wi,y), (8)
where Yis the number of landmarks in Lt. Then we define
a balanced strategy based on NandPand select a landmark
with a smaller historical visit count and a larger transitional
benefit as the subgoal gtof the next time interval:
gt= arg min
lj∈Lt[(1− P(U(wi′,j)))×N(lj)],(9)
where li′=φ(st)andstis the state of the current time step.
Algorithm 1 shows the pseudo-code of HILL. Note that we
use the subgoal selection strategy in Equation 9 as a high-
level teacher policy and use πθhmodeled by a neural network
as a high-level student policy. Both policies map states to
subgoals and are employed with a probability of pand1−p,
respectively. The role of the student policy is mainly to help
increase randomness and prevent falling into local optimum. p
is initialized to 0.5 and gradually increases to 1 in the training
process. θhis trained with SAC using episodes in Bl.
V. E XPERIMENTS
A. Environmental Settings
We evaluate HILL on a suite of challenging continuous
control tasks based on the MuJoCo simulator [13]. These tasks
require the agent to master a combination of locomotion and
object manipulation skills. The reward is sparse: the agent gets
(a) Ant Maze
 (b) Ant FourRooms
 (c) Ant Push
(d) HalfCheetah Hurdle
 (e) HalfCheetah Climbing
 (f) HalfCheetah Ascending
Fig. 2. The continuous control environments with sparse rewards for evaluating HILL and baselines.
0 1 2 3 4 5
Steps ×1e60.00.20.40.60.81.0Success
(a) Ant Maze
0 2 4 6 8 10 12 14
Steps ×1e60.00.20.40.60.81.0Success
 (b) Ant FourRooms
0 1 2 3 4 5
Steps ×1e60.00.20.40.60.81.0Success
 (c) Ant Push
0.0 0.5 1.0 1.5 2.0 2.5
Steps ×1e60.00.20.40.60.81.0Success
(d) HalfCheetah Hurdle
0.0 0.5 1.0 1.5 2.0 2.5 3.0
Steps ×1e60.00.20.40.60.81.0Success
 (e) HalfCheetah Climbing
0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
Steps ×1e60.00.20.40.60.81.0Success
 (f) HalfCheetah Ascending
Fig. 3. Learning curves of HILL and baselines on MuJoCo tasks. The x-axis shows the time step of training, and the y-axis shows the average test success
rate of 10episodes. Each line is the mean of 5runs with shaded regions corresponding to the 95% confidence interval.
0when reaching the goal and −1otherwise. During training,
the agent is initialized with random positions and is tested
under the most challenging setting to reach the other side of
the environment. We conduct experiments on six environments
in MuJoCo, and their visualizations are presented in Figure 2.
The maximum episode length is limited to 500 steps for the
Ant Maze, Ant Push, HalfCheetah Hurdle, and HalfCheetah
Ascending tasks and 1000 steps for the Ant FourRooms and
HalfCheetah Climbing tasks.
B. Comparative Experiments
We compare HILL with the SOTA baselines, which include:
(1)HESS [10]: a GCHRL method realizing active exploration
based on stable subgoal representation learning. (2) LESSON
[9]: a GCHRL method learning subgoal representations with
slow dynamics. (3) HIRO [5]: a GCHRL method using off-
policy correction for efficient exploitation. (4) SAC [29]: the
base RL method we used for training the bi-level policies.
Note that we use the original implementations of LESSON
and HESS on all tasks.The experimental results, as presented in Figure 3, demon-
strate that HILL outperforms all baselines in terms of both
sample efficiency and asymptotic performance. This success
can be attributed to the use of efficient subgoal representations
with temporal coherence and a subgoal selection strategy that
effectively balances exploration and exploitation. In compari-
son, HESS underperforms our method due to its focus on ex-
ploring novel and reachable subgoals, lacking consideration of
the potential benefits that subgoals can provide in completing
the source task. LESSON yields an uptrend similar to HILL
in Ant Push in the early stage. However, its unstable subgoal
representations lead to a subsequent drop in performance.
HIRO improves sample efficiency by relabeling subgoals to
maximize the likelihood of past low-level action sequences.
However, it lacks active exploration and fails to achieve a
balance between exploration and exploitation. SAC performs
poorly across all tasks, showing hierarchical advantages in
solving long-term tasks with sparse rewards.
Fig. 4. Episodes in the state space and representations in the latent space at different learning stages in the Ant Maze task.
(a) State DIST. ( 0.5Msteps)
 (b) State DIST. ( 1.5Msteps)
(c) REP. DIST. ( 0.5Msteps)
 (d) REP. DIST. ( 1.5Msteps)
Fig. 5. Visualization of exploration and exploitation in the Ant Maze task
at 0.5 and 1.5 million steps, respectively. The x, y-axes are positions in the
corresponding space, and the z-axis is the visit counts statistically obtained
from the replay buffer with a capacity of 105. “REP.” denotes the word
“representation” and “DIST.” denotes the word “distribution”.
C. Visualization Analysis
1) Exploration and Exploitation: We visualize the explo-
ration and exploitation statuses by counting the visits of states
in the state space and representations in the latent space,
respectively, as shown in Figure 5. With the guidance of the
novelty measure, HILL actively selects subgoals that are less
explored, resulting in an extensive range of representations
in the latent space and a dispersed distribution in the state
space. As training progresses, the cumulative visits to various
latent representations become uniform, and the utility measure
enables more precise estimates and converges gradually. Con-
sequently, episodes in the state and latent spaces gradually
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0
Steps ×1e60.00.20.40.60.81.0Success
(a) Ant Maze
0 1 2 3 4 5
Steps ×1e60.00.20.40.60.81.0Success
 (b) Ant Push
Fig. 6. Ablation studies of critical modules. “B” denotes “Basic GCHRL”,
“CSRL” denotes “Contrastive Subgoal Representation Learning”, “G-N” de-
notes “building Graphs with the Novelty measure”, and “G-N-U” denotes
“building Graphs with the Novelty measure and the Utility measure”.
stabilize and exhibit little difference. The active exploration
in the early stage and adequate exploitation in the later
stage demonstrate the effectiveness of our proposed balanced
strategy.
2) Representation Learning: We run 10episodes under the
most challenging setting of the Ant Maze task and visualize
their subgoal representation learning processes. We use a
color gradient from purple to yellow to indicate the trend of
an episode, as shown in Figure 4, where purple represents
the start position, and yellow represents the end position.
HILL learns representations with temporal coherence based
on environmental transition relationships in an early stage.
Adjacency states along an episode are clustered to similar
latent representations, and ones multiple steps apart are pushed
away in the latent space. These demonstrate the effectiveness
and efficiency of the proposed negative-power contrastive
representation learning objective.
D. Ablation Study on Various Components
We conduct ablation studies on various components by
incrementally adding critical modules to the basic GCHRL
framework. The learning curves are shown in Figure 6. Basic
GCHRL, which solely relies on SAC as the base RL optimizer
of both levels, fails to learn policies efficiently due to the large
exploration space and sparse rewards. The non-stationarity
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0
Steps ×1e60.00.20.40.60.81.0Success0.2
0.3
0.4
0.5
0.8(a) Stable Ratio k
0.0 0.5 1.0 1.5 2.0 2.5 3.0
Steps ×1e60.00.20.40.60.81.0Success
2
3
5 (b) Representation Dimension d
0.0 0.5 1.0 1.5 2.0 2.5 3.0
Steps ×1e60.00.20.40.60.81.0Successn=1 β=0.1
n=1 β=1
n=1 β=10
n=2 β=0.1
n=2 β=1
n=2 β=10
(c) Scaling Factor β, Power n
0.0 0.5 1.0 1.5 2.0 2.5 3.0
Steps ×1e60.00.20.40.60.81.0Successc=10
c=20
c=50
c=100 (d) Subgoal Selection Interval c
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0
Steps ×1e60.00.20.40.60.81.0Success
FPS
w/o FPS
(e) Landmark Sampling Strategy
0.0 0.5 1.0 1.5 2.0 2.5 3.0
Steps ×1e60.00.20.40.60.81.0Success100
200
300
400
500 (f) Landmark Number
Fig. 7. Ablation studies of hyperparameter selection in the Ant Maze task.
introduced by joint training hinders bi-level policy updates.
Our proposed contrastive representation learning objective
compresses the exploration space by considering the temporal
coherence in GCHRL (B+CSRL). The use of HER allevi-
ates non-stationary issues and enables low-level policies to
converge faster, resulting in better bi-level cooperation, thus
improving exploration efficiency (B+CSRL+HER). Building
latent landmark graphs and measuring novelties at nodes help
agents actively explore novel subgoals (B+CSRL+HER+G-N).
Finally, HILL (B+CSRL+HER+G-N-U) is obtained by consid-
ering both the novelty and utility measures on latent landmark
graphs. By selecting the most valuable subgoal that bal-
ances the two measures, HILL realizes effective exploration-
exploitation trade-offs, resulting in the fastest convergence
speed and highest asymptotic performance.
E. Ablation Study on HyperParameters
We set up ablation studies on several typical hyperparam-
eters of HILL in the Ant Maze task. Figure 7 shows the
evaluation results. We conclude that our method is robust over
an extensive range of hyperparameters.
1) Stable ratio k:Sampling the top kof triplets with lower
representation losses helps to stably constrain changes of well-
learned representations. However, a large kmay disturb the
update of subgoal representation learning, while a small kre-
duces the effectiveness of stability regularization, as indicated
in Figure 7 (a). We set k= 0.3for all tasks.
2) Representation Dimension d:ddetermines the degree of
information compression. The smaller dis, the more compact
the representation abstracted by ϕis. The results in Figure
7 (b) indicate that HILL achieves better performance when
d= 2. Therefore, we set dto2for all tasks.3) Scaling Factor β, Power n:βandnboth affect the
performance of subgoal representation learning. A smaller nor
a larger βpushes negative instances further away. As shown in
Figure 7 (c), HILL maintains robustness when βis small and
achieves better performance when n= 1, β= 0.1. Therefore,
we set n= 1, β= 0.1for all tasks.
4) Subgoal Selection Interval c:caffects the subgoal selec-
tion interval and the learning difficulty of the low-level policy.
A smaller cresults in faster convergence of the low-level pol-
icy but makes high-level decision-making more challenging.
This adversarial relationship is verified in Figure 7 (d). We set
c= 50 for all tasks and baselines in our experiments.
5) Landmark Sampling Strategy: Figure 7 (e) shows that
HILL converges faster with FPS than with uniform sampling.
This is due to FPS’s ability to identify representations that
maximize the coverage of the latent space, which facilitates the
agent’s exploration of new regions. Therefore, FPS is adopted
as the landmark sampling strategy for all tasks.
6) Landmark Number: The number of landmarks has a
significant impact on the coverage of the latent space and
the transition distance between nearby landmarks. Inadequate
landmark numbers lead to poor coverage, while excessive
numbers increase the burden of value estimation. The results
in Figure 7 (f) indicate that HILL achieves better performance
when the number is 400. Therefore, we use it as the landmark
number for all tasks.
VI. C ONCLUSION
This work proposes to address the exploration and exploita-
tion dilemma in hierarchical reinforcement learning by dynam-
ically constructing latent landmark graphs (HILL). We propose
a contrastive representation learning objective to learn subgoal
representations that comply with the temporal coherence in
GCHRL, as well as a latent landmark graph structure that bal-
ances subgoal selection for effective exploration-exploitation
trade-offs. Empirical results demonstrate that HILL signifi-
cantly outperforms the SOTA GCHRL methods on numerous
continuous control tasks with sparse rewards. Visualization
analyses and ablation studies further highlight the effectiveness
and efficiency of various HILL components.
REFERENCES
[1] P. Dayan and G. E. Hinton, “Feudal reinforcement
learning,” Advances in Neural Information Processing
Systems , vol. 5, 1992.
[2] R. S. Sutton, D. Precup, and S. Singh, “Between mdps
and semi-mdps: A framework for temporal abstraction in
reinforcement learning,” Artificial intelligence , vol. 112,
no. 1-2, pp. 181–211, 1999.
[3] A. G. Barto and S. Mahadevan, “Recent advances in hier-
archical reinforcement learning,” Discrete event dynamic
systems , vol. 13, no. 1, pp. 41–77, 2003.
[4] T. D. Kulkarni, K. Narasimhan, A. Saeedi et al. , “Hierar-
chical deep reinforcement learning: Integrating temporal
abstraction and intrinsic motivation,” Advances in neural
information processing systems , vol. 29, 2016.
[5] O. Nachum, S. S. Gu, H. Lee et al. , “Data-efficient
hierarchical reinforcement learning,” Advances in Neural
Information Processing Systems , vol. 31, 2018.
[6] A. S. Vezhnevets, S. Osindero, T. Schaul et al. , “Feudal
networks for hierarchical reinforcement learning,” in In-
ternational Conference on Machine Learning , 2017, pp.
3540–3549.
[7] N. Dilokthanakul, C. Kaplanis, N. Pawlowski et al. ,
“Feature control as intrinsic motivation for hierarchical
reinforcement learning,” IEEE Transactions on Neural
Networks and Learning Systems , vol. 30, no. 11, pp.
3409–3418, 2019.
[8] T. Zhang, S. Guo, T. Tan et al. , “Generating adjacency-
constrained subgoals in hierarchical reinforcement learn-
ing,” Advances in Neural Information Processing Sys-
tems, vol. 33, pp. 21 579–21 590, 2020.
[9] S. Li, L. Zheng, J. Wang et al. , “Learning subgoal
representations with slow dynamics,” in International
Conference on Learning Representations , 2020.
[10] S. Li, J. Zhang, J. Wang et al. , “Active hierarchical ex-
ploration with stable subgoal representation learning,” in
International Conference on Learning Representations ,
2021.
[11] B. Eysenbach, R. R. Salakhutdinov, and S. Levine,
“Search on the replay buffer: Bridging planning and
reinforcement learning,” Advances in Neural Information
Processing Systems , vol. 32, 2019.
[12] S. Emmons, A. Jain, M. Laskin et al. , “Sparse graphical
memory for robust planning,” Advances in Neural In-
formation Processing Systems , vol. 33, pp. 5251–5262,
2020.
[13] E. Todorov, T. Erez, and Y . Tassa, “Mujoco: A physics
engine for model-based control,” in IEEE/RSJ Interna-
tional Conference on Intelligent Robots and Systems ,
2012, pp. 5026–5033.
[14] O. Nachum, H. Tang, X. Lu et al. , “Why does hierarchy
(sometimes) work so well in reinforcement learning?”
arXiv:1909.10618 , 2019.
[15] A. Levy, G. Konidaris, R. Platt et al. , “Learning multi-
level hierarchies with hindsight,” in International Con-
ference on Learning Representations , 2019.
[16] A. P ´er´e, S. Forestier, O. Sigaud et al. , “Unsupervised
learning of goal spaces for intrinsically motivated goal
exploration,” arXiv preprint arXiv:1803.00781 , 2018.
[17] S. Nair and C. Finn, “Hierarchical foresight: Self-
supervised learning of long-horizon tasks via visual sub-
goal generation,” arXiv preprint arXiv:1909.05829 , 2019.
[18] D. Ghosh, A. Gupta, and S. Levine, “Learning actionable
representations with goal-conditioned policies,” arXiv
preprint arXiv:1811.07819 , 2018.
[19] L. Wiskott and T. J. Sejnowski, “Slow feature analysis:
Unsupervised learning of invariances,” Neural computa-
tion, vol. 14, no. 4, pp. 715–770, 2002.
[20] L. Li, R. Yang, and D. Luo, “Focal: Efficient fully-offline
meta-reinforcement learning via distance metric learning
and behavior regularization,” in International Conferenceon Learning Representations , 2020.
[21] J. Zhang, N. Wetzel, N. Dorka et al. , “Scheduled intrin-
sic drive: A hierarchical take on intrinsically motivated
exploration,” arXiv:1903.07400 , 2019.
[22] M. C. Machado, M. G. Bellemare, and M. Bowling,
“Count-based exploration with the successor represen-
tation,” in Proceedings of the AAAI Conference on Arti-
ficial Intelligence , vol. 34, no. 04, 2020, pp. 5125–5133.
[23] F. R ¨oder, M. Eppe, P. D. Nguyen et al. , “Curious
hierarchical actor-critic reinforcement learning,” in Inter-
national Conference on Artificial Neural Networks , 2020,
pp. 408–419.
[24] K. Yamamoto, T. Onishi, and Y . Tsuruoka, “Hierarchical
reinforcement learning with abductive planning,” arXiv
preprint arXiv:1806.10792 , 2018.
[25] J. Li, C. Tang, M. Tomizuka et al. , “Hierarchical planning
through goal-conditioned offline reinforcement learning,”
arXiv:2205.11790 , 2022.
[26] W. Shang, A. Trott, S. Zheng, C. Xiong, and R. Socher,
“Learning world graphs to accelerate hierarchical rein-
forcement learning,” arXiv:1907.00664 , 2019.
[27] J. Jin, S. Zhou, W. Zhang, T. He, Y . Yu, and R. Fakoor,
“Graph-enhanced exploration for goal-oriented reinforce-
ment learning,” 2021.
[28] L. Zhang, G. Yang, and B. C. Stadie, “World model
as a graph: Learning latent landmarks for planning,” in
International Conference on Machine Learning , 2021,
pp. 12 611–12 620.
[29] T. Haarnoja, A. Zhou, P. Abbeel et al. , “Soft actor-critic:
Off-policy maximum entropy deep reinforcement learn-
ing with a stochastic actor,” in International Conference
on Machine Learning , 2018, pp. 1861–1870.
[30] T. Schaul, D. Horgan, K. Gregor et al. , “Universal value
function approximators,” in International Conference on
Machine Learning , 2015, pp. 1312–1320.
[31] S. Vassilvitskii and D. Arthur, “k-means++: The ad-
vantages of careful seeding,” in Proceedings of the
Eighteenth Annual ACM-SIAM Symposium on Discrete
Algorithms , 2006, pp. 1027–1035.
[32] H. Tang, R. Houthooft, D. Foote et al. , “# exploration: A
study of count-based exploration for deep reinforcement
learning,” Advances in Neural Information Processing
Systems , vol. 30, 2017.
[33] M. S. Charikar, “Similarity estimation techniques from
rounding algorithms,” in Proceedings of the Thiry-fourth
Annual ACM Symposium on Theory of Computing , 2002,
pp. 380–388.
[34] J. M. McQuillan and D. C. Walden, “The arpa network
design decisions,” Computer Networks , vol. 1, no. 5, pp.
243–289, 1977.
[35] M. Andrychowicz, F. Wolski, A. Ray et al. , “Hindsight
experience replay,” Advances in neural information pro-
cessing systems , vol. 30, 2017.