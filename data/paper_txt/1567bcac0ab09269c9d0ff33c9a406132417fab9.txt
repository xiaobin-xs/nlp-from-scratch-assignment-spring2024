A Pretrainer’s Guide to Training Data:
Measuring the Effects of Data Age, Domain Coverage,
Quality, & Toxicity
Shayne Longpre1♢*Gregory Yauney2♢*Emily Reif3♢Katherine Lee2,3♢
Adam Roberts3Barret Zoph4†Denny Zhou3Jason Wei4†Kevin Robinson3
David Mimno2♢Daphne Ippolito3♢
1MIT2Cornell University3Google Research4OpenAI
Abstract
Pretraining is the preliminary and fundamental step in developing capable language models (LM). Despite
this, pretraining data design is critically under-documented and often guided by empirically unsupported
intuitions. To address this, we pretrain 28 1.5B parameter decoder-only models, training on data curated (1)
at different times, (2) with varying toxicity and quality filters, and (3) with different domain compositions.
First,wequantifytheeffectofpretrainingdataage. Atemporalshiftbetweenevaluationdataandpretraining
data leads to performance degradation, which is not overcome by finetuning. Second, we explore the effect
of quality and toxicity filters, showing a trade-off between performance on standard benchmarks and
riskoftoxicgenerations. Ourfindingsindicatetheredoesnotexistaone-size-fits-allsolutiontofiltering
trainingdata. Wealsofindthattheeffectsofdifferenttypesoffilteringarenotpredictablefromtextdomain
characteristics. Lastly, we empirically validate that the inclusion of heterogeneous data sources, like books
andweb,isbroadlybeneficialandwarrantsgreaterprioritization. Thesefindingsconstitutethelargestsetof
experiments to validate, quantify, and expose many undocumented intuitions about text pretraining, which
we hope will help support more informed data-centric decisions in LM development.
2012 Eval TasksToxic GenerationToxic Identification2020 Eval TasksDomain-Specific Knowledge
CodeWebPubmedAcademicWikipediaBooks2013201620192022ToxicLow qualityEvaluate Change in Performance on Downstream TasksSelect Pretraining DataPretrain Model
Figure 1: The experimental pretraining curation pipeline includes three steps: sub-selecting data from C4 or the Pile,
pretraining a language model, and evaluating its change in performance over several benchmarks.
*Work completed while a Student Researcher at Google Research.
†Work completed at Google Research.
♢Core contributor. Correspondence: slongpre@media.mit.eduarXiv:2305.13169v2  [cs.CL]  13 Nov 2023
Contents
1 Introduction 3
2 Methodology 4
2.1 Pretraining Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
2.2 Data Curation Choices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
2.3 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
2.4 Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
3 Impact of Data Curation on Data Characteristics 7
4 Impact of Dataset Age on Pretrained Models 9
5 Impact of Quality & Toxicity Filters on Pretrained Models 11
6 Impact of Domain Composition on Pretrained Models 13
7 Discussion 15
8 Limitations 16
9 Related Work 17
10 Conclusion 19
Appendix 28
1 Introduction
Thestrongperformance(Chowdheryetal.,2022;Nostalgebraist,2022;OpenAI,2023;Google,2023),and
emergentabilities(Weietal.,2022)ofmodernlanguagemodels(LMs)dependonself-supervisedpretraining
on massive text datasets. All model developers implicitly or explicitly decide the composition of these
datasets: what data sources to include, whether to filter for attributes such as quality and toxicity, and
when to gather new documents. While many of the most prominent models do not document their curation
procedures (OpenAI, 2023; Google, 2023), or only document whichprocedures they used (Brown et al., 2020;
Nostalgebraist, 2022; Scao et al., 2022; Touvron et al., 2023), they rarely document whythey chose those
protocolsorwhateffecttheyhad. Thisdocumentationdebtleavespractitionerstobeguidedbyintuitions
and precedents, neither thoroughly evaluated (Bandy and Vincent, 2021; Sambasivan et al., 2021). Given the
outsizedandfundamentalroleofpretrainingdatainmodernLMs,webelievethisneglectfulpracticehas
detracted from responsible data use and hampered effective model development (Rogers, 2021; Gebru et al.,
2021; Bender and Friedman, 2018).
Amongthesmallnumberofgeneral-purposeLMsdominatingcommunityuseanddiscussion,theprevailing
focus has been on the scale of pretraining data and number of optimization steps (Brown et al., 2020;
Nostalgebraist, 2022; Google, 2023). In this work, we systematically test how common data design decisions
affectmodelperformance—specifically: thetimeofcollection,contentfilteringstrategy(toxicity/quality),
and domain composition. We study the impacts in two ways. First, we present observational measurements
of the effect of existing quality and toxicity filtering methods (Section 3). We document how these filters
affectarangeofcharacteristicsintwomajorpretrainingdatasets,C4(Raffeletal.,2020)andthePile(Gao
et al., 2020). Second, we rigorously evaluate these dataset design decisions on downstream tasks. This
is done by evaluating decoder-only autoregressive LMs each pretrained on a dataset modified along one
dimension oftime, toxicity, quality, ordomain composition. Our contributions aresummarized as findings
and recommendations to model developers.
The Age of a Dataset (Section 4). We see performance degradation if evaluation data is eitherbefore or
afterpretraining data collection, and this deficit isn’t overcome with substantial finetuning. Further, this
phenomenonisexacerbatedinlargermodels. Whilerarelyacknowledged,weshowitseffectcanmeaningfully
complicate comparisons between new and old models, depending on the age of the evaluation dataset.
QualityandToxicityFilters(Section5). Filteringfordocumentqualityandtoxicityhavesignificantbut
oppositeeffectsonmodelbehaviour. Qualityfiltering,removinglow-qualitytext,substantiallyincreasesboth
toxic generation and downstream performance across tasks we tested, despite reducing the amount of training
data. Ontheotherhand,removingtoxicdatatrades-offfewertoxicgenerationsforreducedgeneralization
performance. Inverse toxicity filters, which remove the least toxic content, demonstrate targeted benefits.
Lastly,evaluationondatasetswithhighqualitytextaren’tnecessarilyimprovedbyremovinglow-quality
text from the dataset. Performance effects due to quality filtering are mostly positive, but the benefits are not
predictable from text characteristics. These findings demonstrate that one size (filter) does not fit all , and there
is a need for practitioners to develop more targeted quality or inverse toxicity filters for their tasks.
Domain Compositions (Section 6). The best performing domains comprise high-quality (Books) and
heterogeneous(Web)data,corroboratingBrownetal.(2020);Chowdheryetal.(2022);Xieetal.(2023a).
However,thesetextsourcescontributemosttotoxicgeneration. Still,wefoundthatthebenefitsoftrainingon
thesedatasourcesisoftengreaterthandatacollectionforatargeteddomain,andsorecommendpractitioners
focus future collection on more books and diverse web data. Additionally, our best performing models still
usealldatasources(evenattherelativelysmallscaleof1.5Bparameters); thus,werecommendpractitioners
generously include data sources less relevant to their downstream tasks (Madaan et al., 2022).
Toourknowledge,theseexperimentsconstitutethelargestpubliclydocumentedLMdatacurationstudy,
spanning 28 1.5B parameter models. Their findings empirically quantify, validate, and, occasionally, challenge the
entrenchedsetofunder-examinedpretrainingassumptions;whichwebelievejustifiestheircomputational
cost (Section 8). As the majority of the community has adopted a small set of models for most research and
applications (BERT, T5, GPT-2, GPT-3), pretraining data curation decision have long-term ramifications. We
hope these results better inform model developers training the next wave of LMs.
3
Table1: Alistof well-knownlanguagemodelsandaquantitativebreakdownoftheirpretrainingdata ,
including represented domains; if the Pile or C4 are used, the percent of multilingual (M-L) data (meaning
non-English and non-code); if Toxicity or Quality data filters were used, as either automatic Heuristics (H)
orClassifiers(C);ifthedatasetispublic(Pub),andwhatyearthedatawascollectedupto. Ifadatasetis
“Part”public,thenallofitsconstituentcorporaarepublic,butnotthefinalmixture. InRepresentedDomains,
extended from (Zhao et al., 2023), Web includes the Common Crawl and other web scrapes; Dialog includes
forum, social media and conversations; Academic includes research papers, textbooks, and mathematics.
Represented Domains (%) Filters Data
Model Wiki Web Books Dialog Code Acad Pile C4 M-L Tox Qual Pub Year
Bert 76 24 ✗ ✗ HPart 2018
GPT-2 100 ✗ ✗ HPart 2019
RoBerta 790 3 ✗ ✔ HPart 2019
XLNet 889 3 ✗ ✔ HPart 2019
T5 <199 ✗ ✔ H H ✔2019
GPT-3 382 16 ✗ ✔ 7% C ✗2021
GPT-J/Neo 1.538 15 4.5 1328 ✔Part C ✔2020
GLaM 646 20 28 ✗ ✔ C ✗2021
LaMDA 1324 5013 ✔ ✔ 10% C C ✗2021
AlphaCode 100 ✗ ✗ H ✗2021
CodeGen 124 10 34022 ✔Part HPart 2020
Chinchilla 165 10 4 ✔ ✔ H C ✗2021
Minerva <11.5 <1 2.5<195 ✔ ✔ <1% C ✗2022
BLOOM 560 10 51010 ✔ ✔ 71% H C Part 2021
PaLM 428 13 50 5 ✗ ✔ 22% C ✗2021
Galactica 17 1 784 ✔Part HPart 2022
LLAMA 4.582 4.5 24.52.5Part ✔4% CPart 2020
2 Methodology
We measure how pretraining data curation choices affect downstream performance. Figure 1 illustrates
ourapproach: eachexperimentstartswithapretrainingdataset,appliesafilterthatremovesdocuments,
pretrains a language model on the curated dataset, and finally evaluates the model on downstream tasks.
2.1 Pretraining Datasets
Webeginwithtwocommon,publiclyavailablepretrainingdatasets: C4(Raffeletal.,2020)andthePile(Gao
et al., 2020). Both have received basic initial heuristic filtering for English language and content quality. We
furtherdeduplicatebothdatasetsusingtheapproximatededuplicationmethoddescribedinLeeetal.(2022).
C4 (Raffel et al., 2020) The English Colossal Clean Crawled Corpus (C4) is a snapshot of Common Crawl
from 2019, which includes a mix of news, legal, wikipedia, and generic web documents (Dodge et al., 2021),
filteredforwell-formedEnglishtext.∗WhiletheoriginalversionofC4filteredoutanydocumentscontaining
words from a “bad words list”, our version does not. C4 remains one of the most widely adopted fully open
source datasets for textual training, given its permissive license. It is a key component of many LMs, as
shown in Table 1.
ThePile(Gaoetal.,2020) isan800GBdatasetconsistingofdatafrom22sources. TheseincludeaCommon
Crawlwebscrapeaswellasmorediversecollectionsofacademic,books,coding,medical,legalandsocial
sources (see Table 8), which more closely resemble the reported data sources in larger non-open source
modelslikePaLM(Chowdheryetal.,2022),Chinchilla(Hoffmannetal.,2022),andtheGPT-3series(Brown
∗https://commoncrawl.org/
4
etal.,2020). NotethatthePile’scorporacompositionwasmanuallyselected,andsomeoptionswereexcluded
on the grounds of being too toxic or explicit.
2.2 Data Curation Choices
We evaluate variations in the pretraining data based on three categories of interventions.
Dataset Age We create new versions of C4 by regenerating snapshots of the Common Crawl from different
years (see Figure 10). Multiple time-based collections are not available for the Pile.
Domain Filtering Both C4 and the Pile draw from multiple distinct data sources, but the Pile explicitly
delineates22distinctsourcesfromwebpages,wikipediaarticles,coderepositories,onlineforums,legaltexts,
andresearchpaperarchives. Tocontrolforthetopicalcontentofthepretrainingcollection,weselectively
remove documents from different domains (see Table 8).
ContentFiltering DatasetsderivedfromtheCommonCrawlandotherweaklycuratedinternetsourcestend
to contain large amounts of low-quality, toxic, or offensive content. As a result, curators often apply content-
based filters. Deciding what to include and what not to include is a challenging and context-dependent
problem: A“high-quality”Redditpostdoesnotlooklikea“high-quality“academicpaper;andevenwith
academic papers, quality measured by peer review has high variance (Cortes and Lawrence, 2021).
Thereareseveralapproachestodeterminingdocumentappropriateness. Thesimplestfiltersusefeatures
such as sentence length, presence of stopwords and punctuation, and repetitiousness to identify pages that
do not contain usable text (Rae et al., 2021; Yang et al., 2019; Laurençon et al., 2022; Zhang et al., 2022).
Negatively-defined filters identify a category of text to be removed, and assume that everything else is
usable. Forexample,Raffeletal.(2020)removedocumentsthatcontainwordsfromalistof“badwords”.
Positively-defined filters identify a category of text to keep, and remove everything else (Du et al., 2022;
Touvron et al., 2023; Brown et al., 2020).
Inthiswork,weevaluatetheimpactoftwodocument-level,classifier-basedfiltersthathavebeenusedwidely
in the development of state-of-the-art language models. These include negatively-defined, toxiccontent
(textthatisprofane,explicit,insulting,orthreatening)andpostively-defined qualitycontent(textsimilar
toknown“high-quality”sources). Itisimportanttoemphasizethatwedonothavegroundtruth: forthe
purposes of this paper we will use the description toxicorqualityto refer to a document that triggers one of
these automated classifiers, notto indicate a document that achieves those characteristics for a human reader.
QualityFilters Mostrecentlanguagemodelscreatequalityclassifierstodistinguishbetween“high-quality”
corpora and other documents (Table 1). These are usually then applied to crawled web pages. Examples of
high-quality reference corpora are (1) Wikipedia, WebText and books for GPT-3 (Brown et al., 2020), (2)
Wikipedia,booksandafewselectedwebsitesforPaLM(Chowdheryetal.,2022)andGLaM(Duetal.,2022),
and(3)pagesusedasreferencesinWikipediaforLLaMA(Touvronetal.,2023). Inourwork,weusethe
classifieremployedbyPaLMandGLaM,whichassignseachdocumentascorefrom0(highquality)to1
(low quality). We experiment with removing documents that fall above four quality thresholds: 0.975,0.95,
0.9,0.7, along with an inverse filter that instead removes the highestquality documents belowa threshold.
ToxicityFilters Toidentifytoxiccontent,weuseJigsaw’sPerspectiveAPI†,whichwastrainedoncomments
fromonline forumsand assignstoxicityscores basedon whetherannotatorsfound thecomment tocontain
profanity/obscenity, identity-based negativity, insults, or threats. While the Perspective API, as with any
classifier,hasbeenshowntobeimperfect—itfalselylabelssomeneutraltextastoxicanditstrainingdata
reflects the normative values of its annotators—it has been shown to be far more accurate than heuristic and
rule-based classifiers (Friedl, 2023; Gargee et al., 2022; Lees et al., 2022).
†https://www.perspectiveapi.com
5
The Perspective API outputs a score from 0 (unlikely to be toxic) to 1 (very likely to be toxic). The documen-
tationrecommendsusingascorethresholdofanywherefrom0.3to0.9tofilterdocuments,dependingon
the practitioner’s goals.‡We experiment with removingdocuments with scores above five different toxicity
thresholdvalues 0.95,0.9,0.7,0.5,and 0.3. Documentsaboveagiventhresholdarefilteredout,alongwith
an inverse filter that removes documents with the leastpredicted toxicity belowa threshold.
In addition to the classifier-based filter, we also experiment with the n-gram based filter used by Raffel et al.
(2020) in the original version of the C4 dataset. This filter removes all documents that contain any word
present in the “List of Dirty, Naughty, Obscene, or Otherwise Bad Words”.§
2.3 Evaluation
Tomeasure theeffectsoftime, topicand toxicity, weevaluatepretrained modelsonEnglish-language tasks
for toxicity identification, toxic generation, dozens of question-answering (QA) tasks from diverse domains,
and several tasks with temporal annotations. In choosing evaluations, we compare the general utility of the
differentmodels,aswellastheirperformanceontasksweexpecttobeinfluencedbythedatasetcharacteristics
being ablated. Since we are comparing the performance of different pretrained models, we evaluate the
performance of each pretrained model on downstream tasks by finetuning the model on the relevant dataset
for each task and evaluated on the same testing data (using the default splits for each task unless otherwise
noted). Asaresult,any systematic differencesbetweenfinetunedresultscanonlybeattributabletodifferences
inpretraining. For alltaskswereportmean performancerelativetoa baseline,usuallytheperformance of
models trained on an unfiltered dataset.
Evaluating Domain Generalization We evaluate on the union of two question-answering benchmarks:
Machine Reading for Question Answering (MRQA) (Fisch et al., 2019) and UnifiedQA (Khashabi et al.,
2020),whichtogetherconsistof30uniqueQAdatasets. TheseQAdatasetsspanarangeofdomains,allowing
us to measure the impact of topic alignment (see Table 9).
EvaluatingTemporalMisalignment Priorworkhasshownthatadataset’scollectiontimecanaffectthe
downstream model’s abilities (Lazaridou et al., 2021; Agarwal and Nenkova, 2022). Luu et al. (2021) release
several datasets in which increasing temporal distance between finetuning and evaluation time decreases test
performance. Wechoose5ofthesedatasetsfromvaryingdomainstoevaluatewhetherasimilarphenomenon
exists between pretraining and evaluation time: PubCLS, NewSum, PoliAffs, TwiERC, and AIC.
Evaluating Toxic Generation Generateing profane, sexually explicit, insulting, or obscene text or text that
attacksidentitygroupsortargetsprotectedhumanattributeslimitstheapplicationsLMsmaybeusedfor
(Gehman etal., 2020). We evaluate this behavior with languagemodel promptsdesigned toelicit biasedor
toxic outputs related to gender, race, and religion (Chowdhery et al., 2022), and then measuring the fraction
ofgeneratedcontinuationswhichareassignedahightoxicityscorebythePerspectiveAPI(seeAppendix
C.3fordetails). WealsousetheRealToxicityPromptsdataset(Gehmanetal.,2020),whichconsistsoftext
excerpts from the OpenWebText dataset (Gokaslan* et al., 2019) that were labeled as toxic by the Perspective
API.
Evaluating Toxicity Identification While some applications require LMs not to generate toxic text, in
other applications it is important for LMs to recognize such language. Toxicity Identification has become
particularly critical asa step incontent moderation formajor communication platforms(NYT, 2020; Singh,
2019). Definitionsvarybysetting,targetinghatespeech,stereotypes,socialbias,orsomedefinitionoftoxicity.
Weevaluatethisabilitywithavarietyoftoxicityinterpretations,usingtrainandtestsetsfromSocialBias
Frames (SBF, Sap et al., 2020), DynaHate (DH, Vidgen et al., 2021), and Toxigen (Hartvigsen et al., 2022).¶
‡Seehttps://developers.perspectiveapi.com/s/about-the-api-score
§https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words
¶We use the offensiveness detection task from Social Bias Frames. DynaHate releases 4 rounds of adversarial datasets, for which we
use the test sets for Round 3 (R3) and Round 4 (R4).
6
2.4 Models
Forallourexperiments,weusetwosizesofdecoder-only,Transformer-basedlanguagemodels,trainedin
theT5Xcodebase(Robertsetal.,2022). Ourmainexperimentsuse LM-XL,a1.5Bparameterdecoder-only
modelsimilartothet5.1.1-XLarchitectureconfigurationtrainedwithanautoregressivenext-token-prediction
objective. Forexperimentsthatmeasurescalingeffects,weuse LM-Small ,a20Mparameterdecoder-only
model similar to the t5.1.1-small configuration. These configurations are popular, show decent performance
(Wangetal.,2022)andcangeneratetextwithoutadditionalfinetuning. Additionaldetailsonpretraining
and finetuning are available in Appendix C
3 Impact of Data Curation on Data Characteristics
Section Findings
•The Pile’s documents are on average longer, more readable and higher quality than documents in
C4 but contain more personally identifiable information (PII).
•Books is an outlier domain, having the longest, most readable, most toxic, and most PII-filled
documents, while also containing high-quality text.
•High toxicity and low quality documents have similarly high PII amounts but otherwise have very
different average length and quality and toxicity levels.
•More recent web-scraped text is more diverse and less toxic but also lower quality.
Beforeevaluatingtheeffectofdataablationsonmodels,wepresentobservationalstatisticsonthepretraining
datasetsthemselves. ThisanalysisrevealshowthePile’sdomainscomparetoC4andtooneanother,and
how curation or filtering choices impact features of the data, sometimes inadvertently. We find that there are
substantial interactions between curation choices.
We calculate a range of features for each document, including toxicity and quality metrics; categories of
personallyidentifiableinformation(PII);andtextstatisticssuchasaveragewordlength,readability,type-
token ratio, and sentiment. For more details and analysis on these features see Appendix D.
C4vs thePile Figure9 showsthe differencesbetween thetwosource datasets. Documentsin thePileare
on average longer (2.4x), have more non-ASCII characters (1.9x) indicating greater linguistic range, and are
alsomeasuredashigher quality(1.2x) andmore readable(1.8x). Pile documentsalso containmore PII,in
particular personal names, addresses, and emails.
Toxicity and Quality While it is reasonable to assume that high toxicity should correlate with low quality,
Figure2showsthattherelationshipismorecomplicated: infact,toxicityandqualityarenotwell-aligned
withoneanother. Hightoxicitydocumentshavehighertextqualitythanlowtoxicitydocuments. Thereis
also little discernible difference in feature measurements for profanity, toxicity, and sexually explicit content
between content classified as low vs. high quality.
Domains Looking at characteristics of the Pile by domain in Figure 2 suggests an explanation. The Books
subset stands out as having substantially more profane, toxic, and sexual content, but also greater predicted
quality. While we might expect books to be high quality, in the sense that they typically contain meaningful,
well-edited sentences, they also contain strong language and erotic subjects. This may also explain why
documentsclassifiedashightoxicityinbothC4andthePilearemuchlonger(2.5xand3.5xrespectively),
more profane (5x and 4.4x), sexually explicit (4.6x and 4.2x), and toxic (3.6x and 3.5x). However, Pile
documents with high toxicity are 1.4-1.9 times more likely to have PII of various kinds, while in C4 this is not
true. Documents classified as high quality in C4 were longer (1.3x and 1.2x), and had more names (1.6x and
1.8x), but fewer emails, addresses, and phone numbers.
7
CCOpen-
Web WikiPub-
Med Books CodeAca-
demic Legal Social C4
Profanity
0.111.0x 1.0x .5x .7x4.3x
1.4x 1.6x.6x1.7x.9x
Toxicity
0.151.1x 1.2x.6x .7x3.5x
1.0x .9x .7x1.6x1.1x
Sexually
Explicit
0.121.0x 1.0x .8x 1.0x4.1x
1.1x 1.4x.8x 1.3x 1.0x
Text
Quality0.311.2x1.8x3.0x
.3x1.9x
.3x .3x1.0x .9x .8x
Has
Person
Name 0.411.3x1.7x 1.7x
.3x2.4x
.2x1.5x1.0x 1.2x .9x
Has
Email
0.031.4x 1.4x.5x .0x>5x
.7x4.1x
.1x1.9x.9x
Has
Address
0.04.7x .4x .7x .6x>5x
1.0x>5x3.8x
.7x .6x
Has
Phone
0.021.5x 1.1x.0x .3x>5x
.6x .9x .6x2.3x1.4x
Number
Of
Characters6154.7x .6x .5x 1.0x>5x
.6x>5x
1.5x2.7x
.4x
Type
Token
Ratio0.491.1x 1.1x 1.1x 1.0x
.2x.8x.6x.8x1.0x 1.2x
%
Non-Ascii
Characters 5.67e-03 .5x3.7x
.5x .1x .6x1.1x.3x .2x1.9x
.5x
%
All
Caps0.03 .8x .8x.6x1.2x.8x1.3x 1.3x1.1x1.4x
.8x
Sentiment 0.971.1x .9x 1.0x 1.0x 1.0x .9x 1.0x .9x 1.0x1.2x
Readability
18 .6x .7x .8x .8x.5x2.1x
.8x .7x 1.0x.5xDomains in The Pile(a) Domains
Low
ToxicityHigh
ToxicityHigh
QualityLow
Quality
0.10 .6x5.0x
1.1x .9x
0.16.7x3.6x
1.1x .9x
0.12 .6x4.6x
1.0x 1.0x
0.261.0x 1.2x1.6x
.1x
0.371.0x1.5x 1.3x
.6x
0.031.0x .9x .9x1.2x
0.021.0x 1.1x.8x1.3x
0.031.0x.8x .7x1.5x
2592.8x2.5x
1.2x.6x
0.581.0x.9x 1.0x 1.1x
2.91e-031.0x1.3x 1.2x.8x
0.021.0x1.2x1.0x 1.1x
1.161.0x.9x .9x1.1x
101.0x.8x1.0x 1.0xC4 (b) Toxicity/quality in C4
Low
ToxicityHigh
Toxicity
0.11.7x4.4x
0.15.8x3.5x
0.12.7x4.2x
0.311.0x1.1x
0.411.0x1.5x
0.031.0x1.5x
0.04.9x1.9x
0.021.0x1.4x
6154.8x3.5x
0.491.0x.8x
5.67e-03 .8x.6x
0.031.0x 1.0x
0.971.0x .9x
181.0x.8xThe Pile (c) Toxicity/qualityin
the Pile
Figure 2: Feature differencesacross slices of thepretraining datasets. Bars show the ratio between themean feature
valueforthesliceandthemeanvalueforthedataset(thePileorC4),whichisindicatedbyahorizontalgrayline. For
example, Wiki text has half the profanity and three times the qualityvalues as the average for the Pile.
Among the domains we studied, OpenWeb provides the most lexical and linguistic diversity, with the
highest non-ASCII characters and type-token ratio. Wikipedia presents the highest quality text, before Books
and OpenWeb. Technical domainssuch asPubMed, Code, and Academic score low onpredicted quality,
indicating that overly-specific positively-defined filters on web documents may remove substantial amounts
of potentially useful specialized text.
TimeComparingacrossdifferentcollectiontimesofC4(inFigure9),weseeacoupleofsteadytrends. The
percentage of non-ASCII characters increased steadily in more recent years while the measured text quality
declines. This growth may be due to increasing non-English content, but could also correspond to rising use
ofemojisandnon-ASCIIpunctuation. Toxicityscoresalsodecreaseslightlyinlateryears,whilesentiment
increases.
8
4 Impact of Dataset Age on Pretrained Models
Section Findings
•Both models and evaluation datasets become stale.
•Temporal misalignment between pretraining and evaluation data is not overcome by finetuning.
•Temporal misalignment complicates evaluation of models trained at different times, as older
evaluationdatasetsmaybecomestaleandnewerevaluationdatasetsmayunder-estimateperformance
of older models.
•The effects of pretraining misalignment are stronger for larger models than smaller models.
Whilemodelsarefrequentlyandcheaplyupdatedwithnewfinetuningdata,theexpenseofpretrainingmeans
theNLPcommunityhasreliedonrelativelyfewstaticpretrainedmodelsthatarerarelyupdatedorexchanged.
BERT,RoBERTa,GPT-2,andT5variants,allpretrainedpriorto2020,constitutethemajority(estimatedat
~58%asofApril16,2023)ofallmodelsdownloadedonHuggingFace. Priorworkdemonstratesthatlanguage
use changes over time (Altmann et al., 2009; Labov, 2011) and that temporal misalignment between finetuning
andevaluationdatasetscorrelateswithdegradedperformance,visibleacrosssettingsanddomains(Luu
etal.,2021;Lazaridouetal.,2021;AgarwalandNenkova,2022;Jangetal.,2022). Incontrast,weexamine
the effect of temporal misalignment between pretraining data and evaluation. In evaluating the impact of
pretraining time across data domains, we can quantify the impact this design choice has on NLP broadly.
2010 2012 2014 20162013
2016
2019
2022Pretrain Years78.9 79.2 78.5 75.1
76.8 78.7 79.0 76.3
75.0 76.3 77.1 73.2
74.0 75.7 76.8 73.4PubCLS
2010 2012 2014 20162013
2016
2019
202223.6 32.1 27.7 17.9
23.3 32.0 27.9 19.1
22.8 31.2 27.4 18.1
22.7 31.2 27.4 17.8NewSum
2014 2015 2016 2017 2018 20192013
2016
2019
202285.0 84.9 84.9 82.7 83.1 83.5
85.2 84.8 85.9 83.1 83.4 83.4
84.6 84.4 84.6 84.0 83.8 84.6
82.7 83.7 84.4 82.9 82.7 83.6TwiERC
2014 2015 2016 2017 2018 2019
Eval Years2013
2016
2019
2022Pretrain Years98.2 98.0 95.0 91.3 94.4 88.7
98.1 98.2 95.2 93.1 95.1 88.5
97.8 98.7 93.9 93.4 96.0 90.5
97.6 98.4 94.4 91.4 95.1 89.0AIC
2012 2013 2014 2015 2016 2017 2018 2019 2020 2021
Eval Years2013
2016
2019
202282.7 89.0 91.2 71.2 70.8 74.7 71.5 82.0 82.2 74.9
80.4 88.1 90.6 70.9 72.3 75.8 72.6 82.2 82.4 76.0
80.2 87.8 90.7 70.4 72.0 75.8 73.4 83.1 82.8 75.9
79.4 87.1 89.4 70.8 71.4 75.0 71.0 82.5 83.3 76.8PoliAff
Figure 3: Temporal Misalignment between Pretraining and Evaluation causes performance degradation . Four
LM-XL’s, each pretained on a different C4 time split, are evaluated on each time split across five datasets. Heatmap
colors are normalized by column, following Luu et al. (2021) to show the best pretraining year for each evaluation year.
We pretrain four autoregressive language models on versions of C4: 2013, 2016, 2019, and 2022. For each
version we begin with Common Crawl data and remove all data that was scraped after the cutoff year.
FollowingLuuetal.(2021),wemeasuretheeffectoftemporalmisalignmentbyusingevaluationtasks(from
News, Twitter, and Science domains) that have training and test sets split by year. After pretraining, we
finetuneeachmodeloneachdataset’straining-yearsplitseparately,thenevaluateoneverytest-yearsplit.
Full details and results are in Appendix C.4 and Appendix E.1, respectively.
First,wereplicatetheperformancedegradationobservedbyLuuetal.(2021)duetofinetuningandevaluation
misalignment on the five tasks in Figure 12. Next, we estimate the effects of temporal misalignment between
pretraining and evaluation (Figure 3). Since all models were finetuned on the training sets of the evaluation
9
tasks, we show that temporal misalignment during pretraining persists even with temporally-relevant
finetuning data.
-8 -7 -6 -5 -4 -3 -2 -1 0 1 2 3 4 5 6 7 8 910 12
Pretrain Year - Evaluation Year0123456Relative Improvement (%)
Figure 4: The mean relative performance over 5 datasets (y-axis) increases as temporal misalignment (x-axis)
approaches zero. The boxplot indicates the median (solid line), mean (triangles), quartile range (boxes), and rest of the
distribution (whiskers). Note that each dataset has different evaluation year ranges.
Performancedegradationstronglycorrelateswithpretrainingmisalignmentanditseffectsarenon-trivial.
Luuetal.(2021)formalizeadefinitionforTemporalDegradation(TD),whichmeasurestheperformance
change observed from one year difference between the finetuning and evaluation years. We generalize TD to
alsomeasuretheeffectofoneyeardifferencebetweenpretrainingtimeandevaluationtime,asdescribed
inAppendixC.4. Furthermore,wemeasurethePearsoncorrelation rbetweentheperformancedifference
and the temporal difference to understand the strength of the correlation. In Table 2 we find temporal
degradationishighestforfinetuning(2.8onaverage),asexpected,butalsosurprisinglyhighforoneyear
of pretraining (0.4)—particularly for the Newsdomain. The average Pearson correlation of 0.61indicates a
strong correlation between pretraining temporal misalignment and performance degradation. All five tasks
pass a one-sided Wald test with p < 0.05, validating the slope is greater than zero.
Finetuning Pretraining
LM-Small LM-XL LM-Small LM-XL
Domain Task TD rTDr TD rTDr
NewsPubCLS 5.82 0.84 5.63 0.80 0.02 0.01†0.59 0.67
NewSum 0.80 0.82 2.91 0.92 -0.31 -0.29 0.73 0.45
TwitterPoliAff 3.74 0.84 4.93 0.89 0.50 0.21 0.28 0.56
TwiERC 0.49 0.73 0.53 0.82 0.05 0.27 0.23 0.72
Science AIC 0.94 0.83 0.24 0.36 0.11 0.18†0.23 0.66
Mean 2.36 0.81 2.84 0.76 0.08 0.07 0.41 0.61
Table 2: Temporal Degradation (TD) measures the expected performance degradation from one year of tem-
poral misalignment. We report TD first between finetuning and evaluation, then pretraining and evaluation,
forLM-XLandLM-Small , across five tasks. Pearson correlation rindicates the correlation strength between
performance and temporal change. Temporal Degradation due to pretraining is significant and persistent
across domains. All correlations are significant at p < 0.05unless marked with†.
Pretraining misalignment is not overcome by significant finetuning. The temporal degradation due
to pretraining suggests models pretrained on data from the same time frame as target evaluations will
10
have advantages over models trained on much older or newer data. Notably, this effect is observed for
models which are finetuned on the full temporally-relevant training sets. This suggests that even substantial
finetuning cannot overcome pretraining data that is temporally misaligned.
PretrainingmisalignmenteffectsareasymmetricandhaveimplicationsforNLPevaluations. Weobserve
performance degradation regardless of whether the pretraining data was collected before or after the evalua-
tion data. While we would not expect a 2019 checkpoint to perform well on questions about COVID, we also
findthat2022checkpointsperformlesswellonObama-eraevaluationsthanearliermodels. Inparticular,
Figure4showsperformancedegradationisasymmetric: itissteeperwhentheevaluationyearisafterthe
pretrainingyear(bluebars)asopposedtothereverse(redbars). Thisfindingsuggeststhatbothmodels
andevaluationsbecomestale: oldermodelsperformlesswellthannewermodelsonnewevaluationsand
newer modelswill performless well onolder evaluations. Thisphenomenon mayhave subtle implications
forNLPexperimentscomparingmodelspretrainedatdifferenttimes. Forinstance,newerevaluationsets
may appear much more difficult than old evaluation sets when applied to established, but less fresh, models.
Similarly, older evaluations may underestimate the capabilities of newer models.
Temporal Degradation is greater for larger models We find more temporal degradation for LM-XL(1.5B
parameters) than for LM-Small (20M parameters). As shown in Table 2, we do not find the same temporal
degradationeffectsofpretrainingweresignificantfor LM-Small models. Thissuggeststhatlargermodels
mayhaveagreatersensitivitytotemporalinformationthansmallermodels,whichmaynothavethecapacity
to take advantage of subtle temporal features at all. Full results for LM-Small experiments are provided in
Appendix E.1.
5 Impact of Quality & Toxicity Filters on Pretrained Models
Section Findings
•Quality and toxicity filters have very different effects.
•Quality filters improve performance significantly, despite removing training data.
•Quality filtering effects are not easily predicted by dataset characteristics. Future filters should
weigh more than one dimension of quality.
•Toxicityfilteringtradesoffgeneralizationandtoxicityidentificationabilityforreducedriskoftoxic
generation.
•When optimizing for toxicity identification tasks, practitioners should use an inverse toxicity filter.
Most modern large language models use some form of quality and/or toxicity filtering for their pretraining
datasets (Table 1). To curb toxicity, T5 uses n-gram filters, Gopher and Chinchilla use SafeSearch filters,
andLaMDAuses“safetydiscriminators”. Qualityheuristicsareuniversallyappliedforweb-scrapeddata,
with newer models like LLaMA, the GPT-series and the PaLM-series all relying on quality classifiers. To
compareandquantifytheeffectsofthesetwofiltertypes,weimplementqualityandtoxicityfiltersatvarious
thresholds, as described in Section 2.2, to vary the quantity of toxic and low-quality text present when
pretraining models on the Pile and C4.
Qualityfilterssignificantlyimproveperformanceacrossnearlyalltasks,despitereducingtrainingdata
quantity and variety. We see the quality filters improve nearly all downstream tasks: toxicity identification
by2%(Figure5,right)andmostQAtaskcategoriesby1-6%(Figure6). Ofmostinterest,theseimprovements
arerealizeddespiteremoving10%+ofthetrainingdata,eventhoughwefindthatremovingdatausually
leads to a decrease in performance (Section 6). While the average performance peaks at T= 0.975for
theQAtasks,greaterqualityfilteringstilloutperformstheunfilteredbaselineonaverage. Forthetoxicity
identificationexperiments,theperformanceisstillimprovingafter T= 0.7,where55%ofthedatasethas
been filtered out.
11
60%
 50%
 40%
 30%
 20%
 10%
 0% 10% 20%
T oxic Generation Score6%
4%
2%
0%2%T oxicity Identification ScoreFull DatasetToxicity Filtering, LM-XL C4
Inverse Filter
T=0.7T=0.5
T=0.3
Most FilteringT=0.95T=0.9
more toxic less toxic
2%
 0% 2% 4% 6% 8% 10% 12% 14%
T oxic Generation Score2%
1%
0%1%2%3%T oxicity Identification ScoreFull DatasetQuality Filtering, LM-XL C4
Inverse FilterT=0.9T=0.7Most Filtering
T=0.975T=0.95
more toxic less toxicFigure5: Toxicityfiltering thepretrainingdataset decreasestheabilityof LM-XLtoidentify toxicityand
to generate toxic text. Quality filtering surprisingly increases both abilities. Documents with scores below
a given threshold were filtered out.
Wiki Web Books Biomed AcademicCommon
SenseContrast
Sets Average
Inverse T=0.5 (73%)
Full Dataset (100%)
T=0.975 (91%)
T=0.95 (84%)
T=0.9 (73%)
T=0.7 (46%)-5.0 -4.5 2.1 -2.2 -2.7 1.2 -6.4 -3.1
0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
1.2 0.7 -2.2 6.1 6.4 4.7 6.1 2.5
-1.2 1.0 -4.0 3.7 -0.3 3.2 4.9 1.0
-0.3 0.8 -3.5 1.8 1.0 1.9 6.8 1.2
-1.2 0.8 -6.7 1.7 0.8 2.0 4.2 0.76
4
2
0246
Figure 6: Quality filtering C4 increases LM-XL’s downstream performance on all QA task domains,
except for Books. The quality filter threshold is on the x-axis, with percentage of training data remaining in
parenthesis. Each column represents a set of QA evaluations from a domain. The ‘Full Dataset’ is unfiltered,
and the ‘Inverse’ filter removes the highest quality data instead.
Datasetqualitycharacteristicsarenotstronglyindicativeoffilteringeffects. InSection3,Books,Wikipedia,
and Web data are classified as highest quality. Figure 6 shows that despite this, quality filtering provides
the least benefit to QA tasks in these categories, even hurting the performance for Books. On the other end,
academicandbiomedicaldataarerankedamongthelowestquality,buttheirQAtasksbenefitthemostfrom
quality filtering.
Optimizing on one measure of quality is not sufficient to predict or improve performance across domains.
Most interestingly, Wikipedia and Web QA tasks are among the most hurt by the inverse filter—suggesting
these domains are not affected as much by the absence of the lowest quality data as the presence of the
highest quality data. Also unexpectedly, both the quality and inverse quality filters led to models with
highertoxicgenerationtendencies(Figure5,right)—theonedimensionalmeasureofqualitycapturedbythe
quality scores is not sufficient to explain this behaviour. In other words, different segments of data along this
classifier’s quality spectrum can have strong but varied effects on different domains. It suggests practitioners
should move beyond one measurement of quality and consider multiple.
One size does not fit all. Toxicity Filtering leads to a trade-off between toxic identification and toxic
generation goals. Filtering using a toxicity classifier, we find a trade-off: models trained from heavily
filtered pretraining datasets have the least toxic generation but also the worst toxicity identification (Figure 5,
left). Similarly,Figure7showstheperformanceofQAtasksunrelatedtotoxicityarehurtbytoxicityfiltering,
12
Wiki Web Books Biomed AcademicCommon
SenseContrast
Sets Average
Inverse T=0.06 (92%)
Full Dataset (100%)
T=0.95 (98%)
T=0.9 (95%)
T=0.7 (86%)
T=0.5 (76%)
T=0.3 (61%)0.4 -1.4 3.8 0.7 4.9 4.1 2.7 1.7
0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
-1.0 -0.4 0.2 -0.5 0.6 1.7 1.3 0.2
-2.2 -1.1 -0.6 -3.0 0.2 2.9 0.2 -0.7
-2.1 -1.4 0.1 -2.9 0.1 -0.9 -0.2 -1.2
-4.2 -2.4 -0.9 -3.3 -1.1 -0.3 -0.1 -2.0
-3.8 -4.4 -1.4 -2.5 -0.3 -1.3 -3.5 -2.74
3
2
1
01234
Figure 7: Toxicity filtering C4 reduces LM-XL’s downstream performance on most QA task domains. The
toxicity filter threshold is on the x-axis, with percentage of training data remaining in parentheses. Each
column represents a set of QA evaluations from a domain. The ‘Full Dataset’is unfiltered, and the ‘Inverse’
filter removes the lowest toxicity data instead.
thoughthismaybeduetotheoveralldecreaseintrainingdata. Ultimately,theintendedbehaviourofthe
model should inform the filtering strategy, rather than one size fits all. Most interesting of all, the strongest
performance on toxicity identification for every dataset comes from the inverse toxicity filter. Practitioners
optimizing for performance on toxic domains should intentionally apply inverse filters .
6 Impact of Domain Composition on Pretrained Models
Section Findings
•Inclusion of Common Crawl, OpenWeb and Books have the strongest positive effects on down-
stream performance. Data source heterogeneity is more important than data quality or size.
•Targeted data helps targeted evaluations, but not always as much as including heterogeneous web
domains.
•It is beneficial to include as many pretraining data sources as possible.
AsshowninTable1,pretrainingdatasetsseektogeneralizetoawidearrayofdownstreamtasksbycombining
data from a diverse set of domains. How does the choice of pretraining source domains impact downstream
performance? WeempiricallyanswerthisquestionbyablatingpretrainingsourcesfromthePileone-at-a-time
and measuring the downstream performance change in 27 QA tasks from diverse domains.
WefirstgroupthePiledatasourcesintoninedomainsrepresentingconceptualsourcesthatpractitionerscould
choosetolicenseorscrapemoreof: CommonCrawl(CC),OpenWeb,Wikipedia,Books,PubMed,Academic,
Code & Math, Legal, and Social (see Table 8). These are sorted in ascending order by size. We choose to
maintainthesizedisparitiesinthesesources,simplybecausetheyreflectreality: curatedWikipediacontentis
innatelyfinite,whilewebandbooksaremuchmoreabundant. Wethenpretrain LM-XLwiththefulldataset
minus each category, yielding nine models, then finetune each for QA using Natural Questions. Finally, we
evaluatethemodelon27uniquedatasetsfromMRQA(Fischetal.,2019)andUnifiedQA(Khashabietal.,
2020) that have also been partitioned into domains. Full details are documented in Appendix C.5.
Common Crawl, OpenWeb, and Bookshavethe strongestpositiveeffects ondownstream performance.
Figure 8 shows that average downstream performance degrades the most when we remove web-based
domains like CC, Books, and OpenWeb, corroborating recent findings by Xie et al. (2023a). In particular,
these sources improve performance on challenging Common Sense and Contrast Sets tasks. While CC is the
13
Wiki Web Books Biomed AcademicCommon
SenseContrast
Sets Average
Full Dataset (100%)
No Social (99%)
No Wiki (98%)
No Books (93%)
No OpenWeb (93%)
No Legal (91%)
No Academic (87%)
No Pubmed (85%)
No Code (81%)
No CC (73%)0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
-0.8 -3.7 2.6 0.1 3.5 -3.5 3.5 0.4
-1.3 -5.3 3.0 0.2 0.9 -4.4 7.2 -0.3
-3.5 -6.3 1.0 0.0 -1.6 -6.5 -4.4 -2.7
-2.0 -4.1 0.1 -1.0 0.6 -5.8 -2.9 -1.4
-2.7 -2.9 3.8 0.4 0.8 -2.6 -0.4 -0.6
-0.3 -2.5 0.3 -0.9 2.2 -1.1 4.3 0.2
-0.3 -3.0 3.9 -5.8 -1.5 -5.9 3.9 -1.2
-0.5 -3.1 2.9 -1.2 1.2 -5.8 4.4 -0.1
-3.2 -6.2 -2.9 -4.6 -5.9 -8.0 -5.2 -4.8
8
6
4
2
02468
Figure8: QAtasksareaffectedbyremovingdomainswhenpretraining LM-XL.Eachrowrepresentsamodelwith
onedomainremoved,thesizeoftheremainingdatasetisshownattheleftinparentheses. Eachcolumnrepresentsa
setofQAevaluationsfromadomain. The Full Dataset modelrepresentstheunfilteredPile LM-XL,andallscoresare
relative to this Base model.
largest chunk of textin the Pile, Booksand OpenWebare smaller but provide the most heterogeneous and
predicted-qualitycontent(seeSection3). Theseresultssuggestthatmoredataisnotnecessarilyasimportant
a factor as a combination of heterogeneity and quality.
Domainheterogeneityisoftenmorebeneficialthantargeteddata,evenfortargetedevaluations. Ablating
apretrainingdomainhasvaryingeffectsondownstreamQAperformance. Predictably,performancedegrades
when we remove domains with close alignment between the pretraining and downstream data sources:
removing PubMed hurts the BioMed QA evaluations, dropping Wikipedia hurts the Wikipedia benchmarks,
andremovingwebcontenthurtswebevaluations. However,removingtargeteddomainsdoesnotnecessarily
have as significant an effect on related downstream domains as removing the large heterogeneous domains.
For instance, removing CC from the pretraining dataset reduces performance on downstream Academic QA
tasks to a much greater extent than removing the Academic domain. Our hypothesis is that CC, OpenWeb
andBookscontainextensivecoverageofmanytopics,soremovingtheAcademic-specificcategoryofsources
does not remove all relevant academic information.
The best performing models use allthe pretraining data sources. Despite the importance of data het-
erogeneity, the best mean performance still comesfrom models that train on all, or nearly all, the data. The
exceptionsare theremoval oftargetedsource domainslike thePile’sCode orAcademic(advancedscience
andmathjournals)domains. ThesearebothlargebutperhapsnotwellmatchedwiththeQAevaluationsets,
which do not require coding skills or scientific rigour beyond that found on Wikipedia and from web-based
sources. This finding suggests that both the quantity and diversity of open source data remain a bottleneck
for current pretraining methods.
WebandBooksdomainscausethebiggesttrade-offbetweentoxicidentificationandgeneration. We
next consider whether reducing a model’s pretraining exposure to toxic content affects either its propensity
togeneratetoxiclanguageoritsabilitytoidentifytoxiclanguage. Table3showsthatthelargestdecreases
inbothtoxicitygenerationandidentificationwerecausedbyremovingCC(26.9%ofthedata),OpenWeb
(6.9%), and Books (6.9%). This is consistent with the observation that Web and Books data had the highest
concentration of text predicted to be toxic Section 3. These results suggest a trade-off: better performance on
QA (Section 6) and toxicity identification comes at the cost of more toxic generation.
14
Table 3: Effect of the Pile’s domain composition on toxicity identification and generation. Removing Books,
CommonCrawl and OpenWeb lead to the greatest decrease in toxicity metrics. Removing Wikipedia had
a strong increase in toxicity generation.
Filter % Data Toxicity Identification ( ↑) Toxic Generation ( ↓)
SBF Toxigen DH R3 DH R4 ScoreRTP-T RTP-NT RepBias Score
Full Dataset 100.090.7 90.8 88.7 84.1 0.088.9 45.4 4.6 ±0.70.0
No Social 98.890.9 91.0 87.8 84.9 +0.185.4 47.2 4.7 ±0.8+0.4
No Wiki 97.990.6 90.8 88.1 83.6 -0.489.0 49.4 4.8 ±0.6+4.2
No Books 93.189.9 90.3 87.1 82.6 -1.387.4 43.5 4.0 ±0.8-6.2
No OpenWeb 93.189.9 90.3 86.4 82.5 -1.588.0 42.1 4.3 ±0.6-5.2
No Legal 91.090.9 90.8 88.1 83.0 -0.488.2 46.1 4.7 ±0.8+0.8
No Academic 87.190.7 91.0 88.2 84.5 +0.086.5 46.4 4.5 ±0.7-1.2
No Pubmed 85.190.6 90.8 88.0 84.3 -0.287.6 46.3 4.6 ±0.7-0.2
No Code 80.991.0 91.2 88.5 84.5 +0.287.6 46.5 4.7 ±0.7+0.6
No CC 73.189.9 90.0 85.3 82.4 -1.987.8 46.2 4.3 ±0.6-2.1
7 Discussion
Guided by intuition: undocumented & unknown Pretraining dataset curation has been guided by in-
tuitions: collectionsshouldbelarge,diverse,andhighquality. Decisionsareoftendrivenbytheneedfor
something “good enough” or by precedents that may themselves not have been thoroughly evaluated (Sam-
basivanetal.,2021). Similarly,modeldevelopersoccasionallyneglecttoshareempiricalinsights,maintaining
a knowledge gap, often referred to as “documentation debt” (Bandy and Vincent, 2021).
Our results show that choices made in pretraining curation affect models in significant ways that cannot be
easily erased by subsequent finetuning. We urge both model producers and model users to think of dataset
curation policiesas a formof hyperparameter, muchlike learning ratesor networkdimensions. Exhaustive
search methods that work for single scalar values will not, however, scale to curation policies that affect
terabytes of data. While our results are necessary to establish that pretraining curation matters, they are
not sufficient to answer all questions. In this section we therefore make specific recommendations, but our
primaryresultisthatweneedbettertoolsformodelingtherelationshipbetweendataandmodelcapabilities.
Ageofthepretrainingcorpus. Inanidealworld,modelswouldbecontinuouslyre-trainedonthemost
up-to-datedataavailable. However,giventheexpenseofdatacollectionandre-training,modelcreatorsmust
make a choice between efficiency and model staleness. More subtly, we also find that using newer data can
adda “presentist”biaswhenevaluating retrospective tasks. Theeffectof stalenessisnotovercome even by
plentiful finetuning data for the given task, and this effect is worse for the larger, more capable models. This
result complements findings by Schulman (2023) that finetuning on newer data can aggravate hallucination
fornewdatathatisnotwell-groundedatpretrainingtime. Thesetentativefindingssuggestthetemporal
propertiesofpretrainingcorporaareincreasinglyessentialtoconsiderforlargermodels,formorenoveltasks
(lessfinetuningdata),andforinstructiontuningmodels. Currentpracticeincludesaugmentingprompts
with retrieved, recent data to help overcome stale pretraining data. While this can conceivably help mitigate
staleness, retrieving relevant text is a challenge in its own right.
We recommend model creatorsreport the temporal distribution ofpretraining data, which is notcurrently
standard practice (Hoffmann et al., 2022; Thoppilan et al., 2022; Anthropic AI, 2023; Cohere AI, 2023). Users
shouldbeabletopredictotherwiseunforeseenperformancedegradationsonmuchnewerdatasets,orbe
aware of the potential side effects of finetuning models on information not covered in pretraining.
Data source composition. Decisions on the composition of a corpus intended for pretraining can have
substantial impacts on downstream performance. Of the two corpora we consider in this paper, C4 contains
only one data source, a single scrape of the Common Crawl, while the Pile is a collection of 22 data sources.
It is more complex and costly to assemble a corpus which contains diverse sources, writing styles, and
15
thematic areas. Achieving this diversity might also leave models vulnerable to less careful curation or gaps
in practitioner knowledge.
In our experiments, we ablate the Pile by systematically omitting each of its constituent datasets before
pretraining, and then measuring the impact on standard benchmarks. Our results suggest that practitioners
shouldnotomitanydatasourcesifgeneralizationtoasmanytext-to-texttasksisthegoal,andthatfuture
work should focus on collecting more diverse web and books content, which yield the largest benefits. These
findings are somewhat consistent with hypotheses that the volume of training data remains a limiting factor,
especially given licensing constraints (Nostalgebraist, 2022).
Filtering for toxicity and quality. The Common Crawl contains an enormous amount of low quality
(advertisements,repetitive,non-human-readable,etc.) andtoxictext. Manystate-of-the-artlanguagemodels
filteroutthistextbeforetraining,eitherusingbadwordslists(Raffeletal.,2020),heuristics,orclassifiers
(Du et al., 2022; Brown et al., 2020; Chowdhery et al., 2022). Deciding on how much and what kind of text to
filteroutrequiresnon-trivialnormativedecisions,andallofthesefilteringapproachesinvolvethemodel
creator intentionally modifying the bias of their datasets and thus their models.
Inourexperiments,weexposeanimplicittrade-offbetweenamodel’sgeneralizationabilitiesanditstendency
to generate toxic content. This behavior is modulated by quality and toxicity filters. In fact, over-sampling on
moretoxicdocumentsleadstothebestperformanceontoxicidentification. Thisobservation,coupledwithev-
idencethatrecentworkisusingpost-hocmethodstocurbunwantedtoxicgeneration(e.g. instructiontuning
(Chung et al., 2022) or steerable decoders (Dathathri et al., 2020; Welbl et al., 2021)), suggests practitioners
should prioritize toxic identification rather than curbing toxic generation abilities during pretraining.
Wefindthatourqualityfilter (the sameusedbyPaLM,trainedtokeepcontentresembling Wikipediaand
Books) significantly improves performance across domains, despite removing largeportionsof the training
data. Perplexingly, the Books domain is the one exception to the above observation, as its content ranks
amongthehighestquality. Ingeneral,observationalqualitycharacteristicsofthedataarenotsufficientto
predict which domains will benefit most from quality filtering. Our analysis suggests that performance on a
task/domainisnotinfluenced onlybyhowmuchpoorqualitydata(i.e. thatwhichisunlikeWikipedia/Books)
is removed, but also by other aspects of quality, such as how much of the highest or mid-quality data is
represented along this specific measurement dimension.
8 Limitations
Compute Expense & Single Shot Experiments To our knowledge, this is the largest publicly documented
LMpretrainingdataablationstudy,spanning281.5Bparametermodels—trainingmoremodelswithdifferent
datavariantsfromscratchthanGLaM(Duetal.,2022),miniBertas(Warstadtetal.,2020),MultiBerts(Sellam
et al., 2022), and even Pythia (Biderman et al., 2023), which focuses on preserving data composition and
order. Itisimportanttoacknowledgeeachofthesepretrainings,withtheircorrespondingfinetuningand
evaluations is computationally and environmentally costly. With this in mind, we made the careful decision
on what experiments to pursue—narrowing our list to: age of the corpora, quality filters, toxicity filters, and
the choice of source domains. We carefully curated the choice of experiments in advance, without the luxury
of multiple rounds of reflection and repetition, common in many NLP experimental settings. As a result, we
struckabalanceasbestwecouldbetweenthecomputationalcosts,andreproduciblevalidity. Wehopeto
justify the merits of our selection and also point out the surprises that motivate future work or a deeper look
into the results.
Blackbox APIs An additional limitation is our use of Perspective’s API for evaluating the toxicity of
generations. While most of our toxicity filters and evaluations were in a compressed time period, Pozzobon
et al. (2023) have since demonstrated the irreproducibility of black-box APIs, which may have shifting
implementations over time. We also believe that while this is the standard procedure for popular toxic
generationbenchmarkslikeRealToxicityPrompts,therelianceonAPIsandnarrowevaluationsettingcan
have limited implications for toxic generation in real applications. For the time being, these are the best
proxies we have.
16
English vs Multilingual Data Our analysis was limited to two English datasets. It’s important to note
that training composition is an even more crucial question for multilingual and non-English models, where
optimally balancing corpora from different languages and finding large-enough high-quality corpora can be
very challenging (Chung et al., 2023).
RelevancetoZero-&Few-ShotPromptedSettings Ourexperimentsfocusonfinetunedsettingsrather
than zero- or few-shot prompting. This choice is motivated by finetuning being more applicable for 1.5B
parameter models and also in many applied settings. We cannot establish how well these findings translate
to prompted settings (without finetuning), but suspect they are strong correlated.
9 Related Work
Pretraining Dataset Curation There have been dozens of general-purpose models trained for natural
languageunderstandingandgenerationtasks. Earlymodelsinthisspace,suchasELMO(Petersetal.,2018),
BERT(Devlinetal.,2019),andBERT’svariousdescendants(Liuetal.,2019;Lanetal.,2020),focusedon
strong finetuning performance for a variety of natural language inference tasks, as well as semantically
meaningful language embeddings. These systems were trained on semi-curated datasets such as Wikipedia,
BookCorpus(Zhuetal.,2015),andnewsarticlesfromtheOneBillionWordBenchmark(Chelbaetal.,2013).
XLNet (Yang et al., 2019) broke away from this use of curated datasets to include documents from Common
Crawl into their pretraining dataset. T5 (Raffel et al., 2020), which introduced the C4 dataset, was one of the
firstpretrainedlanguagemodelstotrainexclusivelyonCommonCrawldata. MultilingualversionsofT5
(Xue et al., 2021) and BERT were trained on Common Crawl and Wikipedia, respectively.
GPT-2wasoneofthefirstmodelsintendedprimarilyforgeneration(Radfordetal.,2019). DeemingCommon
Crawl too noisy to be practical for training generative models, they developed WebText, a dataset containing
websites linked to from highly-ranked posts on Reddit. Subsequent generative models proposed mixing
large amounts of noisy Common Crawl data with smaller corpora perceived as high-quality. The GPT-Neo
model family (Black et al., 2022) trained on the Pile, which augments the Common Crawl with ArXiV, Stack
Exchange,legaldocuments,books,Github,andothermorecuratedsourced(Gaoetal.,2020). Morerecently,
OPT(Zhang etal., 2022)trained onthe Pileaugmented with socialmedia data(Baumgartner etal., 2020),
and LLaMA (Touvron et al., 2023) trained on C4 augmented with Github, Stack Exchange, books, and
other sources. Pythia trained on the Pile, with and without duplication (Biderman et al., 2023). Finally, the
BLOOM model family (Scao et al., 2022) trained on the ROOTS Corpus, which crowd-sourced a collection of
“identified” datasets, coming from known, high-quality sources in a variety of languages.
Allofthemodelsmentionedsofararepubliclyavailable. However,companiesareincreasinglytrainingtheir
bestmodelsonproprietarydatasets,withonlylimitedhintsastothedatacomposition. AtAlphabet,models
such as Gopher (Rae et al., 2021), GLaM (Du et al., 2022) , LaMDA (Thoppilan et al., 2022), and PaLM
(Chowdheryetal.,2022)havebeentrainedonmixturesofwebtext,books,news,code,Wikipedia,anddialog
data. At OpenAI, GPT-3 (Brown et al., 2020) was trained on Common Crawl, WebText (GPT-2’s training
set), books, and Wikipedia. Subsequent versions of their model have also included code. Most of these
modelshaveacknowledgedusingvariousformsoffilteringtechniquestoimprovethequalityofweb-derived
training data. These include classifiers designed to exclude content which looks least like “high-quality”
sourcessuchasbooksorWikipedia(Chowdheryetal.,2022;Ouyangetal.,2022), usingGoogle’sSafeSearch
for identifying toxic content (Rae et al., 2021), and various heuristics based on document length and the
presence or absence of certain words or characters.
Pretraining Dataset Analysis Dodge et al. (2021) find significant amounts of low-quality patent, military,
and machine-generated text in C4, and a dearth of English text from American minority communities
as well as from non-Western communities like India or Nigeria post-filtering, and so recommend against
filtering. In contrast, Luccioni and Viviano (2021) recommend more robust filtering practices to curb the
significant presence of hate speech and sexually explicit content they find in C4 even after filtering. Similarly,
Kreutzer et al. (2022) find that multilingual pretraining corpora are also dominated by low-quality text,
particularlyforlowerresourcelanguages. Leeetal.(2022);Kaddour(2023)showthebenefitsofdeduplicating
17
pretraining datasets, which often contain a great deal of repeated content. Lastly, Zhao et al. (2023) reviews
pretraining data sources, strategies for quality filtering, and the importance of data distribution. Their
summary corroborates our findings regarding domain composition and quality filtering, in particular.
Data, Toxicity, & Quality Research into the quality and toxicity of datasets and their resulting models has
seenmixedfindings. Allofthemajormodelsreportusingsignificantdatapre-processingandtoxicity/quality
filters, including BERT, T5, BLOOM, OPT, ChinChilla, PaLM, LaMDA, and the GPT-3 series, with the largest
of these now using classifiers. This widespread adoption suggests there are significant implicit benefits, even
though they not often externally reported. GLaM does empirically report performance improvements from
filtering, particularly on Natural Language Generation (NLG) tasks (Du et al., 2022).
However, in academia, a few works caution against the use of detoxification techniques, including data
filters,whichcanreducemodelperplexityonunderrepresentedcommunities(Xuetal.,2021;Welbletal.,
2021). Welbl et al. (2021) also reports that a toxicity classifier reduces toxicity more than than applying
data toxicity data filters, but Xu et al. (2021) show this yields the worst perplexity on underrepresented
communities. Meadeetal.(2022)furthercorroboratesthatimprovementsonbiasbenchmarkscorrelates
withdeteriorationsingenerallanguagemodelingabilities. Furthermore,investigatingGPT-3’sdescribed
qualityfilter,Gururanganetal.(2022)finditsqualityjudgmentsareunalignedwithfactualityorliterary
acclaim but are instead aligned with some notion of langauge ideology more correlated with wealthier zip
codes. Works in the vision domain show data filtering has important detoxification benefits but can reduce
performance (Nichol et al., 2022) or introduce other biases (Nichol, 2022). In summary, pretraining data
filtersareubiquitousinthedevelopmentofnon-toxicandhigh-qualitymodels,buttheyarepronetoreducing
their abilities to serve underrepresented communities and may introduce new biases.
Additionalworkhasshownthatinstructiontuning(Chungetal.,2022;Longpreetal.,2023)andformsof
alignment tuning (Ouyang et al., 2022; Bai et al., 2022) have both reduced unwanted toxic generation.
Data&Time Naturallanguageisknowntoevolveandchangeovertime(Altmannetal.,2009;Labov,2011;
Eisenstein et al., 2014; Jaidka et al., 2018). As language’s distribution shifts, the ability of models to perform
wellonnewtestsetshasalsobeenshowntodegrade,duetotheirstaticknowledgeofrecentevents,syntactic
and semantic practices (Lazaridou et al., 2021; Agarwal and Nenkova, 2022; Longpre et al., 2021). Luu et al.
(2021);Lazaridouetal.(2021);Liskaetal.(2022);Yaoetal.(2022);ZhangandChoi(2021);Jangetal.(2022)
offer evaluation sets to measure this phenomena. Proposed remedies include finetuning on more recent data
(Luuetal.,2021),adaptive/continuouspretraining(Lazaridouetal.,2021;RöttgerandPierrehumbert,2021),
data augmentation (Singh and Ortega, 2022), modeling text with its timpestamps (Dhingra et al., 2022). To
our knowledge, no work has thoroughly investigated the effects of temporal degradation when pretraining
from scratch.
Data&Domains Thecompositionofpublicdatasets,likeC4andthePile,isguidedmostlybylicensing,
which severely restricts availability. Even so, Villalobos et al. (2022); Nostalgebraist (2022); Hoffmann
et al. (2022) suggest we are imminently exhausting high-quality text data on the web to train compute-
optimal larger LMs, at least with existing training efficiency. This poses a challenge, given the demonstrated
importanceofhighqualityanddiversetrainingdatatostronggeneralization(Gaoetal.,2020;Papadimitriou
andJurafsky,2020). Agreatdealofliteraturehasdedicateditselftoadaptingstaticpretrainedmodelstonew
downstreamdomains,usingdomainadaptivepretraining(Gururanganetal.,2020),findingintermediate
finetuning tasks(Pruksachatkun etal., 2020),dynamically balancing datasources (Wang et al.,2020), data
selection (Iter and Grangier, 2021; Albalak et al., 2023), augmentation (Longpre et al., 2019), and active
learning (Longpre et al., 2022). Another line of work demonstrates the potential of pretraining on carefully
crafted synthetic data (Wu et al., 2022).
Mostsimilartothissectionofourwork,Xieetal.(2023a)re-balancemixturesofthePiletoachievemore
performant and efficient convergence. Xie et al. (2023b) use importance sampling to select subsets of the Pile
mostusefulfortargetdownstreamtasks,inlieuofqualityfilters,toachieve2%improvementondownstream
tasks. Pruksachatkun et al. (2020) systematically benchmark the effects of intermediate finetuning tasks,
similar to how we benchmark different compositions of pretraining tasks.
18
Model & Data Scaling Prior work has explored scaling model size (Kaplan et al., 2020; Tay et al., 2022; Du
et al., 2022), the amount of pretraining data or the number of pretraining steps (Liu et al., 2019; Chowdhery
etal.,2022;Brownetal.,2020). Chinchillainvestigatedandreportedoptimalcomputescalinglaws,expressing
a relationship between model and data size (Nostalgebraist, 2022). Recent work has demonstrated that new
abilities emerge at greater scale (Wei et al., 2022), but also that many of these benefits can be distilled or
compressed into smaller models (Taori et al., 2023; Movva et al., 2022). In this work, we investigate how
temporal pretraining misalignment varies on different model sizes, which to our knowledge was previously
unanswered.
10 Conclusion
The relative age of documents, content filters, and data sources each have significant effects on downstream
modelbehaviour. Theseeffectscanbereduced,butnoteliminated,byfinetuning. Werecommendthatmodel
developers and users pay close attention to these details in designing/selecting the model most relevant to
their needs, as each decision has a specific, quantifiable trade-off profile. For instance, it may be important to
decide between improving toxicity identification or reducing toxic generation, performance on brand new or
older data sources, andbiomedical or books text domains. These countless choicesare inherent in curating
any pretraining dataset. While we are only able to evaluate a small fraction of these, we are able to show
whichchoicesmatterandbyhowmuch,andwehopetoinspirefurtherworkevaluatingdatasetcomposition
and predicting behaviors of models given pretraining datasets.
Acknowledgements
We would like to thank Daniel Smilkov for his technical assistance in characterizing large corpora, Maarten
Bosma and Jacob Andreas for their early guidance on this project, Tom Small for his visual design support,
and Noah Constant for feedback on the paper. This work is supported by NSF #1652536.
References
Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S Cor-
rado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al. TensorFlow: Large-scale machine learning on
heterogeneous distributed systems. arXiv preprint arXiv:1603.04467 , 2016.
Oshin Agarwal and Ani Nenkova. Temporal effects on pre-trained models for language processing tasks.
Transactions of the Association for Computational Linguistics , 10:904–921, 2022.
AlonAlbalak,ColinRaffel,andWilliamYangWang. Improvingfew-shotgeneralizationbyexploringand
exploiting auxiliary data. arXiv preprint arXiv:2302.00674 , 2023.
EduardoGAltmann,JanetBPierrehumbert,andAdilsonEMotter. Beyondwordfrequency: Bursts,lulls,
and scaling in the temporal distributions of words. PLOS one , 4(11):e7678, 2009.
Anthropic AI. Introducing Claude, 2023. URL https://www.anthropic.com/index/introducing-claude .
Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen,
Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional AI: Harmlessness from ai
feedback. arXiv preprint arXiv:2212.08073 , 2022.
Jack Bandy and Nicholas Vincent. Addressing “documentation debt” in machine learning research: A
retrospective datasheet for bookcorpus. arXiv preprint arXiv:2105.05241 , 2021.
19
JasonBaumgartner,Savvas Zannettou,Brian Keegan,Megan Squire,and JeremyBlackburn. ThePushshift
Reddit dataset. In Proceedings of the international AAAI conference on web and social media , volume 14, pages
830–839, 2020.
Emily M Bender. Linguistic fundamentals for natural language processing: 100 essentials from morphology
and syntax. Synthesis lectures on human language technologies , 6(3):1–184, 2013.
Emily M. Bender and Batya Friedman. Data statements for natural language processing: Toward mitigating
system bias and enabling better science. Transactions of the Association for Computational Linguistics , 6:
587–604, 2018. doi: 10.1162/tacl_a_00041. URL https://aclanthology.org/Q18-1041 .
Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O’Brien, Eric Hallahan, Mo-
hammadAflahKhan,ShivanshuPurohit,USVSNSaiPrashanth,EdwardRaff,etal. Pythia: Asuitefor
analyzing large language models across training and scaling. arXiv preprint arXiv:2304.01373 , 2023.
SidBlack,StellaBiderman,EricHallahan,QuentinAnthony,LeoGao,LaurenceGolding,HoraceHe,Connor
Leahy,KyleMcDonell,JasonPhang,etal. GPT-NeoX-20B:Anopen-sourceautoregressivelanguagemodel.
Challenges & Perspectives in Creating Large Language Models , page 95, 2022.
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Nee-
lakantan,PranavShyam, GirishSastry,AmandaAskell, SandhiniAgarwal,ArielHerbert-Voss, Gretchen
Krueger,TomHenighan,RewonChild,AdityaRamesh,DanielM.Ziegler,JeffreyWu,ClemensWinter,
ChristopherHesse,MarkChen,EricSigler,MateuszLitwin,ScottGray,BenjaminChess,JackClark,Christo-
pher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are
few-shot learners, 2020.
Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony Robinson.
One billion word benchmark for measuring progress in statistical language modeling. arXiv preprint
arXiv:1312.3005 , 2013.
MarkChen,JerryTworek,HeewooJun,QimingYuan,HenriquePondedeOliveiraPinto,JaredKaplan,Harri
Edwards,YuriBurda,NicholasJoseph,GregBrockman,etal. Evaluatinglargelanguagemodelstrained
on code. arXiv preprint arXiv:2107.03374 , 2021. URL https://arxiv.org/abs/2107.03374 .
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Hyung Won Chung,
CharlesSutton,SebastianGehrmann,ParkerSchuh,etal. PaLM:ScalinglanguagemodelingwithPathways.
arXiv preprint arXiv:2204.02311 , 2022. URL https://arxiv.org/abs/2204.02311 .
Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,
MostafaDehghani,SiddharthaBrahma,etal. Scalinginstruction-finetunedlanguagemodels. arXivpreprint
arXiv:2210.11416 , 2022.
Hyung Won Chung, Xavier Garcia, Adam Roberts, Yi Tay, Orhan Firat, Sharan Narang, and Noah Constant.
Unimax: Fairer and more effective language sampling for large-scale multilingual pretraining. In The
Eleventh International Conference on Learning Representations , 2023.
Cohere AI. Cohere command nightly, 2023. URL https://docs.cohere.com/docs/command-beta .
Corinna Cortes and Neil D Lawrence. Inconsistency in conference peer review: Revisiting the 2014 NeurIPS
experiment. arXiv preprint arXiv:2109.09774 , 2021.
Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski,
andRosanneLiu. Plugandplaylanguagemodels: Asimpleapproachtocontrolledtextgeneration. In
International Conference on Learning Representations , 2020.
JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova. BERT:Pre-trainingofdeepbidirectional
transformers for language understanding. NAACL, 2019. URL https://aclanthology.org/N19-1423 .
20
Bhuwan Dhingra, Jeremy R Cole, Julian Martin Eisenschlos, Daniel Gillick, Jacob Eisenstein, and William W
Cohen. Time-aware language models as temporal knowledge bases. Transactions of the Association for
Computational Linguistics , 10:257–273, 2022.
Jesse Dodge, Maarten Sap, Ana Marasović, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret
Mitchell, and Matt Gardner. Documenting large webtext corpora: A case study on the Colossal Clean
Crawled Corpus. In Proceedings of the 2021 Conference on EmpiricalMethodsin Natural Language Processing ,
pages 1286–1305, 2021.
NanDu,YanpingHuang,AndrewM.Dai,SimonTong,DmitryLepikhin,YuanzhongXu,MaximKrikun,
YanqiZhou, AdamsWei Yu,OrhanFirat,BarretZoph, LiamFedus,Maarten Bosma,ZongweiZhou, Tao
Wang,YuEmmaWang,KellieWebster,MariePellat,KevinRobinson,KathleenMeier-Hellstern,TojuDuke,
Lucas Dixon, Kun Zhang, Quoc V Le, Yonghui Wu, Zhifeng Chen, and Claire Cui. GLaM: Efficient Scaling
of Language Models with Mixture-of-Experts. ICML, 2022. URL https://arxiv.org/abs/2112.06905 .
JacobEisenstein,BrendanO’Connor,NoahASmith,andEricPXing. Diffusionoflexicalchangeinsocial
media.PloS one, 9(11):e113114, 2014.
István Endrédy and Attila Novák. More effective boilerplate removal-the goldminer algorithm. Polibits, 48:
79–83, 2013.
AdamFisch,AlonTalmor,RobinJia,MinjoonSeo,EunsolChoi,andDanqiChen. Mrqa2019sharedtask:
Evaluating generalization in reading comprehension. In Proceedings of the 2nd Workshop on Machine Reading
for Question Answering , pages 1–13, 2019.
Paul Friedl. Dis/similarities in the design and development of legal and algorithmic normative systems: the
case of perspective api. Law, Innovation and Technology , pages 1–35, 2023.
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace
He, Anish Thite, Noa Nabeshima, et al. The Pile: An 800GB dataset of diverse text for language modeling.
arXiv preprint arXiv:2101.00027 , 2020.
TianyuGao,AdamFisch,andDanqiChen. Makingpre-trainedlanguagemodelsbetterfew-shotlearners.
ACL, 2021. doi: 10.18653/v1/2021.acl-long.295. URL https://aclanthology.org/2021.acl-long.295 .
MattGardner,YoavArtzi,VictoriaBasmov,JonathanBerant,BenBogin,SihaoChen,PradeepDasigi,Dheeru
Dua, Yanai Elazar, Ananth Gottumukkala, et al. Evaluating models’ local decision boundaries via contrast
sets. InFindings of the Association for Computational Linguistics: EMNLP 2020 , pages 1307–1323, 2020.
SK Gargee, Pranav Bhargav Gopinath, Shridhar Reddy SR Kancharla, CR Anand, and Anoop S Babu.
Analyzing and addressing the difference in toxicity prediction between different comments with same
semanticmeaninginGoogle’sPerspectiveAPI. In ICTSystemsandSustainability: ProceedingsofICT4SD
2022, pages 455–464. Springer, 2022.
Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal
Daumé III au2, and Kate Crawford. Datasheets for datasets, 2021.
SamuelGehman,SuchinGururangan,MaartenSap,YejinChoi,andNoahASmith. RealToxicityPrompts:
Evaluatingneuraltoxicdegenerationinlanguagemodels. In FindingsoftheAssociationforComputational
Linguistics: EMNLP 2020 , pages 3356–3369, 2020.
Aaron Gokaslan*, Vanya Cohen*, Ellie Pavlick, and Stefanie Tellex. OpenWebText corpus, 2019. URL
http://Skylion007.github.io/OpenWebTextCorpus .
Google. PaLM 2 technical report, 2023. URL https://ai.google/static/documents/palm2techreport.
pdf.
GoogleCloudNLP. GoogleCloudinfotypedetector,2023a. URL https://cloud.google.com/dlp/docs/
infotypes-reference .
21
Google Cloud NLP. Google Cloud analyzing sentiment, 2023b. URL https://cloud.google.com/natural-
language/docs/analyzing-sentiment .
Suchin Gururangan, Ana Marasović, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A
Smith. Don’t stop pretraining: Adapt language models to domains and tasks. In Proceedings of the 58th
Annual Meeting of the Association for Computational Linguistics , pages 8342–8360, 2020.
SuchinGururangan,DallasCard,SarahKDrier,EmilyKGade,LeroyZWang,ZeyuWang,LukeZettlemoyer,
andNoahASmith. Whoselanguagecounts ashighquality? Measuringlanguageideologiesin textdata
selection. arXiv preprint arXiv:2201.10474 , 2022.
ThomasHartvigsen,SaadiaGabriel,HamidPalangi,MaartenSap,DipankarRay,andEceKamar. Toxigen: A
large-scalemachine-generateddatasetforadversarialandimplicithatespeechdetection. In Proceedings
of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages
3309–3326, 2022.
JordanHoffmann,SebastianBorgeaud,ArthurMensch,ElenaBuchatskaya, TrevorCai, ElizaRutherford,
Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal
large language models. arXiv preprint arXiv:2203.15556 , 2022.
DanIterandDavidGrangier. Onthecomplementarityofdataselectionandfinetuningfordomainadaptation.
arXiv preprint arXiv:2109.07591 , 2021.
KokilJaidka, NiyatiChhaya, andLyle Ungar. Diachronicdegradationoflanguagemodels: Insights from
socialmedia. In Proceedingsofthe56thAnnualMeetingoftheAssociationforComputationalLinguistics(Volume
2: ShortPapers) ,pages195–200,Melbourne,Australia,July2018.AssociationforComputationalLinguistics.
doi: 10.18653/v1/P18-2032. URL https://aclanthology.org/P18-2032 .
JoelJang,SeonghyeonYe,ChanghoLee,SoheeYang,JoongboShin,JanghoonHan,GyeonghunKim,and
MinjoonSeo. TemporalWiki: Alifelongbenchmarkfortrainingandevaluatingever-evolvinglanguage
models. arXiv preprint arXiv:2204.14211 , 2022.
JeanKaddour. Theminipilechallengefordata-efficientlanguagemodels. arXivpreprintarXiv:2304.08442 ,
2023.
JaredKaplan,SamMcCandlish,TomHenighan,TomBBrown,BenjaminChess,RewonChild,ScottGray,
Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint
arXiv:2001.08361 , 2020. URL https://arxiv.org/abs/2001.08361 .
DanielKhashabi,SewonMin, TusharKhot,AshishSabharwal,OyvindTafjord,PeterClark, andHannaneh
Hajishirzi. UnifiedQA:CrossingformatboundarieswithasingleQAsystem. In FindingsoftheAssociationfor
ComputationalLinguistics: EMNLP2020 ,2020. URL https://aclanthology.org/2020.findings-emnlp.
171.
J Peter Kincaid, Robert P Fishburne Jr, Richard L Rogers, and Brad S Chissom. Derivation of new readability
formulas (automated readability index, fog count and flesch reading ease formula) for navy enlisted
personnel. Technical report, Naval Technical Training Command Millington TN Research Branch, 1975.
JuliaKreutzer,IsaacCaswell,LisaWang,AhsanWahab,DaanvanEsch,NasanbayarUlzii-Orshikh,Allahsera
Tapo, Nishant Subramani, Artem Sokolov, Claytone Sikasote, et al. Quality at a glance: An audit of
web-crawled multilingual datasets. Transactions of the Association for Computational Linguistics , 10:50–72,
2022.
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti,
Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural Questions: a benchmarkfor
question answering research. Transactions of the Association for Computational Linguistics , 7:453–466, 2019.
William Labov. Principles of linguistic change, volume 3: Cognitive and cultural factors , volume 3. John Wiley &
Sons, 2011.
22
Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut.
ALBERT: A lite BERT for self-supervised learning of language representations. In International Conference
on Learning Representations , 2020.
Hugo Laurençon, Lucile Saulnier, Thomas Wang, Christopher Akiki, Albert Villanova del Moral, Teven
Le Scao, Leandro Von Werra, Chenghao Mou, Eduardo González Ponferrada, Huu Nguyen, et al. The
BigScienceROOTSCorpus: A1.6TBcompositemultilingualdataset. In Thirty-sixthConferenceonNeural
Information Processing Systems Datasets and Benchmarks Track , 2022.
AngelikiLazaridou,AdhiKuncoro,ElenaGribovskaya,DevangAgrawal,AdamLiska,TayfunTerzi,Mai
Gimenez, Cyprien de Masson d’Autume, Tomas Kocisky, Sebastian Ruder, et al. Mind the gap: Assessing
temporal generalization in neural language models. Advances in Neural Information Processing Systems , 34:
29348–29363, 2021.
Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and
Nicholas Carlini. Deduplicating training data makes language models better. In Proceedings of the 60th
Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 8424–8445,
2022.
Alyssa Lees, Vinh Q Tran, Yi Tay, Jeffrey Sorensen, Jai Gupta, Donald Metzler, and Lucy Vasserman. A
new generation of Perspective API: Efficient multilingual character-level transformers. arXiv preprint
arXiv:2202.11176 , 2022. URL http://perspectiveapi.com.com .
Adam Liska, Tomas Kocisky, Elena Gribovskaya, Tayfun Terzi, Eren Sezener, Devang Agrawal, Cyprien
De Masson D’Autume, Tim Scholtes, Manzil Zaheer, Susannah Young, Ellen Gilsenan-Mcmahon, Sophia
Austin, Phil Blunsom, and Angeliki Lazaridou. StreamingQA: A benchmark for adaptation to new
knowledge over time in question answering models. In Proceedings of the 39th International Conference
onMachineLearning , pages13604–13622, 2022. URL https://proceedings.mlr.press/v162/liska22a/
liska22a.pdf .
Yinhan Liu,Myle Ott, NamanGoyal, Jingfei Du,Mandar Joshi, DanqiChen, Omer Levy,Mike Lewis, Luke
Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized bert pretraining approach. arXiv
preprint arXiv:1907.11692 , 2019.
ShayneLongpre,YiLu,ZhuchengTu,andChrisDuBois. Anexplorationofdataaugmentationandsampling
techniques for domain-agnostic question answering. In Proceedings of the 2nd Workshop on Machine Reading
for Question Answering , pages 220–227, 2019.
ShayneLongpre,KartikPerisetla,AnthonyChen,NikhilRamesh,ChrisDuBois,andSameerSingh. Entity-
based knowledge conflicts in question answering. In Proceedings of the 2021 Conference on Empirical Methods
in Natural Language Processing , pages 7052–7063, 2021.
Shayne Longpre, Julia Rachel Reisler, Edward Greg Huang, Yi Lu, Andrew Frank, Nikhil Ramesh, and
ChristopherDuBois. Activelearningovermultipledomainsinnaturallanguagetasks. In NeurIPS2022
Workshop on Distribution Shifts: Connecting Methods and Applications , 2022.
ShayneLongpre,LeHou,TuVu,AlbertWebson,HyungWonChung,YiTay,DennyZhou,QuocVLe,Barret
Zoph,JasonWei,etal. TheFlancollection: Designingdataandmethodsforeffectiveinstructiontuning.
arXiv preprint arXiv:2301.13688 , 2023.
Alexandra Sasha Luccioniand Joseph D Viviano. What’s in thebox? a preliminary analysis ofundesirable
content in the Common Crawl corpus. arXiv preprint arXiv:2105.02732 , 2021.
Kelvin Luu, Daniel Khashabi, Suchin Gururangan, Karishma Mandyam, and Noah A Smith. Time waits for
no one! analysis and challenges of temporal misalignment. arXiv preprint arXiv:2111.07408 , 2021.
AmanMadaan,ShuyanZhou,UriAlon,YimingYang,andGrahamNeubig. Languagemodelsofcodeare
few-shot commonsense learners, 2022.
23
Nicholas Meade, Elinor Poole-Dayan, and Siva Reddy. An empirical survey of the effectiveness of debiasing
techniques for pre-trained language models. In Proceedings of the 60th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) , pages 1878–1898, 2022.
Rajiv Movva, Jinhao Lei, Shayne Longpre, Ajay Gupta, and Chris DuBois. Combining compressions for
multiplicativesizescalingonnaturallanguagetasks. In Proceedingsofthe29thInternationalConferenceon
Computational Linguistics , pages 2861–2872, 2022.
Alex Nichol. DALL-E 2 pre-training mitigations, 2022. URL https://openai.com/research/dall-e-2-
pre-training-mitigations .
Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob Mcgrew,
Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-
guided diffusion models. In International Conference on Machine Learning , pages 16784–16804. PMLR,
2022.
Nostalgebraist. Chinchilla’s wild implications. AI Alignment Forum , 2022. URL https://www.
alignmentforum.org/posts/6Fpvch8RR29qLEWNH/chinchilla-s-wild-implications .
The Open Team NYT. To apply machine learning responsibly, we use it in moderation,
2020. URL https://open.nytimes.com/to-apply-machine-learning-responsibly-we-use-it-in-
moderation-d001f49e0644 .
OpenAI. GPT-4 technical report. arXiv preprint arxiv:2303.08774 , 2023. URL https://arxiv.org/pdf/2303.
08774.pdf .
Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with
human feedback. arXiv preprint arXiv:2203.02155 , 2022. URL https://arxiv.org/abs/2203.02155 .
IsabelPapadimitriouandDanJurafsky. Learningmusichelpsyouread: Usingtransfertostudylinguistic
structure in language models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language
Processing (EMNLP) , pages 6829–6839, 2020.
MatthewE.Peters,MarkNeumann,MohitIyyer,MattGardner,ChristopherClark,KentonLee,andLuke
Zettlemoyer. Deep contextualized word representations. NAACL, 2018. URL https://aclanthology.
org/N18-1202 .
LuizaAmadorPozzobon,BeyzaErmis,PatrickLewis,andSaraHooker. Onthechallengesofusingblack-box
APIs for toxicity evaluation in research. In ICLR 2023 Workshop on Trustworthy and Reliable Large-Scale
Machine Learning Models , 2023.
Yada Pruksachatkun, Jason Phang, Haokun Liu, Phu Mon Htut, Xiaoyi Zhang, Richard Yuanzhe Pang,
ClaraVania,KatharinaKann,andSamuelBowman. Intermediate-tasktransferlearningwithpretrained
language models: When and why does it work? In Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics , pages 5231–5247, 2020.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models
areunsupervisedmultitasklearners. OpenAIblog ,1(8):9,2019. URL https://d4mucfpksywv.cloudfront.
net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf .
Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides,
Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models: Methods, analysis &
insights from training Gopher. arXiv preprint arXiv:2112.11446 , 2021. URL https://arxiv.org/abs/2112.
11446.
ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee,SharanNarang,MichaelMatena,YanqiZhou,
Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer.
Journal of Machine Learning Research , 21:1–67, 2020. URL https://arxiv.org/abs/1910.10683 .
24
AdamRoberts,HyungWonChung,AnselmLevskaya,GauravMishra,JamesBradbury,DanielAndor,Sharan
Narang,BrianLester,ColinGaffney,AfrozMohiuddin,CurtisHawthorne,AitorLewkowycz,AlexSalcianu,
Marc van Zee, Jacob Austin, Sebastian Goodman, Livio Baldini Soares, Haitang Hu, Sasha Tsvyashchenko,
AakankshaChowdhery,JasmijnBastings,JannisBulian,XavierGarcia,JianmoNi,AndrewChen,Kathleen
Kenealy, Jonathan H. Clark, Stephan Lee, Dan Garrette, James Lee-Thorp, Colin Raffel, Noam Shazeer,
MarvinRitter,MaartenBosma,AlexandrePassos,JeremyMaitin-Shepard,NoahFiedel,MarkOmernick,
Brennan Saeta, RyanSepassi, Alexander Spiridonov, JoshuaNewlan, and Andrea Gesmundo. Scaling up
models and data with t5xand seqio.arXiv preprint arXiv:2203.17189 , 2022. URL https://arxiv.org/
abs/2203.17189 .
Anna Rogers. Changing the world by changing the data. In Proceedings of the 59th Annual Meeting of
the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language
Processing (Volume 1: Long Papers) , pages 2182–2194, 2021.
Paul Röttger and Janet Pierrehumbert. Temporal adaptation of BERT and performance on downstream
document classification: Insights from social media. In Findings of the Association for Computational Lin-
guistics: EMNLP2021 ,pages2400–2412,PuntaCana,DominicanRepublic,November2021.Association
for Computational Linguistics. doi: 10.18653/v1/2021.findings-emnlp.206. URL https://aclanthology.
org/2021.findings-emnlp.206 .
Nithya Sambasivan, Shivani Kapania, Hannah Highfill, Diana Akrong, Praveen Paritosh, and Lora M Aroyo.
“Everyone wants to do the model work, not the data work”: Data cascades in high-stakes AI. In CHI,
CHI ’21, New York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450380966. doi:
10.1145/3411764.3445518. URL https://doi.org/10.1145/3411764.3445518 .
Maarten Sap, Saadia Gabriel, Lianhui Qin, Dan Jurafsky, Noah A. Smith, and Yejin Choi. Social bias frames:
Reasoning about social and power implications of language. In Proceedings of the 58th Annual Meeting of the
Association for Computational Linguistics , pages 5477–5490, Online, July 2020. Association for Computational
Linguistics. doi: 10.18653/v1/2020.acl-main.486. URL https://aclanthology.org/2020.acl-main.486 .
TevenLeScao,AngelaFan,ChristopherAkiki,ElliePavlick,SuzanaIlić,DanielHesslow,RomanCastagné,
AlexandraSashaLuccioni,FrançoisYvon,MatthiasGallé,etal. BLOOM:A176b-parameteropen-access
multilingual language model. arXiv preprint arXiv:2211.05100 , 2022.
John Schulman. Reinforcement learning from human feedback: Progress and challenges. Berkeley EECS ,
2023. URL https://www.youtube.com/watch?v=hhiLw5Q_UFg .
ThibaultSellam,SteveYadlowsky,IanTenney,JasonWei,NaomiSaphra,AlexanderD’Amour,TalLinzen,
Jasmijn Bastings, Iulia Raluca Turc, Jacob Eisenstein, et al. The MultiBERTs: BERT reproductions for
robustness analysis. In International Conference on Learning Representations , 2022.
AyushSinghandJohnEOrtega. Addressingdistributionshiftattesttimeinpre-trainedlanguagemodels.
arXiv preprint arXiv:2212.02384 , 2022.
Spandana Singh. Everything in moderation: An analysis of how internet platforms are us-
ing artificial intelligence to moderate user-generated content. New America , 2019. URL
https://www.newamerica.org/oti/reports/everything-moderation-analysis-how-internet-
platforms-are-using-artificial-intelligence-moderate-user-generated-content/ .
RohanTaori,IshaanGulrajani,TianyiZhang,YannDubois,XuechenLi,CarlosGuestrin,PercyLiang,and
TatsunoriB.Hashimoto. StanfordAlpaca: Aninstruction-followingLLaMAmodel. https://github.com/
tatsu-lab/stanford_alpaca , 2023.
YiTay,MostafaDehghani,SamiraAbnar,HyungWonChung,WilliamFedus,JinfengRao,SharanNarang,
VinhQTran,DaniYogatama,andDonaldMetzler. Scalinglawsvsmodelarchitectures: Howdoesinductive
bias influence scaling? arXiv preprint arXiv:2207.10551 , 2022.
25
Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,
Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. LaMDA: Language models for dialog applications. arXiv
preprint arXiv:2201.08239 , 2022. URL https://arxiv.org/abs/2201.08239 .
HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,TimothéeLacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. LLaMA: Open and efficient foundation
language models. arXiv preprint arXiv:2302.13971 , 2023.
BertieVidgen,TristanThrush,ZeerakWaseem,andDouweKiela. Learningfromtheworst: Dynamically
generateddatasetstoimproveonlinehatedetection.In Proceedingsofthe59thAnnualMeetingoftheAssociation
forComputationalLinguisticsandthe11thInternationalJointConferenceonNaturalLanguageProcessing(Volume
1: LongPapers) , pages1667–1682, Online, August2021.AssociationforComputationalLinguistics. doi:
10.18653/v1/2021.acl-long.132. URL https://aclanthology.org/2021.acl-long.132 .
Pablo Villalobos, Jaime Sevilla, Lennart Heim, Tamay Besiroglu, Marius Hobbhahn, and Anson Ho. Will
we run out of data? an analysis of the limits of scaling datasets in machine learning. arXiv preprint
arXiv:2211.04325 , 2022.
Thomas Wang, Adam Roberts, Daniel Hesslow, Teven Le Scao, Hyung Won Chung, Iz Beltagy, Julien Launay,
and Colin Raffel. What language model architecture and pretraining objective work best for zero-shot
generalization? ICML, 2022. URL https://arxiv.org/abs/2204.05832 .
Xinyi Wang, Yulia Tsvetkov, and Graham Neubig. Balancing training for multilingual neural machine
translation. In Proceedingsofthe58thAnnualMeetingoftheAssociationforComputationalLinguistics ,pages
8526–8537, 2020.
Alex Warstadt, Yian Zhang, Haau-Sing Li, Haokun Liu, and Samuel R Bowman. Learning which fea-
tures matter: RoBERTa acquires a preference for linguistic generalizations (eventually). arXiv preprint
arXiv:2010.05358 , 2020.
Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten
Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. TMLR, 2022.
URL https://openreview.net/forum?id=yzkSU5zdwD .
JohannesWelbl,AmeliaGlaese,JonathanUesato,SumanthDathathri,JohnMellor,LisaAnneHendricks,
KirstyAnderson,PushmeetKohli,BenCoppin,andPo-SenHuang. Challengesindetoxifyinglanguage
models. In Findings of the Association for Computational Linguistics: EMNLP 2021 , pages 2447–2469, 2021.
Yuhuai Wu,FelixLi,and PercyLiang. Insightsintopre-trainingviasimpler synthetictasks. In Advancesin
Neural Information Processing Systems , 2022.
Sang Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy Liang, Quoc V Le,
TengyuMa,andAdamsWeiYu. DoReMi: Optimizingdatamixturesspeedsuplanguagemodelpretraining.
arXiv preprint arXiv:2305.10429 , 2023a.
Sang Michael Xie, Shibani Santurkar, Tengyu Ma, and Percy Liang. Data selection for language models via
importance resampling. arXiv preprint arXiv:2302.03169 , 2023b.
Albert Xu, Eshaan Pathak, Eric Wallace, Suchin Gururangan, Maarten Sap, and Dan Klein. Detoxifying
language models risks marginalizing minority voices. In Proceedings of the 2021 Conference of the North
AmericanChapteroftheAssociationforComputationalLinguistics: HumanLanguageTechnologies ,pages2390–
2397, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.190.
URL https://aclanthology.org/2021.naacl-main.190 .
Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and
Colin Raffel. mT5: A massively multilingual pre-trained text-to-text transformer. In Proceedings of the 2021
Conference of the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies , pages 483–498, 2021.
26
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. XLNet:
Generalizedautoregressivepretrainingforlanguageunderstanding. Advancesinneuralinformationprocessing
systems, 32, 2019.
Huaxiu Yao, Caroline Choi, Bochuan Cao, Yoonho Lee, Pang Wei Koh, and Chelsea Finn. Wild-Time: A
benchmark of in-the-wild distribution shift over time. In Thirty-sixth Conference on Neural Information
Processing Systems Datasets and Benchmarks Track , 2022.
Michael Zhang and Eunsol Choi. SituatedQA: Incorporating extra-linguistic contexts into qa. In Proceedings
of the 2021 Conference on Empirical Methods in Natural Language Processing , pages 7371–7387, 2021.
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan,
Mona Diab, Xian Li, Xi Victoria Lin, et al. OPT: Open pre-trained transformer language models. arXiv
preprint arXiv:2205.01068 , 2022.
WayneXinZhao,KunZhou,JunyiLi,TianyiTang,XiaoleiWang,YupengHou,YingqianMin,BeichenZhang,
Junjie Zhang, Zican Dong, et al. A survey of large language models. arXiv preprint arXiv:2303.18223 , 2023.
YukunZhu,RyanKiros,RichZemel,RuslanSalakhutdinov,RaquelUrtasun,AntonioTorralba,andSanja
Fidler. Aligningbooksandmovies: Towardsstory-likevisualexplanationsbywatchingmoviesandreading
books. In Proceedings of the IEEE international conference on computer vision , pages 19–27, 2015.
27
Appendix
Contents
A Contributions 28
B Expanded Literature Review 29
C Experimental Details 29
C.1 Pretraining Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
C.2 Finetuning Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
C.3 Toxicity Evaluation Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
C.4 Time Evaluation Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
C.5 Evaluating Domains with Question Answering Datasets . . . . . . . . . . . . . . . . . . . . . 32
D Impact of Data Curation on Data Composition: Further Analysis 34
D.1 Feature Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
D.2 Breakdown of the Quality Filter on Pile Domains . . . . . . . . . . . . . . . . . . . . . . . . . 34
E Experimental Results 36
E.1 Temporal Degradation Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
E.2 Toxicity & Quality Filtering Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
A Contributions
•ShayneLongpre Projectleadandprimarycoder. Ledexperimentdesign,implementation,pretraining,
evaluation, and analysis.
•GregoryYauney Corecontributor. Ledevaluationimplementationandanalysisontoxicityandquality
filtering section (Section 5). Contributed code for Section 3. Also supported writing and analysis.
•Emily Reif Core contributor. Led the analysis of data characteristics pre- and post-curation (Section
3). Also supported writing and analysis.
•KatherineLee Corecontributor. Supportedinfrastructureimplementation,debugging,overallanalysis
and framing, especially for Section 4.
•David Mimno Core contributor. A primary advisor, supporting analysis, framing, writing, and
particularly discussion of key take-aways and recommendations (Section 7).
•DaphneIppolito Corecontributor. Theprimaryadvisorandalsoacodingcontributor,supporting
both the analysis, framing, and writing as well as running many of the experiments and evaluations.
•Adam Roberts Supporting advisor, especially on modeling choice and pretraining infrastructure.
•Barret Zoph Supporting advisor on experiment design.
•Denny Zhou Supporting advisor on writing and framing.
•Jason Wei Supporting advisor on experiment design, writing and framing.
•Kevin Robinson Supporting advisor on toxicity evaluations and their implementation.
28
Table 4: Additional notes on each model’s filtering details.
Model Filtering Details
Bert “ignore lists, tables, and headers”
GPT-2 removed Wikipedia
RoBerta CC filtered to news and Winograd-like subsets
XLNet “heuristics to aggressively filter out short or low-quality articles”
T5 Heuristic quality, toxicity, and length filters; code removed
GPT-3 Filtered based on similarity to high-quality reference corpora.
GPT-J/Neo Uses fasttext classifier on Pile-CC, with OpenWebText2 as the high-quality reference.
GLaM Classifier with Wikipedia, books and selected websites as positive examples
LaMDA “LaMDA SSI and safety discriminators are also used to score and filter 2.5M turns of dialog data
sampled from the pre-training dataset”, which are then trained on.
AlphaCode Filtering heuristics to exclude automatically generated code
CodeGen Heuristic filters for code quality
Chinchilla Heuristic-based quality filtering, SafeSearch filter
Minerva Same as PaLM for non-academic data
BLOOM heuristic-based quality and porn filtering
PaLM Same as GlaM
Galactica Apply several quality filters: exclude papers from journals with certain keywords or low journal
impact factor
LLaMA Classifier to filter out low-quality and un-Wikipedia-like text
B Expanded Literature Review
Table 4 lists popular and well-known models trained in the last several years and a summary of the available
information about their training data.
C Experimental Details
This section provides further details on the methodology and hyperparameter settings used for pretraining,
finetuning, and evaluation.
To allow for a model that can generate without finetuning but also perform well after finetuning, we rely
on the extensive experiments of Wang et al. (2022). Their empirical results suggest these criteria are met
with a Causal Decoding architecture with a Full Language Modeling pretraining objective (“CD-FLM”),
which permits generation without finetuning, followed by a Prefix Language Modeling objective (PLM) for
finetuning, where the causal attention mask is removed from the original prompt.
C.1 Pretraining Details
Our two pretraining datasets are C4 (Raffel et al., 2020) and the Pile (Gao et al., 2021). We use the same
vocabulary for both as used in the original T5 from Raffel et al. (2020). All training is conducted using T5X
(Roberts et al., 2022) and Tensorflow (Abadi et al., 2016) on TPUs. Specific hyperparameters for LM-XLand
LM-Small pretraining are detailed in Table 5.
C.2 Finetuning Details
Unlessotherwisenoted,evaluationwasperformedbyfinetuningonthetrainsetforeachbenchmarktask,and
then evaluating on either the validation or test set (specified in each section). Finetuning hyperparameters
are given in Table 6.
29
Table5:Pretraininghyperparameters WeadoptdefaultpretraininghyperparametersfromWangetal.(2022),
whoselect theirparameterstofairlycompareacross awiderange ofT5-basedpretrainingand architecture
experiments.
Parameter LM-XL LM-Small
TPUs 8x8x8 8x8
Batch Size 4096 4096
Sequence Length 512 512
Training Steps 88,064 88,064
Dropout 0.0 0.0
Base Learning Rate 0.5
Decay Factor 0.5
Warmup Steps 1000
Steps per Decay 20000
Table6:FinetuningandEvaluationParametersforeachsetofDownstreamTasks. Wereportthefinetuning
hyperparametersettingsandevaluationmetricusedforfinetuntingandevaluatingthepretrainedmodels.
We conduct finetuning for four sets of tasks: toxicity identification tasks (Toxigen, Social Bias Frames,
andDynaHate),NaturalQuestions(forpretrainingdomaintransferanalysis),generalNLUperformance
(SuperGLUE), and the Time tasks (including PubCLS, NewSum, PoliAff, TwiERC, and AIC). For T5 Small
models, we modify the number of training steps accordingly, as shown in the last row.
Parameter Tox-Identify Natural Qs SuperGLUE Time
LM-XL
TPUs 8x8 8x8 8x8 8x8
Sequence Length 128 512 512 128
Batch Size 128 128 128 128
Dropout 0.1 0.1 0.1 0.1
Training Steps 10k 50k 100k See Table 7
Learning Rate 1e-3 1e-3 1e-3 See Table 7
Eval Metric AUC-ROC Acc (By Dataset) See Table 7
LM-Small (where different)
Training Steps 30k 50k 100k See Table 7
C.3 Toxicity Evaluation Details
In this section we describe the evaluation details for the results presented in Section 5.
Toxicity Identification For toxicity identification evaluation, we finetune separately on each of the follow-
ing datasets: Social Bias Frames (SBF, Sap et al., 2020), DynaHate (DH, Vidgen et al., 2021), and Toxigen
(Hartvigsenetal.,2022). Wethenevaluateontheirrespectivetestsets,usingAUC-ROC,whichcomputes
the error rates over different classification thresholds. Note that each of these datasets has a slightly different
versionoftoxicitytheyareevaluatingfor: biasesandoffensivestereotypesimpliedinlanguageforSocialBias
Frames,typesofhatefulspeechlikeanimosity,dehumanization,andderogationforDynaHate,andadversar-
ial, subtle and implicit forms of toxicity towards minority groups (e.g., stereotyping, microaggressions), for
Toxigen. We average over them to understand a model’s capabilities in identifying toxicity broadly.
Toxic Generation For toxic generation, we do not do any finetuning. Instead, we evaluate how often each
model generates toxic continuations. To measure amodel’s propensity to generate sexual, profane, or toxic
responses to a mix of benign and malicious inputs, we use prompts from the RealToxicityPrompts (Gehman
etal.,2020)dataset,aswellasthepromptsetdevelopedtoexplorerepresentationalbiasinPaLM(Chowdhery
et al., 2022).
30
For each model, we generate 25 responses per prompt, using top- ksampling ( k= 40), with a temperature of
1.0. Following Chowdhery et al. (2022), we use the Perspective API to score responses, labeling those with a
toxicityscore >= 0.5astoxic. Wecomputetheprobabilityoneofthesegenerationsperpromptistoxic,for
two evaluation splits, one with non-toxic inputs, and one with toxic inputs. The resulting evaluation metrics
are denoted RPT-T (for Toxic inputs) and RPT-NT (for Non-Toxic inputs).
The Representational Bias benchmark was constructed from the identity terms in (Rae et al., 2021) and
is described in more detail in Chowdhery et al. (2022). It uses templates constructed for several identity
markers, for which we use the subset related to gender, race, and religion. Following the practice with
RealToxicityPrompts, we sample 50 responses per prompt, use top- ksampling ( k= 40,temperature = 1.0),
and then classify a response as toxic if Perspective API evaluates its toxicity score >= 0.5. We average
thetoxicityratesperidentitymarkerandperexampletocomputetheoverallRepBiasscore,wherehigher
indicates more toxic responses were produced on average. We also compute the 95% confidence interval to
show where changes in mean are significant.
C.4 Time Evaluation Details
ThissectiondescribestheevaluationdetailsfortheresultspresentedinSection4. Inappliedsettings,the
available training data (either for pretraining or finetuning) may be from different years than the test-time
data. To mimic these situations, Luu et al. (2021) construct several datasets segmented by the year they
are collected from in order to measure the performance impact of differences in the time of collection of
finetuning and evaluation splits. As described in Section 2.3, we select 5 of the datasets that are shown to be
quite sensitive to these temporal misalignments, and that cover different tasks and data sources. These tasks
are summarization, named entity recognition, classifying political affiliation, classifying academic topic, and
classifying the news source.
Duetotheuniquenatureofeachofthesetasksinthetemporaldegradationexperiments, wesimplyfinetune
on each task individually, before evaluating on their respective test sets. For each dataset, we finetune using
4x4 TPUs with a batch size of 64, a maximum sequence length of 128, and we validate every 500 training
steps. We select the test set score with the highest validation accuracy across training. The best learning
rateandthetotalnumberofstepsrequiredtoreachconvergencevariedbymodelandmodelsize,andare
reportedin Table7. Thesehyperparameters arechosen basedoninitial experimentsattemptingto produce
stable learning curves which peak near the values observed in Luu et al. (2021).
Table 7:Time Dataset & Training Details : For each of the five datasets used to evaluate the model’s ability
overdifferenttemporalperiods,wereportthelearningrateandnumberofstepsusedineachmodelsize.
These hyperparameters were chosen to ensure consistent convergence and stability within our infrastructure
settings.
LM-XL LM-Small
Domain Task Metric LR Steps LR Steps
NewsPubCLS Acc 1e-4 30k 1e-3 30k
NewSum Rouge-L 5e-4 40k 1e-3 40k
TwitterPoliAff Acc 1e-4 15k 1e-4 15k
TwiERC Acc 1e-4 30k 1e-3 30k
Science AIC Acc 1e-4 30k 1e-3 60k
WefollowLuuetal.(2021)’sexactprescriptionincalculatingTemporalDegradation(TD),aswellastheir
reportedPearsoncorrelationmeasurements( r). Temporaldegradationcanbeinterpretedastheaveragerate
of deterioration in performance for a time period, measured in years. Since a temporal deterioration score is
calculated per evaluation year, we average over all evaluation years to compute a final TD score for a dataset.
Furthermore, each dataset has a different span of available training and evaluation years. To account for this,
we followLuu et al.(2021) in presentingthe Pearson correlationcoefficient, which presentsthe strenght of
31
therelationshipbetweentimedifferencesandperformancedeterioration. WealsoreplicatetheWaldtest
with null hypothesis that the slope is zero.
For evaluating the temporal degradation of pretraining, TD p, we modify Luu et al. (2021)’s original formula
tomeasurethedifferent D(t′→t)where t′isnowthepretrainingyear. However,inthissetting,performance
samplesarerepresentedwithdifferentfinetuningyears. Toaccountforthis,weonlycomparetherelative
performance changes of the pretraining year tp, against models with the same finetuning tfand evaluation
years te. In other words, given Stp→tf→te, we will only compare its performance to St′p→tf→tewhere t′
p̸=tp,
buttfandteare fixed to their respective values.
D(t′
p→te) =−(St′p→t′′
f→te−Stp→t′′
f→te)∗sign (t′
p−te)
Insomeedgecases,thereisnoevaluationyearequivalenttoapretrainingyear, ∀t∈T, tp̸=te,andsothe
term Stp→t′′
f→te)doesnotexist. Inthiscase, wesetthistermtobetheonewhere tpandteareclosest. And,
as before, the precise term used will depend on which version of tfis being calculated for.
C.5 Evaluating Domains with Question Answering Datasets
This section describes the evaluation details for the results presented in Section 6. These experiments involve
pretrainingmodelswithdifferentsubsetsofthecorporafromthePile(Gaoetal.,2020)andseeingtheeffects
onavarietyofdownstreamevaluationdomains,representedbyquestionansweringdatasets. Assuch,we
are able to map the effects of pretraining domains to evaluation domains.
First, we discuss the construction of the pretraining domains. We partition the Pile’s source datasets into
categoriesrepresentingthematicallysimilarsourcesofdata,asseeninTable8. Werefertothesecategoriesas
Domains. These domain partitions are subjective and cannot perfectly separate out text into these categories.
Forinstance,Wikipedia,Books,andCommonCrawldatainevitablycontainsomeAcademicinformation,
but overall these partitions represent distinct features (see Section 3) that we have attempted to delineate by
areas of interest to practitioners and researchers. Prior work has attempted to measure, emphasize, or target
(either for inclusion or exclusion) the particular categories ofdata we’ve used in our partitions, such as more
booksandstructureddata(Brownetal.,2020;Chowdheryetal.,2022),codedata(Chenetal.,2021),and
legal data (Dodge et al., 2021), among others.
The Domains of the Pile were then each separately ablated from pretraining to understand the effect of their
absence. Toevaluatetheirabsenceontheperformanceofdownstreamdomains,wechosetousethequestion
answering task expressly because there is a wide variety of similarly formatted evaluation datasets available.
For these question answering datasets we train only on Natural Questions (Kwiatkowski et al., 2019), a
popular QA dataset, to teach the model the general task. For evaluation, as described in Section 2.3, we use
UnifiedQA (Khashabi et al., 2020) and MRQA (Fisch et al., 2019)’s collection of datasets to evaluate how
each pretrained modelperforms on a given“domain”, or set ofdatasets with similar sourcecharacteristics.
We partition the question answering datasets from UnifiedQA and MRQA into five categories. Datasets with
Wikipediadocumentsrepresentedintheircollectionareassignedtothe Wikicategory,datasetswithscraped
web documents or news are assigned to the Webcategory, and so on. Datasets may belong to multiple
categories, depending on how they were constructed. The question answering evaluation partitions are
showninTable9. Finally,weevaluateoneachquestionansweringdatasetandreporttheaverageF1score
for each category.
32
Table8:Partitions ofthe Pile’s Data Sourcesinto Domains The Pilecontains 22 distinct sourcesof data,
which we manually partition into 9 thematically similar domain clusters.
Category Components Size Description
CC Pile-CC 227 GB AfilteredsetofCommonCrawlwebsites,scrapedwith
JusText (Endrédy and Novák, 2013).
OpenWeb OpenWebText2 63GB ScrapedOpenWebTextCorpususingupvotedReddit
outgoing links.
Wikipedia Wikipedia (en) 6 GB The English scrape of Wikipedia.
Books Books3, BookCorpus2, Gutenberg
(PG-19)118 GB The Bibliotik general literature collection, PG-19’s pre-
1919 western classics, and BookCorpus’s set of yet un-
published works.
PubMed PubMedCentral,PubMedAbstracts 109 GB Biomedical articles from 1946 to present
Academic ArXiv, PhilPapers, NIH ExPorter 60 GB Preprint academic papers in Math, Computer Science,
Physics, and Philosophy.
Code & Math Github, StackExchange, DM Mathe-
matics135 GB Code repositories, documentation, coding questions
and answers, and mathematical problems.
Legal FreeLaw, USPTO Backgrounds 74 GB Court filings, judicial opinions, and patents
Social Ubuntu IRC, EuroParl, Enron
Emails, HackerNews, OpenSubti-
tles, YoutubeSubtitles33 GBMovieandvideosubtitles,chatlogs,emails,andtext
from social news websites.
Base All 825 GB A wide mix of online text from the web, wikipedia,
books, academic articles, code, legal, and social
sources.
Table 9: Partitions of Question Answering evaluation datasets from the UnifiedQA (Khashabi et al.,
2020) and MRQA (Fisch et al., 2019) collections. To evaluate the performance of pretraining strategies
on different text domains, we assign datasets into categories corresponding to their source material:web-
based,wikipedia,academic,biomedical,orand/books). Certaindatasetsarealsodesignedspecificallyto
testadvancedcommonsensereasoning,ordecisionboundariesusingcontrastsets(Gardneretal.,2020).
Datasets can belong to multiple categories.
Category Datasets Description
Wiki AmbigQA,DROP,HotpotQA,NaturalQuestions,
Quoref,RelationExtraction,ROPES,SearchQA,
SQuAD-1, SQuAD-2, TriviaQADatasets with Wikipedia text.
Web AmbigQA, CommonsenseQA, DuoRC, Natu-
ralQuestions, NewsQA, SearchQA, TriviaQADatasets partially sourced or collected from the
web, including user logs and news.
Books NarrativeQA A dataset sourced from books.
BioMed ARC-Easy, ARC-Hard, BioASQ, TextbookQA Datasetswithhigh-schoolorgraduatelevelsci-
entific or medical content.
Academic AI2-Elementary-Science,ARC-Easy,ARC-Hard,
RACE, ROPES, TextbookQAGeneral academic data and exams.
Common Sense CommonsenseQA, PhysicalIQA, SocialIQA Datasets which test common sense reasoning.
Contrast Sets Contrast-Set-DROP, Contrast-Set-Quoref,
Contrast-Set-ROPESDatasetsre-configuredasContrastSets(Gardner
et al., 2020), which are manual perturbations to
make examples more challenging.
33
D Impact of Data Curation on Data Composition: Further Analysis
D.1 Feature Definitions
As discussed in Section 3, we calculated a set of features across all datapoints to better understand the
distribution shifts for each ablation. The full list of features is as follows:
•Profanity,Toxicity,andSexuallyExplicit ThePerspectiveAPIclassifiestextasviolatingorpassing
each of these categories, as described in Section 2.2.
•Text Quality The same bag-of-words-based linear classifier as used in PaLM (Chowdhery et al., 2022)
andGLaM(Duetal.,2022),isusedtodistinguishbetweentextthatlookslikeWikipediaandbooks
from other text, as described in Section 2.2.
•Personally Identifiable Information (PII) A basic classifier, similar to Google Cloud NLP (2023a),
detects the presence of four categories of personally identifiable information: names,phone numbers ,
addresses , andemails.
•Readability The Flesch–Kincaid readability test (Kincaid et al., 1975) is applied to each document,
assigningdocumentsagradelevelbasedonthenumberofwordspersentenceandnumberofsyllables
per word.
•Average Word Length Measured in characters.
•Document Length Measured in characters.
•Non-ASCII Characters Measured as a percentage of all characters in the document.
•All-caps Words Measured as a percentage of all words in the document.
•Type-Token Ratio A measure of the lexical diversity, or the ratio of unique tokens to total tokens
(Bender, 2013).
•Sentiment The score assigned by a classifier similar to Google Cloud NLP (2023b), evaluating the
overall sentiment of the text along a spectrum from positive to negative.
Temporal informationin pretraining data While wecollected versions ofC4 at fourdifferent years, each
of these versions may also contain data from prior years. We estimate the temporal information in the
pretrainingdatabycountinginstancesofdatesfrom2000to2025ineachcorpus. Wedoseethatthereare
many mentions of the year of collection, with a quick dropoff of about 5 years earlier (see Figure 10). This is
necessarily a limited experiment asan article written in 2016 may still mention something occurring in the
futurein2019. However,sincewebsitecreationdatesarenotpartoftheweb-scrape,weusethisasaproxyto
estimate website creation dates.
D.2 Breakdown of the Quality Filter on Pile Domains
Whilethequalityfiltersaretypicallyappliedtolarge,heterogeneousdatasetssuchasC4,wealsoranthe
qualityclassifieronthePiletogetabetterunderstandingofwhattypesofdatapointsactuallypassedthe
quality filtering thresholds. The results are shown in Figure 11.
34
2013 2016 2019 2022
Profanity 0.101.4x 1.4x1.0x 1.0x
Toxicity 0.161.3x 1.3x1.0x 1.0x
Sexually
Explicit0.121.2x 1.2x1.0x 1.0x
Text
Quality0.261.1x 1.1x 1.0x.8x
Has
Person
Name0.371.3x 1.3x1.0x 1.1x
Has
Email0.032.2x 2.4x
1.0x1.9x
Has
Address 0.021.3x 1.3x1.0x1.4x
Has
Phone 0.031.4x 1.5x1.0x1.3x
Number
Of
Characters25921.1x 1.2x1.0x1.3x
Type
Token
Ratio0.581.0x 1.0x 1.0x 1.0x
%
Non-Ascii
Characters2.91e-03 .8x .9x 1.0x1.3x
%
All
Caps0.021.3x 1.3x1.0x 1.0x
Sentiment1.16 .9x .9x 1.0x 1.0x
Readability10 .9x .9x 1.0x .9xFeatures Across Time (C4)(a) Time in C4
C4 Pile
0.101.0x 1.1x
0.161.0x .9x
0.121.0x 1.0x
0.261.0x1.2x
0.371.0x 1.1x
0.031.0x1.2x
0.021.0x1.7x
0.031.0x.7x
25921.0x2.4x
0.581.0x.9x
2.91e-031.0x1.9x
0.021.0x1.2x
1.161.0x.8x
101.0x1.8xFeatures across C4 and The Pile (b) C4 vs the Pile
Figure9: FeaturedifferencesacrossC4andthePile,andtimesnapshotsofC4 Barheightindicatesaveragefeature
valueofeachdataset,exceptforthePIIcategorieswhichshowthefractionofdatapointscontainingthatPIItype. The
numbers are the fraction difference between the dataset and the baseline, which in this case is C4. The gray dashed line
and gray number show the actual value for the baseline.
2000 2003 2006 2009 2012 2015 2018 20210%10%20%30%C4 2013
2000 2003 2006 2009 2012 2015 2018 20210%10%20%30%C4 2016
2000 2003 2006 2009 2012 2015 2018 20210%10%20%30%C4 2019
2000 2003 2006 2009 2012 2015 2018 2021
Year0%10%20%30%Percentage of year mentions in datasetC4 2022
Figure 10: Date instances in each of the C4 temporal
pretraining versions.
0 0.025 0.06 0.1 0.2
Filtering cutoff0.00.20.40.60.81.01.2Number of examples1e8Data with a Quality Score >= Cutoff
Books
Social
Academic
Wiki
Legal
OpenWeb
PubMed
Code
CCFigure 11: Breakdown of domains in the Pile after fil-
tering for multiple quality cutoffs.
35
E Experimental Results
In this section, we lay out the raw results for our toxicity, quality, and temporal degradation evaluations,
spanning several evaluation datasets.
E.1 Temporal Degradation Results
Luu et al. (2021) measure the temporal degradation due to finetuning and evaluation misalignment. Before
attemptingtoevaluatemisalignmenteffectsspecificallyfor pretraining ,wemimictheirfinetuningexperiments.
Figure 12 shows our results, which corroborate the findings of (Luu et al., 2021).
2010 2012 2014 20162010
2012
2014
2016Finetune Years92.7 51.0 58.6 52.6
55.3 92.3 77.6 74.8
81.7 83.9 90.9 83.6
75.1 82.6 84.2 87.0PubCLS
2010 2012 2014 20162010
2012
2014
201631.9 32.1 25.2 17.1
21.5 39.0 29.9 14.4
19.5 34.8 34.7 15.6
19.5 20.6 20.6 25.8NewSum
2014 2015 2016 2017 2018 20192014-
2015
2016-
2017
2018-
201985.6 85.4 85.7 83.0 81.6 82.1
85.4 83.7 84.7 84.4 83.2 83.9
82.1 84.2 84.4 82.2 84.9 85.4TwiERC
2014 2015 2016 2017 2018 2019
Eval Years2014-
2015
2016-
2017
2018-
2019Finetune Years98.4 97.9 95.4 91.3 94.3 86.9
97.9 98.4 94.8 93.2 95.6 90.6
97.4 98.6 93.7 92.4 95.4 90.0AIC
2012 2013 2014 2015 2016 2017 2018 2019 2020 2021
Eval Years2009-
2013
2014-
2015
2016-
2017
2018-
2019
2020-
2021100.0 100.0 95.2 73.1 67.2 56.4 53.1 71.0 74.7 68.0
94.6 97.8 100.0 97.6 82.0 67.8 57.6 73.6 73.6 67.8
85.6 92.4 94.3 86.8 91.8 92.8 80.5 82.5 79.8 69.1
73.7 85.0 87.5 58.6 67.5 86.9 91.2 94.5 90.5 81.5
49.5 64.8 75.3 38.1 49.6 72.7 78.2 90.5 94.8 93.3PoliAff
Figure 12: A replicationof how temporal misalignment infinetuning affects task performance (Luu et al.,2021).
IncontrasttoFigure3,whichshowstheeffectsofpretrainingmisalignment,thisfigurefocusesonthemorewell
established effect of finetuning misalignment.
Next we share the original evaluation results from which we computed the temporal degradation values
for both finetuning and pretraining. These contain a cross-section of the scores produced using a given
pretrainingyear( y-axis),finetuningyear(s)( y-axis),foranevaluationyear( x-axis). Theseresults,Tables10
to 13, are provided for both LM-XLandLM-Small , for comparison.
36
Table10: Left:Fullresultsonthe PubCLS temporaltasksplitsfrom(Luuetal.,2021). Thistaskevaluatesnews
article sourceclassification, measured withAccuracy. Right:Full results onthe NewSum summarization
task temporal splits from (Luu et al., 2021), evaluated in Rouge-L.
Pretrain Finetune Eval Time
Time Time 2010 2012 2014 2016
LM-XL
20132010 93.7 51.9 58.4 52.5
2012 60.2 94.6 78.4 75.6
2014 83.1 85.6 90.8 84.8
2016 78.7 84.7 86.2 87.6
20162010 93.8 51.6 59.2 53.2
2012 55.2 93.9 79.5 77.0
2014 81.5 86.2 92.8 85.6
2016 76.9 82.9 84.3 89.6
20192010 92.9 50.6 58.6 52.2
2012 53.4 90.5 75.9 72.9
2014 81.3 83.2 90.6 82.8
2016 72.3 81.1 83.4 84.8
20222010 90.5 49.9 58.4 52.4
2012 52.4 90.4 76.4 73.9
2014 80.9 80.7 89.3 81.1
2016 72.3 81.7 83.0 86.1
LM-Small
20132010 92.9 51.9 60.2 54.1
2012 55.4 93.3 75.7 75.9
2014 78.2 81.9 89.9 82.5
2016 70.5 80.0 80.7 87.4
20162010 93.0 51.8 58.8 53.2
2012 56.7 92.9 77.7 75.5
2014 77.3 80.2 89.6 81.4
2016 69.9 80.1 82.1 87.7
20192010 92.9 51.3 59.2 53.0
2012 58.9 93.3 76.4 75.6
2014 78.4 82.1 90.2 82.7
2016 69.8 81.4 80.8 87.7
20222010 93.3 51.6 59.1 53.2
2012 56.2 93.2 75.6 75.1
2014 76.4 81.0 90.1 81.7
2016 67.8 80.4 80.1 86.8Pretrain Finetune Eval Time
Time Time 2010 2012 2014 2016
LM-XL
20132010 33.3 32.8 24.6 16.8
2012 21.4 39.5 30.0 14.1
2014 19.9 35.0 35.1 14.9
2016 19.9 21.2 21.1 25.7
20162010 31.9 33.3 27.1 17.8
2012 21.4 39.0 30.1 15.3
2014 20.2 35.0 34.5 17.2
2016 19.6 20.8 20.0 26.1
20192010 31.8 31.6 24.8 16.7
2012 21.4 39.1 29.3 13.6
2014 18.6 33.8 34.0 15.7
2016 19.5 20.1 21.4 26.2
20222010 30.7 30.8 24.4 17.2
2012 21.6 38.2 30.1 14.3
2014 19.5 35.5 35.0 14.7
2016 19.1 20.4 19.9 25.2
LM-Small
20132010 22.7 25.0 20.1 13.5
2012 14.0 24.5 19.5 9.9
2014 13.1 21.8 21.3 9.6
2016 14.1 17.8 17.5 18.4
20162010 22.1 25.5 20.7 14.0
2012 14.0 23.8 19.7 9.6
2014 13.5 22.8 21.5 10.0
2016 14.1 19.5 19.1 18.5
20192010 23.5 26.4 21.4 14.3
2012 14.5 25.4 20.6 10.1
2014 14.0 23.6 22.5 10.5
2016 15.1 20.1 19.2 18.5
20222010 23.4 26.2 21.1 14.1
2012 13.9 24.4 19.4 9.5
2014 13.6 23.2 21.7 9.7
2016 14.3 19.3 18.3 18.2
37
Table 11: Full results on the TwiERC temporal task splits from Luu et al. (2021). This task evaluates Twitter
Named Entity Classification with Accuracy.
Pretrain Finetune Eval Time
Time Time 2014 2015 2016 2017 2018 2019 2014 2015 2016 2017 2018 2019
LM-XL LM-Small
20132014-2015 98.0 97.7 94.6 88.0 93.1 83.4 86.1 85.5 85.7 83.2 80.5 81.9
2016-2017 98.2 96.6 94.4 91.6 94.0 88.2 86.1 84.0 84.7 83.9 84.0 83.7
2018-2019 97.4 97.6 94.0 91.5 95.4 87.9 82.9 85.2 84.2 81.2 84.6 85.0
20162014-2015 98.4 98.3 95.1 87.5 92.5 82.7 86.2 85.7 86.2 82.7 81.5 81.7
2016-2017 97.8 97.5 94.6 91.9 93.3 86.7 86.7 84.1 86.0 85.1 83.2 83.4
2018-2019 96.7 98.0 94.1 91.3 95.7 87.6 82.7 84.6 85.5 81.5 85.5 85.0
20192014-2015 98.3 97.7 94.4 88.4 93.7 82.1 85.6 85.4 85.3 83.1 82.2 83.2
2016-2017 97.7 97.5 93.5 89.6 94.3 88.6 85.7 83.8 83.8 85.4 83.5 84.8
2018-2019 96.4 97.9 93.5 90.3 95.9 88.1 82.4 83.9 84.7 83.5 85.6 86.0
20222014-2015 98.4 98.1 95.1 88.1 94.1 84.6 84.4 84.8 85.6 83.0 82.0 81.7
2016-2017 97.9 97.2 93.8 89.4 94.6 88.3 83.2 83.1 84.5 83.1 82.2 83.6
2018-2019 96.5 97.6 93.9 90.7 96.3 87.9 80.5 83.1 83.2 82.6 84.0 85.7
Table 12: Full results on the AICtemporal task splits from (Luu et al., 2021). This task evaluates the
classification of science articles from Semantic Scholar into those published at ICML or AAAI, measured
with Accuracy.
Pretrain Finetune Eval Time
Time Time 2014 2015 2016 2017 2018 2019 2014 2015 2016 2017 2018 2019
LM-XL LM-Small
20132014-2015 98.7 97.5 95.6 89.0 94.0 86.0 74.5 75.3 80.4 74.0 71.9 69.5
2016-2017 98.2 98.0 95.0 93.1 95.2 90.2 74.3 74.0 77.0 75.4 74.7 70.9
2018-2019 97.7 98.5 94.4 91.8 94.0 89.9 68.1 70.2 76.2 71.2 75.4 75.0
20162014-2015 98.5 98.4 95.6 92.0 94.3 86.3 74.9 75.9 81.7 74.4 71.0 70.7
2016-2017 98.0 98.1 95.4 94.0 95.1 89.7 74.1 72.9 78.9 74.0 74.1 70.0
2018-2019 97.6 98.2 94.6 93.4 95.8 89.4 69.5 70.3 76.7 72.1 75.3 75.3
20192014-2015 98.2 98.5 95.0 93.6 94.8 88.0 74.9 75.9 79.4 76.8 70.3 69.7
2016-2017 97.9 98.8 94.0 94.0 96.4 91.4 73.9 74.5 78.4 75.0 74.9 69.7
2018-2019 97.3 98.9 92.7 92.5 96.7 92.0 67.8 69.8 77.5 73.9 75.4 76.2
20222014-2015 98.2 97.4 95.3 90.6 94.2 87.3 72.8 78.6 78.3 72.6 70.7 69.5
2016-2017 97.5 98.9 94.7 91.7 95.8 90.9 71.9 73.4 77.6 74.4 72.6 69.0
2018-2019 97.0 98.8 93.1 91.9 95.1 88.7 66.8 71.6 74.6 73.9 74.7 72.7
38
Table 13: Full results on the PoliAfftemporal task splits from Luu et al. (2021). This task evaluates classifica-
tion of political affiliation from tweets, measured in Accuracy.
Pretrain Finetune Eval Time
Time Time 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021
LM-XL
20132009-2013 100.0 100.0 95.5 73.5 65.4 56.5 51.1 70.1 74.2 67.2
2014-2015 95.1 97.9 100.0 97.4 81.9 65.2 56.7 72.6 73.0 66.4
2016-2017 88.2 92.8 95.0 87.2 92.3 92.8 80.4 82.6 79.0 69.3
2018-2019 76.8 86.0 88.4 58.9 66.2 87.0 91.3 94.5 90.2 79.9
2020-2021 53.6 68.4 77.0 39.2 48.1 72.0 77.9 90.2 94.7 91.9
20162009-2013 100.0 100.0 94.9 72.9 67.8 55.8 53.7 70.3 73.7 67.5
2014-2015 94.9 98.2 100.0 97.3 82.5 68.4 58.7 73.0 73.4 67.9
2016-2017 85.0 92.6 94.6 87.8 91.8 93.1 80.7 83.0 79.9 69.2
2018-2019 73.1 85.2 87.9 58.3 68.5 88.1 91.3 94.4 90.4 81.5
2020-2021 49.0 64.3 75.5 38.0 50.8 73.4 78.6 90.5 94.6 93.7
20192009-2013 100.0 100.0 95.5 73.3 68.0 57.9 55.2 71.8 74.3 68.4
2014-2015 93.8 97.4 100.0 97.7 82.5 69.7 59.1 74.6 73.9 67.9
2016-2017 85.0 92.7 94.6 87.1 92.0 93.1 82.0 83.4 80.4 68.3
2018-2019 73.8 84.8 87.6 58.4 68.9 86.7 91.9 94.8 90.3 81.4
2020-2021 48.4 64.2 75.6 35.7 48.6 71.7 78.6 90.7 95.0 93.7
20222009-2013 100.0 100.0 94.9 72.6 67.5 55.6 52.3 72.0 76.7 69.0
2014-2015 94.4 97.9 100.0 97.9 81.0 68.0 56.1 74.3 73.9 68.8
2016-2017 84.1 91.5 93.2 85.2 90.9 92.2 78.7 80.9 79.7 69.6
2018-2019 71.1 83.9 86.0 58.9 66.6 85.8 90.3 94.5 91.1 83.0
2020-2021 47.2 62.4 73.0 39.6 50.8 73.6 77.5 90.6 94.9 93.8
LM-Small
20132009-2013 89.1 87.5 80.2 48.5 42.3 38.9 42.4 57.0 62.9 56.4
2014-2015 77.8 88.5 89.5 64.7 50.4 46.3 42.0 60.3 63.3 55.7
2016-2017 40.9 43.4 58.2 36.1 40.0 54.7 47.4 61.2 61.2 54.4
2018-2019 41.2 39.3 44.0 21.7 23.0 42.3 49.8 63.1 67.2 56.9
2020-2021 40.8 37.9 42.6 20.5 22.5 37.2 45.4 64.6 71.9 65.6
20162009-2013 89.9 89.2 80.5 51.7 45.7 39.9 42.6 57.7 62.6 55.4
2014-2015 78.2 87.8 87.4 63.9 49.6 45.6 41.8 59.7 61.9 54.3
2016-2017 51.3 49.3 57.9 37.4 38.1 51.1 46.3 60.2 60.2 53.6
2018-2019 49.8 43.1 46.5 24.4 26.8 42.6 48.3 62.9 66.3 56.2
2020-2021 51.7 43.0 42.5 22.7 24.8 36.3 40.8 61.5 70.1 63.3
20192009-2013 89.2 87.0 77.9 48.5 39.8 38.7 41.7 57.8 64.6 55.6
2014-2015 73.3 87.7 87.9 63.8 48.7 42.8 39.5 57.4 61.8 53.8
2016-2017 34.8 45.7 55.6 36.6 36.2 50.1 44.5 59.8 60.4 53.1
2018-2019 32.6 36.4 43.6 21.6 21.7 41.2 48.7 62.8 66.6 55.7
2020-2021 34.8 37.6 43.7 21.3 21.3 36.0 42.4 62.7 70.9 62.0
20222009-2013 90.3 88.8 79.0 47.9 41.0 37.6 40.9 57.9 64.7 56.6
2014-2015 76.9 89.7 90.3 67.2 54.6 45.2 41.0 60.5 63.4 56.5
2016-2017 41.5 48.8 56.9 37.0 38.6 53.7 47.7 62.0 60.7 53.2
2018-2019 33.0 34.3 39.2 19.9 20.5 43.2 50.9 65.5 68.8 56.4
2020-2021 39.5 37.0 38.5 19.4 19.6 33.6 41.8 65.2 72.8 66.1
39
E.2 Toxicity & Quality Filtering Results
We also provide full results for our experiments with toxicity and quality filters, presented in Section 5. The
evaluation results of the models with toxicityfilters applied to their data are visualized in Figure 5 (left) and
Figure13, withfulldetailsinTable14. Theevaluationresultsofthemodelswith qualityfiltersappliedto
their data are visualized in Figure 5 (right) and detailed in Table 15.
60%
 50%
 40%
 30%
 20%
 10%
 0% 10%
T oxic Generation Score6%
4%
2%
0%2%T oxicity Identification ScoreFull DatasetToxicity Filtering, LM-XL Pile
T=0.95T=0.9T=0.7
T=0.5
T=0.3
Most Filtering
more toxic less toxic
Figure 13: Toxicity filtering the Pile decreases the ability of LM-XLto identify toxicity and to generate
toxic text, just as with toxicity filtering C4.
Table 14: Toxicity filtering the pre-training dataset decreases the ability of LM-XLto identify toxicity and
to generate toxic text. These results are visualized in Figures 5 and 13.
Filter % Data Toxicity Identification ( ↑) Toxicity Generation ( ↓)
SBF Toxigen DH R3 DH R4 ScoreRTP-T RPT-NT RepBias Score
The Pile
Full Dataset 100.090.7 90.8 88.7 84.1 0.088.9 44.4 4.6 ±0.70.0
T=0.95 99.190.6 90.9 87.8 83.5 -0.585.6 43.9 4.6 ±0.8-1.9
T=0.9 97.490.2 90.8 86.4 83.7 -0.980.4 41.9 4.0 ±0.6-9.2
T=0.7 90.889.9 90.9 87.4 82.7 -1.083.3 39.9 2.9 ±0.5-18.1
T=0.5 80.789.4 90.4 86.0 82.8 -1.683.3 35 2.2 ±0.4-26.7
T=0.3 60.188.4 89.9 85.3 81.3 -2.778.5 31.4 2.2 ±0.5-31.1
Ngrams 70.789.7 90.4 86.3 82.4 -1.676.1 33.6 2.5 ±0.6-28.0
C4
Inverse T=0.06 92.293.2 91.4 90.0 85.7 1.487.8 49.6 4.8 ±0.815.6
Full Dataset 100.091.2 91.1 89.0 84.2 0.084.6 41.8 3.9 ±0.70.0
T=0.95 97.790.7 91.3 87.7 83.4 -0.784.3 41.9 3.9 ±0.70.0
T=0.9 94.990.4 90.6 87.5 83.9 -0.981.1 40.3 3.1 ±0.6-9.0
T=0.7 85.890.5 90.5 86.1 82.8 -1.671.3 34.8 2.4 ±0.5-23.8
T=0.5 75.889.8 90.5 86.9 81.9 -1.865.2 30.0 1.8 ±0.4-35.0
T=0.3 60.889.4 90.2 82.1 75.6 -5.255.0 19.8 1.2 ±0.3-52.1
Ngrams 78.689.8 90.7 87.0 81.8 -1.874.7 31.8 2.3 ±0.5-25.6
40
Table 15: Quality filtering the pre-training dataset decreases the ability of LM-XLto identify toxicity but
surprisingly increases toxicity generation. These results are visualized in Figure 5.
Filter % Data Toxicity Identification ( ↑) Toxicity Generation ( ↓)
SBF Toxigen DH R3 DH R4 ScoreRTP-T RPT-NT RepBias Score
C4
Inverse T=0.5 73.391.8 90.1 86.8 82.9 -0.986.3 44.3 4.1 ±0.6+9.7
Full Dataset 100.093.1 91.0 87.4 83.5 0.084.1 41.8 3.4 ±0.60.0
T=0.975 90.693.1 91.3 87.8 82.7 -0.185.4 46.0 3.8 ±0.7+7.3
T=0.95 83.993.2 91.3 89.4 85.0 +1.186.3 44.0 4.2 ±0.6+10.4
T=0.9 73.393.3 91.2 88.6 85.9 +1.285.2 44.8 4.3 ±0.7+11.1
T=0.7 45.693.3 91.4 89.9 86.6 +1.886.5 44.7 4.0 ±0.8+9.6
41