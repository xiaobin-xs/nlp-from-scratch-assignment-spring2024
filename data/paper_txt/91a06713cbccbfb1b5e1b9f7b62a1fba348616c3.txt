VOXTLM: UNIFIED DECODER-ONLY MODELS FOR CONSOLIDATING SPEECH
RECOGNITION, SYNTHESIS AND SPEECH, TEXT CONTINUATION TASKS
Soumi Maiti1, Yifan Peng1, Shukjae Choi2, Jee-weon Jung1, Xuankai Chang1, Shinji Watanabe1
1Carnegie Mellon University, USA242dot Inc., Republic of Korea
ABSTRACT
We propose a decoder-only language model, VoxtLM , that can per-
form four tasks: speech recognition, speech synthesis, text genera-
tion, and speech continuation. V oxtLM integrates text vocabulary
with discrete speech tokens from self-supervised speech features
and uses special tokens to enable multitask learning. Compared to
a single-task model, V oxtLM exhibits a significant improvement
in speech synthesis, with improvements in both speech intelligi-
bility from 28.9 to 5.6 and objective quality from 2.68 to 3.90.
V oxtLM also improves speech generation and speech recognition
performance over the single-task counterpart. Further, V oxtLM is
trained with publicly available data and training recipes and model
checkpoints are open-sourced to make fully reproducible work.
Index Terms —Multitask, speech synthesis, speech recognition,
spoken language model
1. INTRODUCTION
In recent years text language models (textLMs) have emerged as a
powerful generative model in natural language processing (NLP) [1–
3]. These textLMs can accommodate multiple tasks within a single
model, leading to improvement in performance across a variety of
tasks. On the other hand, with advances in discrete speech represen-
tations speech language models (speechLMs) [4, 5] have also been
proposed. However, prior speechLMs focus on individual tasks,
such as speech continuation or text-to-speech (TTS) [6, 7]. Our hy-
pothesis is that by unifying diverse speech tasks into a generative
language model (LM), we can potentially address multiple speech
tasks using a single model with improved generalization thanks to
multitask learning.
Traditionally, speech applications such as automatic speech
recognition (ASR) and text-to-speech (TTS) use encoder-decoder
architectures [8–10]. These architectures consist of an encoder,
for input processing and a decoder, for generating the output. For
example, speech-to-text involves a speech encoder and a text de-
coder, whereas text-to-speech employs a text encoder and a speech
decoder. Integrating task-specific and modality-specific encoder-
decoder components complicates the incorporation of multiple
tasks [11, 12]. In contrast, we can simplify multitask integration
with a joint speech-text decoder-only model (depicted in Fig. 1).
In this work, we investigate two main questions. Firstly, can we
cast diverse speech tasks as language modeling? ASR and TTS are
used as example speech tasks. Secondly, can we combine speech
tasks in a joint speech-text language modeling framework? To this
purpose, we introduce a novel LM framework VoxtLM (Voice-te xt
Language Model). V oxtLM combines multiple speech tasks within
a single autoregressive decoder model. Specifically, we combine
four tasks: speech recognition (speech-to-text), speech synthesis
(text-to-speech), text generation (text-to-text), and speech genera-
tion (speech-to-speech). We create a Voxt (voice + text) vocabulary
Decoder -Only
Model
Speech
T ext
V oxtLM
T ext
Speech
Speech EncoderText Decoder
Speech
ASRText EncoderSpeech  Decoder
T ext
Speech
TTST ext
Fig. 1: ASR and TTS use encoder-decoder architecture while
V oxtLM is decoder-only. In V oxtLM, all parameters are shared be-
tween speech and text modalities, compared to separate encoder/ de-
coder for speech and text.
by merging self-supervised discrete speech tokens with the text vo-
cabulary and incorporate sub-word modeling to efficiently process
long sequences of speech. We show that V oxtLM can model both
ASR and TTS as conditioned language model. In addition, com-
bining four tasks leads to improvement in speech generation, ASR,
and TTS. Significant improvement is observed in the TTS task with
improvement in both intelligibility (28.9 to 5.6) and neural-predicted
quality (2.68 to 3.90). Additionally, we demonstrate that improved
initialization with pretrained textLM and scaling model parameters
help in ASR. To ensure reproducibility, we use publicly available
datasets, open-source our training and inference in the form of
open-source toolkit ESPnet recipe1and make model checkpoints
available. TTS samples are also available.2
2. RELATED WORK
Discrete speech representations. Speech signals can be represented
as two types of discrete tokens: semantic tokens and acoustic to-
kens. Semantic tokens are quantized from self-supervised learning
features (e.g., HuBERT [13], w2v-BERT [14]) through clustering,
which mostly captures the linguistic content. Acoustic tokens are
generated by audio codec models [15, 16]. They capture rich acous-
tic information which is suitable for high-quality speech synthesis,
but they consist of multiple code streams and are thus difficult to
model. In this work, we follow GSLM [4] to use semantic tokens
derived from HuBERT.
Joint modeling of speech and text. Several studies [11,12,17] pro-
pose to learn shared speech-text representations in a self-supervised
manner. However, they employ separate encoders and decoders
for different modalities. They also require additional losses like an
alignment loss to encourage cross-modal transfer between speech
1https://github.com/ESPnet/ESPnet
2https://soumimaiti.github.io/icassp24_voxtlm/arXiv:2309.07937v3  [eess.AS]  24 Jan 2024
Table 1: Voxt data format for different tasks: training and inference. Inference: provided conditions for generating prediction.
Task Training Inference
Condition Prediction
TextLM ⟨generate-text ⟩, Y ⟨generate-text ⟩, Ytest ˆY
SpeechLM ⟨generate-speech ⟩, D ⟨generate-speech ⟩, Dtest ˆD
ASR ⟨start-speech ⟩,D,⟨generate-text ⟩,Y⟨start-speech ⟩, Dtest,⟨generate-text ⟩ ˆY
TTS ⟨start-text ⟩, Y,⟨generate-speech ⟩, D ⟨start-text ⟩, Ytest,⟨generate-speech ⟩ ˆD
Decoder -only
Language ModelSpeech
TokenizerVoxt 
VocabularyText ( )  
Speech
Token
DecoderText ( )Speech ( )
Speech ( )
Fig. 2: Overview of V oxtLM, our proposed autoregressive decoder-
only LM incorporating speech and text within an integrated vocabu-
laryVvoxt. The model uses two additional modules, the speech tok-
enizer and the speech token decoder to facilitate the conversion be-
tween continuous speech signal and discrete speech tokens.
and text. Recent concurrent studies employ a single model for mul-
tiple speech and text conversion tasks [18–20], which are similar
to our approach. SpeechGPT [20] uses a three-stage adaptation
to combine audio generation with textLMs. PolyV oice [18] ap-
plies speechLM to speech-to-speech translation (S2ST) with three
decoder-only LMs. VioLA [19] extends V ALL-E [7] for ASR and
S2ST. Among them, VioLA is the most related method to this work.
However, VioLA does not incorporate speech or text continuation
tasks and requires additional sequence modeling for speech repre-
sentations, which makes it more complicated than our approach.
Moreover, we utilize textually pre-trained OPT [21] for better ini-
tialization inspired by [22] and leverage different speech tokens.
Also in comparison to other works, our work is fully reproducible.
3. METHOD
Consider Y= (yi∈ V txt|i= 1,···, ttxt)is a text utterance from
a vocabulary Vtxtwith length ttxt. The probability of Ycan be ex-
pressed as p(Y) = Πttxt
i=1p(yi|y1,···, yi−1).Now, when dealing
with a continuous speech signal, we can convert it into discrete
speech tokens (dst), represented as D= (di∈ V dst|i= 1,···, tdst)
using a tokenizer. In this context Vdstis the vocabulary of discrete
speech tokens. These discrete speech tokens can be treated as spo-
ken language within Vdstand modeled in a manner similar to text.
We combine text and speech in a new vocabulary Voxt vocabulary by
Vvoxt=Vtxt∪ V dst. Therefore, we can model the probability of both
speech and text tokens as Z, where Z= (zi∈ V|i= 1,···, t).
This probability is expressed as:
p(Z) = Πt
i=1p(zi|z1,···, zi−1). (1)
Here, Zcan represent discrete speech tokens D(V=Vdst)or text
tokens Y(V=Vtxt)or various combinations of YandD.
3.1. VoxtLM
Fig. 2 illustrates the model’s overall architecture. Input of V oxtLM
can be both speech and text within the Vvoxtvocabulary. To process
speech, we use two additional modules to convert between contin-
uous and discrete domains in speech. The speech tokenizer maps
XtoD, while the speech token decoder maps generated ˆDbackTable 2: Number of utterances used in training of different V oxtLM
setups. Bal: balanced data for four tasks; and 3M: uses the same
number (3M) of text-only and speech-only utterances, a balanced
setup for total text and total speech data.
SourceUnpaired PairedConfigurationSpeech Text ASR TTS
DBal 300K 300K 281K 404K LL+LS+LT+VC
D3M 3M 3M 281K 404K LL+LS+LT+VC
DSet 12M 40M 281K 404K LL+LS+LT+VC
DSet+ 12M 40M 11M 404K LL+LS+LT+VC+MLS
toˆX. Similar to [4], our speech tokenizer uses k-means cluster-
ing to derive discrete features from the pretrained HuBERT [13]. It
is worth noting that selecting a small kvalue may capture linguis-
tic information effectively, but might fall short in representing other
acoustic aspects particularly crucial for speech synthesis. We experi-
ment with different kto assess the impact. Furthermore, within Vvoxt
vocabulary, we apply subword modeling [23–25] to replace frequent
patterns with metatokens. Such subword modeling technique is used
to include more contextual information in text [1] or to reduce the
long sequence length of speech [26].
3.1.1. Data format
We use special tokens to guide the model in performing var-
ious tasks. Four such tokens are used: ⟨start-text ⟩and
⟨start-speech ⟩indicate the beginning of text or speech con-
ditioning in the language model. ⟨generate-speech ⟩and
⟨generate-text ⟩instruct the model whether to generate speech
or text. Table 1 shows examples of the Voxt data format for various
tasks during training. Ideally, we can extend to more tasks with
additional task-specific tokens.
3.1.2. Training
V oxtLM consists of an embedding layer and a series of trans-
former [27] decoder layers. The embedding layer maps input Z
(in Eq. 1) into F-dimensional feature space, E= (ei∈RF|i=
1,···, t)using an embedding table of size |Vvoxt| ×F. We use L
transformer decoder layers with Hattention heads. The model’s
output includes a linear layer followed by softmax, generating a
probability distribution over the tokens in Vvoxt. V oxtLM is trained
as an autoregressive language model. In training, teacher forcing
is used for the preceding tokens. Given Z, at each timestep i,
predicted distribution is ˆpi=V oxtLM (z1,···, zi−1). Given true
probability distribution pi, the loss is calculated using cross-entropy
asLCE(pi,ˆpi) =−PVvoxt
c=1pi(c) log ˆpi(c).
Initialization with pretrained textLM . Previous work [22] shows
that in speechLM initializing with a pre-trained textLM achieves bet-
ter performance and faster convergence. Motivated by this, we use
the pretrained textLM OPT [21] to initialize V oxtLM weights and
learn the embedding table from scratch. The same model config-
uration is used as the pretrained model except for |Vvoxt|. OPT is
used due to training on publicly available data and the availability of
smaller pretrained models.
Table 3: Experimental results comparing multitasking V oxtLM against four single-task V oxtLM for textLM, speechLM, ASR and TTS. We
use token size ( k) 50 for all models. Single-task models are trained with all available data, for V oxtLM we report different training data
(Table 2) cases . For ASR we report test-clean/test-other results. D∗
Set: four single-task models whereas other rows depict multitask model.
Source # paramsTextLM SpeechLM ASR TTS
PPL(↓) sWUGGY( ↑) sBLIMP( ↑) PPL(↓) sWUGGY( ↑) sBLIMP( ↑) WER(↓) CER(↓) MOSNet( ↑)
D∗
Set125M 18.3 77.1 80.3 73.8 62.9 53.9 8.8 / 21.4 28.9 2.68
DBal 125M 15.4 77.7 66.7 68.5 60.7 52.7 8.6/20.9 5.6 3.76
D3M 125M 13.5 77.9 68.0 58.1 63.6 55.2 11.0 / 24.4 7.0 3.90
DSet 125M 11.1 80.3 74.2 62.1 62.8 54.1 21.0 / 37.4 8.8 3.86
3.1.3. Inference
The prediction from V oxtLM is expressed as:
prediction ←p(·|condition ). (2)
For TTS, condition is the test text utterance Ytestand prediction is
speech tokens ˆD. In ASR, condition is test speech tokens Dtestand
prediction is the recognized text ˆY. For speech continuation, condi-
tion involves prefix speech tokens Dtestand prediction is continued
speech tokens ˆD. For text continuation, the condition is text Ytest
and prediction is continued text ˆY(summarized in Table 1. We use
beam search in the inference phase.
Speech token decoder. The speech token decoder takes both ˆDand
a speaker embedding sspk∈RNof dimensionality Nas inputs and
produces ˆX. We use the HiFiGAN [28] as the architecture and x-
vector [29] as speaker embedding vector.
3.2. Evaluation Metrics
• For speech and text generation, we use perplexity (PPL) for evalu-
ating models with same vocabulary size. For different vocabulary
size models, we use spot-the-word error using sWUGGY and syn-
tactic score using sBLIMP dev set [30]. sWUGGY and sBLIMP
are chosen as other speech LM works also report them.
• For ASR, we use the word error rate (WER).
• For TTS, we measure intelligibility with character error rate
(CER) and quality using the neural-predicted mean opinion score
(MOS) with MOSNet [31,32]. We choose neural MOS prediction
model because it scales to large number of evaluations and shows
high-correlation with TTS evaluations in English.
4. EXPERIMENTS
Dataset. We use a combination of speech-only, text-only, and paired
speech-text datasets from public corpora.
•Speech-only data : we use LibriLight (LL) [33] with 60K hours of
audiobook speech from 7K speakers (12M utterances).
•Text-only data : we use the Librispeech (LS) [34] external textLM
dataset (40M text utterances).
•Speech-text paired data :
–For ASR, we use Librispeech [34] with 960 hours of data ( 281K
utterances). For an additional supervised data experiment, we use
English Multilingual Librispeech (MLS) [35] with 44K hours of
data from 5490 speakers (11M utterances).
–For TTS, we use LibriTTS (LT) [36] with 580 hours of audiobook
data from 2456 speakers and VCTK (VC) [37] with 44 hours of
studio recorded data from 109 speakers (404K utterances).
We standardized the data by downsampling speech to a 16kHz rate,
converting text to lowercase, and removing punctuation. We use sep-
arate test/dev sets for each task. For textLM and speechLM, we useTable 4: Experimental results comparing with and without initial-
ization with pretrained (PT) textLM for V oxtLM- k50 with DSet.
Name # paramsTextLM SpeechLM ASR TTS
PPL(↓) PPL(↓) WER(↓) CER(↓) MOSNet( ↑)
w/o PT 125M 11.1 62.1 21.0 / 37.4 8.8 3.86
w/ PT 125M 10.4 58.8 13.1 /28.8 9.4 3.92
the test set from LS and dev sets from sWUGGY and sBLIMP, text
for textLM, and speech counterpart for speechLM. For ASR we use
speech-text test set from LS test-clean and test-other and report both
test-clean/test-other separately. In TTS for computational efficiency,
we create a test set of 100 utterances from two speakers from the
LT test-clean. The test speakers are chosen via random sampling
(specifically, speaker ids 1089 and 1284).
Experimental setup. To train the sub-word model, we use paired
text-speech from ASR and TTS datasets. We experiment with three
kvalues (introduced in Sec. 3.1), 50, 200, and 1000, denoted as
VoxtLM -k. We also vary BPE sizes, setting them at 2K,5K, and
10K for kvalues 50, 100 and 200, respectively. We use three con-
figurations, small ( L=12,F=768, H=12), medium ( L=24,F=1024,
H=16), and large ( L=24,F=2048, H=32), with L,HandFde-
tailed in Sec. 3.1.2. We use 4 A100 GPUs for training small/medium
and 8 A100 GPUs for large with Adam optimizer [38] and warmup
learning rate schedule. Training data size varies considerably be-
tween different tasks. For example, the paired data for ASR and TTS
are100×smaller than text-only data and 40×smaller than speech-
only data. We can assume that achieving optimal performance across
all tasks requires balanced data for each of them. It is also worth not-
ing that text-only data is more readily available compared to speech-
only and paired data. Nonetheless, to assess the effect of different
dataset sizes for tasks, we consider balanced and unbalanced data
sets for training, as summarized in Table 2.
4.1. Results
Single vs multitask. We compare multitask and four single-task
models using V oxtLM- k50. Single-task LMs are trained separately
for each task (ASR, TTS, speechLM, and textLM) and are reported
in the first row of Table 3, with each column representing a separate
single-task model. Compared to single-task, V oxtLM shows compet-
itive results for all four tasks, although the best model differs. For
textLM DSetexhibits higher sWUGGY but lower sBLIMP score. In
speechLM, D3Mhas the best scores in both sWUGGY and sBLIMP,
followed by DSet. In TTS, all multitask models show improvement
compared to single task. ASR reports improvement in DBal. We note
that ASR is most affected in the unbalanced case: probably due to
the lower ASR data ratio to textLM/speechLM ( 100×/40×less). A
smaller degradation in ASR is also observed in D3Mwhere ASR data
ratio to textLM/speechLM is relatively better ( 10×less).
Initialization with pretrained textLM. We compare with and with-
out initialization with OPT for V oxtLM- k50 with DSetand report
in Table 4. Initialization improves the performance of three tasks:
Table 5: Experimental results comparing speech token size kfor V oxtLM. We compare the two conditions: DBalandDSet(Table 2).†denotes
initialization with OPT.
Name Source # paramsTextLM SpeechLM ASR TTS
PPL(↓) sWUGGY( ↑) sBLIMP( ↑) PPL(↓) sWUGGY( ↑) sBLIMP( ↑) WER(↓) CER(↓) MOSNet( ↑)
V oxtLM- k50 DBal 125M 15.4 77.7 66.7 68.5 60.7 52.7 8.6 / 20.9 5.6 3.76
V oxtLM- k200 DBal 125M 21.6 77.3 67.9 58.6 61.6 52.1 6.1 / 15.4 3.2 4.36
V oxtLM- k1000 DBal 125M 26.3 76.4 67.6 38.7 60.7 52.5 5.4/14.5 2.6 4.30
V oxtLM- k50†DSet 350M 10.3 81.0 75.1 68.2 62.7 53.8 13.5 / 27.2 6.6 3.91
V oxtLM- k200†DSet 350M 12.7 80.2 78.8 45.7 65.5 55.3 6.5/17.6 3.5 4.36
Table 6: Experimental results comparing larger model size and more supervised data for V oxtLM.†denotes initialization with OPT.
Name Source # paramsTextLM SpeechLM ASR TTS
PPL(↓) sWUGGY( ↑) sBLIMP( ↑) PPL(↓) sWUGGY( ↑) sBLIMP( ↑) WER(↓) CER(↓) MOSNet( ↑)
V oxtLM- k200†DSet 350M 12.7 80.2 78.8 45.7 65.5 55.3 6.5 / 17.6 3.5 4.36
V oxtLM- k200†DSet 1.3B 11.4 80.9 80.2 42.1 66.1 56.7 4.6 / 12.1 3.9 4.33
V oxtLM- k200†DSet+ 350M 13.6 80.1 77.0 45.5 64.1 55.0 3.5 / 8.7 3.5 4.33
V oxtLM- k200†DSet+ 1.3B 12.1 81.2 80.0 40.9 65.6 57.1 2.7/6.5 3.6 4.33
V oxtLM- k1000†DSet+ 125M 18.3 77.3 75.2 36.7 60.7 51.3 4.7 / 11.9 3.9 4.28
V oxtLM- k1000†DSet+ 350M 16.4 79.6 77.8 32.3 62.1 53.2 3.5 / 8.6 6.1 4.27
Table 7: SpeechLM and ASR results: Comparison with the state-of-
the-art models with V oxtLM.†denotes initialization with OPT.
Name Source # paramsSpeechLM ASR
sWUGGY( ↑) sBLIMP( ↑) WER(↓)
GSLM- k50 [4] LL 172M - 55.9 - / -
GSLM- k200 [4] LL 172M - 53.0 - / -
AudioLM [5] LL 900M 71.5 64.7 - / -
ASR-Fbank LS 149M - - 2.2 / 4.6
dst-ASR-Hubert LS 39M - - 4.2 / 10.8
V oxtLM- k200†DSet 1.3B 66.1 56.7 4.6 / 12.1
V oxtLM- k200†DSet+ 1.3B 65.6 57.1 2.7 / 6.5
Table 8: Comparison with the state-of-the-art baseline for TTS with
V oxtLM.†denotes initialization with OPT.
Name Source # paramsTTS
CER(↓) MOSNet( ↑)
VITS [39] LT 97M 7.7 4.20
V oxtLM- k200†DSet 350M 3.5 4.36
V oxtLM- k1000 DBal 125M 2.6 4.30
textLM, speechLM, and ASR. For TTS, a slight degradation in CER
is observed, whereas objective quality improves. In particular, bet-
ter initialization aids ASR performance in the unbalanced scenario
(reducing test-clean WER from 21.0 to 13.1).
Effect of token vocabulary size. We compare k=50, 200 and 1000
as outlined in Table 5. Comparisons are made in DBalandDSet.
For ASR and TTS, performance of k=50 is poor. For speechLM
withDSetbest sores on sWUGGY and sBLIMP are observed with
thek=200 model. TextLM, as expected, does not show a significant
pattern with varying k.
Scalability. Next, we explore whether model size can help with data
balancing by comparing medium and large models with k=200, pre-
sented in Table 6. All metrics in TextLM, speechLM, and ASR show
improvement with larger model. TTS shows a very small degrada-
tion in intelligibility ( 0.4) and quality ( 0.03). To mitigate the smaller
ratio of paired data, we incorporate more supervised data for ASR in
DSet+. We compare with k= 200 and k= 1000 and observe an im-
provement in the ASR task.
Comparison with single-task state-of-the-arts. Furthermore, we
compare with state-of-the-art models in TTS, ASR, and speechLM.
Note that these models aren’t fully comparable due to differences
in training data, strategies, and architecture. Following models areused: for speechLM, we use GSLM [4] and AudioLM [5], for TTS,
we use VITS [39] and for ASR we use E-Branchformer [40]. For
ASR, we compare two models: one using spectrogram as input
(ASR-Fbank) and another using discrete speech tokens as input (dst-
ASR-Hubert), trained following the procedure [26] and the same
speech tokenizer as V oxtLM- k1000. We use a pretrained VITS
model with LibriTTS. For speechLM (Table 7), GSLM- k200 which
uses the same tokenizer and a similar one-stage model, sBLIMP
score is lower compared to V oxtLM. However, in AudioLM which
uses two token representations (acoustic and semantic) and a three-
stage model, both sWUGGY and sBLIMP scores are higher, sug-
gesting potential for further improvement with hierarchical tokens
and multistage training. For ASR, compared to dst-ASR-Hubert,
which used the same tokenizer as V oxtLM, we observe a lower
WER. Compared to ASR-Fbank (no tokenizer), WER is higher,
such a trend is also observed in other discrete ASR models [26]. In
TTS (Table 8), compared to VITS, V oxtLM reports better intelligi-
bility and quality. Although V oxtLM is trained with a larger data set
compared to VITS, it is interesting to note that for traditional TTS
diverse training data with more noise and more speakers degrade
performance but here improvement is observed.
Finally, our experimental results show that both ASR and TTS
can be modeled as language modeling tasks. Moreover, using spe-
cial tokens we can combine ASR and TTS with joint speech-text
language modeling framework. Although the four tasks are quite
different, combining four tasks leads to improvement.
5. CONCLUSION
The integration of speech and text tasks within a joint language mod-
eling framework presents a promising avenue for speech process-
ing. We present a special token-based approach to combine four
speech and text tasks: speech recognition, speech synthesis, text and
speech generation. Our results demonstrate that by integrating dif-
ferent speech tasks into one generative model, we can improve the
performance of the tasks. In particular, TTS shows impressive per-
formance compared to the state-of-the-art VITS. We will expand this
work to include more speech tasks in the future.
Acknowledgements Experiments of this work used the Bridges2 sys-
tem at PSC and Delta system at NCSA through allocations CIS210014 and
IRI120008P from the Advanced Cyberinfrastructure Coordination Ecosys-
tem: Services & Support (ACCESS) program, supported by National Science
Foundation grants #2138259,#2138286, #2138307, #2137603, #2138296.
6. REFERENCES
[1] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever et al. , “Improv-
ing language understanding by generative pre-training,” 2018. 1, 2
[2] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever et al. ,
“Language models are unsupervised multitask learners,” OpenAI blog ,
vol. 1, no. 8, p. 9, 2019. 1
[3] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhari-
wal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al. , “Language
models are few-shot learners,” in Proc. NeurIPS , vol. 33, 2020, pp.
1877–1901. 1
[4] K. Lakhotia, E. Kharitonov, W.-N. Hsu et al. , “On generative spoken
language modeling from raw audio,” Transactions of the Association
for Computational Linguistics , vol. 9, pp. 1336–1354, 2021. 1, 2, 4
[5] Z. Borsos, R. Marinier, D. Vincent et al. , “Audiolm: a language mod-
eling approach to audio generation,” IEEE/ACM TASLP , 2023. 1, 4
[6] T. Hayashi and S. Watanabe, “Discretalk: Text-to-speech as a machine
translation problem,” arXiv preprint arXiv:2005.05525 , 2020. 1
[7] C. Wang, S. Chen, Y . Wu, Z. Zhang, L. Zhou, S. Liu, Z. Chen, Y . Liu,
H. Wang, J. Li et al. , “Neural codec language models are zero-shot text
to speech synthesizers,” arXiv preprint arXiv:2301.02111 , 2023. 1, 2
[8] R. Prabhavalkar, T. Hori, T. N. Sainath, R. Schlüter, and S. Watan-
abe, “End-to-end speech recognition: A survey,” arXiv preprint
arXiv:2303.03329 , 2023. 1
[9] J. Shen, R. Pang, R. J. Weiss, M. Schuster, N. Jaitly, Z. Yang, Z. Chen,
Y . Zhang, Y . Wang, R. Skerrv-Ryan, R. A. Saurous, Y . Agiomvrgian-
nakis, and Y . Wu, “Natural tts synthesis by conditioning wavenet on
mel spectrogram predictions,” in Proc. ICASSP , 2018. 1
[10] Y . Ren, C. Hu, X. Tan, T. Qin, S. Zhao, Z. Zhao, and T.-Y . Liu, “Fast-
speech 2: Fast and high-quality end-to-end text to speech,” in Proc.
ICLR , 2020. 1
[11] J. Ao, R. Wang, L. Zhou, C. Wang, S. Ren, Y . Wu, S. Liu, T. Ko, Q. Li,
Y . Zhang, Z. Wei, Y . Qian, J. Li, and F. Wei, “SpeechT5: Unified-
modal encoder-decoder pre-training for spoken language processing,”
inProceedings of the 60th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) . Dublin, Ireland:
Association for Computational Linguistics, May 2022, pp. 5723–5738.
[Online]. Available: https://aclanthology.org/2022.acl-long.393 1
[12] Z. Chen, Y . Zhang, A. Rosenberg, B. Ramabhadran, P. Moreno,
A. Bapna, and H. Zen, “Maestro: Matched speech text representations
through modality matching,” in Proc. Interspeech , 2022. 1
[13] W.-N. Hsu, B. Bolte, Y .-H. Tsai et al. , “HuBERT: Self-supervised
speech representation learning by masked prediction of hidden units,”
IEEE/ACM TASLP , 2021. 1, 2
[14] Y .-A. Chung, Y . Zhang, W. Han et al. , “w2v-BERT: Combining con-
trastive learning and masked language modeling for self-supervised
speech pre-training,” in Proc. ASRU , 2021, pp. 244–250. 1
[15] N. Zeghidour, A. Luebs, A. Omran et al. , “SoundStream: An end-
to-end neural audio codec,” IEEE/ACM TASLP , vol. 30, pp. 495–507,
2022. 1
[16] A. Défossez, J. Copet, G. Synnaeve, and Y . Adi, “High fidelity neural
audio compression,” arXiv preprint arXiv:2210.13438 , 2022. 1
[17] A. Bapna, Y .-a. Chung, N. Wu, A. Gulati, Y . Jia, J. H. Clark, M. John-
son, J. Riesa, A. Conneau, and Y . Zhang, “Slam: A unified encoder
for speech and language modeling via speech-text joint pre-training,”
arXiv preprint arXiv:2110.10329 , 2021. 1
[18] Q. Dong, Z. Huang, C. Xu, Y . Zhao, K. Wang, X. Cheng, T. Ko,
Q. Tian, T. Li, F. Yue et al. , “Polyvoice: Language models for speech
to speech translation,” arXiv e-prints , pp. arXiv–2306, 2023. 2
[19] T. Wang, L. Zhou, Z. Zhang, Y . Wu, S. Liu, Y . Gaur, Z. Chen, J. Li, and
F. Wei, “Viola: Unified codec language models for speech recognition,
synthesis, and translation,” arXiv preprint arXiv:2305.16107 , 2023. 2
[20] D. Zhang, S. Li, X. Zhang, J. Zhan, P. Wang, Y . Zhou, and X. Qiu,
“Speechgpt: Empowering large language models with intrinsic cross-
modal conversational abilities,” arXiv preprint arXiv:2305.11000 ,
2023. 2
[21] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. De-
wan, M. Diab, X. Li, X. V . Lin et al. , “Opt: Open pre-trained trans-
former language models,” arXiv preprint arXiv:2205.01068 , 2022. 2
[22] M. Hassid, T. Remez, T. A. Nguyen, I. Gat, A. Conneau, F. Kreuk,
J. Copet, A. Defossez, G. Synnaeve, E. Dupoux et al. , “Textually pre-
trained speech language models,” arXiv preprint arXiv:2305.13009 ,
2023. 2
[23] T. Kudo and J. Richardson, “Sentencepiece: A simple and language in-dependent subword tokenizer and detokenizer for neural text process-
ing,” in Proc. EMNLP , 2018, pp. 66–71. 2
[24] R. Sennrich, B. Haddow, and A. Birch, “Neural machine translation of
rare words with subword units,” in Proc. ACL , 2016, pp. 1715–1725. 2
[25] T. Kudo, “Subword regularization: Improving neural network transla-
tion models with multiple subword candidates,” in Proc. ACL , 2018,
pp. 66–75. 2
[26] X. Chang, B. Yan, Y . Fujita, T. Maekaku, and S. Watanabe, “Explo-
ration of efficient end-to-end asr using discretized input from self-
supervised learning,” arXiv preprint arXiv:2305.18108 , 2023. 2, 4
[27] A. Vaswani, N. Shazeer, N. Parmar et al. , “Attention is all you need,”
inProc. NeurIPS , 2017. 2
[28] J. Kong, J. Kim, and J. Bae, “Hifi-gan: Generative adversarial networks
for efficient and high fidelity speech synthesis,” in Proc. NeurIPS ,
H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin,
Eds., vol. 33. Curran Associates, Inc., 2020, pp. 17 022–17 033.
[Online]. Available: https://proceedings.neurips.cc/paper_files/paper/
2020/file/c5d736809766d46260d816d8dbc9eb44-Paper.pdf 3
[29] D. Snyder, D. Garcia-Romero, G. Sell, D. Povey, and S. Khudan-
pur, “X-Vectors: Robust DNN embeddings for speaker recognition,”
inProc. ICASSP , 2018, pp. 5329–5333. 3
[30] T. A. Nguyen, M. de Seyssel, P. Rozé, M. Rivière, E. Kharitonov,
A. Baevski, E. Dunbar, and E. Dupoux, “The zero resource speech
benchmark 2021: Metrics and baselines for unsupervised spoken lan-
guage modeling,” in Proc. NeuRIPS Workshop on Self-Supervised
Learning for Speech and Audio Processing , 2020. 3
[31] C.-C. Lo, S.-W. Fu, W.-C. Huang et al. , “MOSNet: Deep learning
based objective assessment for voice conversion,” in Proc. Interspeech ,
2019, pp. 1541–1545. 3
[32] E. Cooper, W.-C. Huang, T. Toda et al. , “Generalization ability of mos
prediction networks,” in Proc. ICASSP , 2022, pp. 8442–8446. 3
[33] J. Kahn, M. Rivière, W. Zheng et al. , “Libri-light: A benchmark for asr
with limited or no supervision,” in Proc. ICASSP , 2020, pp. 7669–7673.
3
[34] V . Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Librispeech:
An asr corpus based on public domain audio books,” in Proc. ICASSP ,
2015, pp. 5206–5210. 3
[35] V . Pratap, Q. Xu, A. Sriram, G. Synnaeve, and R. Collobert, “Mls:
A large-scale multilingual dataset for speech research,” in Proc. Inter-
speech , 2020, pp. 2757–2761. 3
[36] H. Zen, R. Clark, R. J. Weiss, V . Dang, Y . Jia, Y . Wu, Y . Zhang,
and Z. Chen, “Libritts: A corpus derived from librispeech for
text-to-speech,” in Proc. Interspeech , 2019. [Online]. Available:
https://arxiv.org/abs/1904.02882 3
[37] C. Veaux, J. Yamagishi, K. MacDonald et al. , “Cstr vctk corpus: En-
glish multi-speaker corpus for cstr voice cloning toolkit,” University of
Edinburgh. The Centre for Speech Technology Research (CSTR) , vol. 6,
p. 15, 2017. 3
[38] D. Kingma, “Adam: a method for stochastic optimization,” in Proc.
ICLR , 2014. 3
[39] J. Kim, J. Kong, and J. Son, “Conditional variational autoencoder with
adversarial learning for end-to-end text-to-speech,” in International
Conference on Machine Learning . PMLR, 2021, pp. 5530–5540. 4
[40] K. Kim, F. Wu, Y . Peng, J. Pan, P. Sridhar, K. J. Han, and S. Watanabe,
“E-branchformer: Branchformer with enhanced merging for speech
recognition,” in Proc. SLT) , 2023, pp. 84–91. 4