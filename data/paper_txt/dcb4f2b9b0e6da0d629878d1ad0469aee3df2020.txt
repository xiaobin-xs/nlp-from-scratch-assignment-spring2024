Understanding Masked Autoencoders via Hierarchical Latent Variable Models
Lingjing Kong*1Martin Q. Ma∗1Guangyi Chen1,2
Eric P. Xing1,2Yuejie Chi1Louis-Philippe Morency†1Kun Zhang†1,2
1Carnegie Mellon University2Mohamed bin Zayed University of Artificial Intelligence
Abstract
Masked autoencoder (MAE), a simple and effective self-
supervised learning framework based on the reconstruction
of masked image regions, has recently achieved prominent
success in a variety of vision tasks. Despite the emergence
of intriguing empirical observations on MAE, a theoreti-
cally principled understanding is still lacking. In this work,
we formally characterize and justify existing empirical in-
sights and provide theoretical guarantees of MAE. We for-
mulate the underlying data-generating process as a hierar-
chical latent variable model and show that under reason-
able assumptions, MAE provably identifies a set of latent
variables in the hierarchical model, explaining why MAE
can extract high-level information from pixels. Further, we
show how key hyperparameters in MAE (the masking ratio
and the patch size) determine which true latent variables
to be recovered, therefore influencing the level of semantic
information in the representation. Specifically, extremely
large or small masking ratios inevitably lead to low-level
representations. Our theory offers coherent explanations
of existing empirical observations and provides insights for
potential empirical improvements and fundamental limita-
tions of the masking-reconstruction paradigm. We conduct
extensive experiments to validate our theoretical insights.
1. Introduction
Self-supervised learning (SSL) has achieved tremendous
success in learning transferable representations without la-
bels, showing strong results in a variety of downstream
tasks [12,14,16,23,49]. As a major SSL paradigm, masked
image modeling (MIM) [1–3,11,13,22,41,63,69] performs
the reconstruction of purposely masked image pixels as the
pretraining task. Among MIM methods, masked autoen-
coding (MAE) [22] has gained significant traction due to its
computational efficiency and state-of-the-art performance
in a wide range of downstream tasks.
Empirical observations from previous work reveal vari-
ous intriguing properties of MAE. In particular, aggressive
*Joint first author. †Joint senior author.
Figure 1. Masking-reconstruction under a hierarchical generating
process. In a hierarchical data-generating process, high-level latent vari-
ables (e.g., z1) represent high-level information such as semantics, and
low-level latent variables (e.g., [z2,z3,z4]) represent low-level informa-
tion such as texture. We show that through proper masking, MAE learns
to recover high-level latent variables with identifiability guarantees.
masking has been shown critical to downstream task per-
formances [22,28, 61,63]. It is conjectured that such mask-
ing forces the model to learn meaningful high-level seman-
tic understanding of the objects and scenes rather than the
low-level information such as texture. However, it remains
largely unclear whether such intuitions are sound in princi-
ple. Theoretically verifying and characterizing these empir-
ical insights would not only grant a certificate to the current
approaches but would also offer theoretical insights for al-
gorithmic advancements.
In this work, we establish a principled yet intuitive
framework for understanding MAE and providing identifia-
bility guarantees. Concretely, we first formulate the under-
lying data-generating process as a hierarchical latent vari-
able model (Figure 1), with high-level variables correspond-
ing to abstract and semantic information like classes, and
low-level variables corresponding to elaborate and granular
information like texture. Such latent variable models have
been studied in causal discovery [29, 62]. In [27, 50], it is
hypothesized that complex data, such as images, follow a
hierarchical latent structure.
Stemming from this formulation, we show that under
reasonable assumptions, MAE can recover a subset of thearXiv:2306.04898v1  [cs.LG]  8 Jun 2023
true latent variables within the hierarchy, where the levels
of the learned latent variables are explicitly determined by
how masking is performed. Our theoretical framework not
only unifies existing empirical observations coherently but
also gives rise to insights for potential empirical improve-
ments and fundamental limitations of MAE. Our theory im-
proves the existing nonlinear identifiability results [45, 58]
and can be of independent interest.
Empirically, we deduce several insights from our theo-
retical results and verify them with experiments. Unlike
common belief, MAE trained with extremely high masking
ratios (e.g., 90%) captures low-level information, similar to
models trained with extremely low ratios (e.g., 10%). Our
results suggest that learning high-level semantic informa-
tion is only possible in the non-extreme masking regime.
We also discuss masking designs that can potentially im-
prove current empirical performance.
Contributions. We highlight the following contributions:
• We formulate the underlying data-generating process
as a hierarchical latent variable model. Under such
a formulation, we provide a theoretical guarantee for
MAE by showing that it can recover true latent vari-
ables in the hierarchical model.
• Based on our theoretical results, we establish the con-
nection between masking hyperparameters (i.e., mask-
ing ratios and patch sizes) and the learned representa-
tion and discuss potential improvements and inherent
limitations of MAE.
• We validate our theoretical insights with extensive ex-
perimental results. We illustrate how the semantic
level of the learned representation varies with the ag-
gressiveness of the masking strategy. Interestingly,
representations learned under overly aggressive mask-
ing (e.g.,ß 90% masking ratio) exhibit similar proper-
ties to their counterparts learned with overly conserva-
tive masking (e.g., 10% masking ratio).
2. Theoretical Understanding
2.1. A Hierarchical Data-generating Process
Images, despite their high dimensionality, are well struc-
tured – there is a multitude of statistical dependencies
among pixels determined by their relative distances and vi-
sual semantics. For instance, pixels in close proximity are
often highly dependent, whereas pixels far apart typically
share less information. There has been a plethora of work
adopting this intuition for vision tasks such as image gen-
eration [47, 55, 67]. Similar insights are also addressed in
attempts to learn a part-whole image representation [27,50].z1z2
z3z4z5
z6z7z8 z9z10
x1x2x3x4x5x6x7x8x9x10x11
Figure 2. A hierarchical data-generating process. zrepresents
the latent variables and xstands for the observable variables (i.e.
image pixels). The hierarchical model is generic and is capable of
modeling arbitrary DAGs in the latent space.
In this work, we formulate such an underlying struc-
ture of images with a hierarchical data-generating pro-
cess [1, 29, 62] (Figure 2). Under this formulation, we re-
veal the underpinning principle of MAE and provide iden-
tifiability guarantees. In particular, we show that through
masking-reconstruction, MAE learns the long-range statis-
tical dependencies within the image, which renders it capa-
ble of extracting high-level semantic representations.
Formally, the generating process is defined with a graph
structure G:= (V,E)where Eis the set of all directed
edges and V:= (X,Z)comprises all observable variables
X:={x1, . . . ,xm}(i.e., all pixels) and all latent variables
Z:={z1, . . . ,zn}. Each variable xiorzjrepresents a
multidimensional vector.1The hierarchical latent structure
Gfulfills the following assumption:
Assumption 1. (Data-generating process): There is no di-
rect edge between any two observables: ∀xi,xj∈X,
(xi,xj)/∈Eand(xj,xi)/∈E. Each variable is generated
by its parents in a directed acyclic graph (DAG) according
to:
zi=gzi(Pa(zi),εi),
xj=gxj(Pa(xj),εj),(1)
where gziandgxjare invertible functions, εidenotes ex-
ogenous random variables, and Pa (·)denotes the parents
of a certain node.
The invertible data-generating-module assumption ( gi
andgjbeing invertible) is adopted from prior work identify-
ing latent variables in deep generative models [18, 58]. We
make the following remarks on the hierarchical generating
process. First, we note that we impose minimal constraints
on the graph structure among the latent variables (i.e., the
connectivity among latent variables z); therefore, the hierar-
chical model class is generic and encompasses all possible
DAG structures over latent variables (Figure 2). Next, we
interpret the latent variables zas information related to se-
mantic/content information, such as the shape and contour
1In high-dimensional data like images, there is a larger degree of infor-
mation redundancy, e.g., neighboring pixels. Thus, it is sensible to lump
one-dimensional variables into vectors.
in the image, whereas the exogenous variables εinjected in
each layer represent nuanced information, such as the tex-
ture and contrast of the image. Each structural function gi
mixes the two sources of information and generates a more
low-level variable until pixels x. Lastly, for the upcoming
theoretical results, as long as the data-generating process
conforms to the hierarchical graph assumption, our theory
holds, and the insights do not rely on the knowledge of a
specific graph structure.
2.2. Masked Autoencoders
As a canonical method of masking-reconstruction learn-
ing, MAE [22] randomly masks a subset of pixel patches in
the original image and then reconstructs the masked patches
from the encoded representation of the visible part. More
formally, we formulate the MAE training as follows.
Mask sampling : random masks mare sampled from a
distribution pmwhich is parameterized by the masking ratio
r(i.e., the ratio between the number of masked pixels and
the number of all pixels) and patch size s(i.e., the size of
the minimal masking unit).
MAE encoding :Emc(xmc)maps the unmasked part
xmcto a latent representation ˆc2, where mcdenotes the
complement of the mask index set mand is passed to the
encoder as positional embeddings to indicate the positions
of the visible patches.
MAE decoding :Dm(ˆc,ˆsm)reconstructs the masked
image xmfrom the estimated latent variable ˆc(i.e., the en-
coder output), and the auxiliary information ˆsmembodying
positional embeddings and [MASK] token which are fed
to the decoder in MAE. Although ˆsmis deterministic in
MAE implementation, we view it as a random variable in
our analysis.
With the notation above, the MAE training objective can
be expressed as follows:
L(E, D ) :=Em,x,ˆsm
∥Dm(Emc(xmc),ˆsm)−xm∥2
.(2)
2.3. Identifiability Theory
Building upon the formalization above, we show in The-
orem 1 that each random mask mwould induce a specific
(sub)set of latent variables that fully captures the statistical
dependency between the masked part and the visible part.
We denote this relationship as c⊂Zwhere cis the subset
of the latent variable set Z.
Theorem 1. (Locating the shared information c): In a hier-
archical latent variable structure G, for each specific mask
m, there exists a corresponding minimal set of latent vari-
ables csuch that the generating process of xcan be ex-
pressed as in Figure 3 where the following conditions are
satisfied:
2To avoid notation cluttering, we adopt ˆ·to distinguish the estimated
variables from the true ones in the generating process.xm xmcsm smc c
Figure 3. Information sharing latent models . Here, xmand
xmcdenote the masked part and the visible part of the image x,
respectively. cstands for the maximally shared information be-
tween xmandxmc.smandsmcrefer to the information specific
toxmandxmcrespectively. The dashed line indicates the poten-
tial existence of statistical dependence.
1.xm=gxm(c,sm)andxmc=gxmc(c,smc)where
bothgxmandgxmcare invertible;
2.sm⊥ ⊥(c,smc);
3.cis minimal: ∀c′⊂Zsuch that dim (c′)<dim(c),c′
cannot satisfy the two conditions above.
Suchcand the corresponding smare unique and can be
located from the hierarchical structure by executing Algo-
rithm 1. Furthermore, smccan be found through Algo-
rithm 2.
The proof, Algorithm 1, and Algorithm 2 can be found
in Appendix A. We note that although the minimal cand
its corresponding smare unique for a given mask m, there
is no unique smcin general. Algorithm 2 returns one such
instance.
Theorem 1 states that for each mask m, there exists
a corresponding cthat represents all the information con-
tained in the visible part xmcthat is conducive to recon-
structing the masked part xm. Algorithm 1 can locate such
cin the hierarchy and directly characterizes the impact of
masking on the property of c.
Next, in Theorem 2, we show that MAE learning objec-
tive (Equation 2) estimates cspecified in Theorem 1, and
MAE attains a form of identifiability of c. We first lay out
the assumptions:
Assumption 2. (MAE model): For any mask m, the MAE
decoder Dm(ˆc,ˆsm)has a non-singular Jacobian matrix
almost anywhere, and there exists an invertible function
˜gmc(·)such that MAE encoder Emc(·) = [˜ g−1
mc(·)]1:dc
where [·]1:dcdenotes the dimensions corresponding to c.
Moreover, (Dm,˜gmc)forms an invertible mapping between
(ˆc,ˆsm,ˆsmc)and(xm,xmc)
Next, we show MAE identifies the shared information c:
Theorem 2. (Identifiability of c): For each mask m, given
the dimensions (dc, dsm)the encoder function Emc(·)re-
covers all information of clocated in Theorem 1, i.e., there
exists a one-to-one mapping h, s.t.,h(c) =ˆc.
In the following, we discuss our assumptions and results.
The proof can be found in Appendix B.
Assumption interpretation. Assumption 1 follows prior
work identifying latent variables in deep generative mod-
els [18, 58] to ensure that latent variables are recoverable
from pixels. Assumption 2 requires the MAE encoder Emc
to be part of an invertible function output – this is mild and
allows the encoder to be more flexible than invertible func-
tions. The decoder Dm(ˆc,ˆsm)is assumed to be locally in-
vertible in ˆcalmost surely, allowing for a broader class than
invertible functions, e.g., nondegenerate polynomials. The
joint invertibility of (Dm,˜gmc)ensures no information loss
during the estimation process.
How does MAE work? Theorem 2 states that the MAE
objective (Equation 2) essentially serves to estimate the
shared variable cand is able to restore all information in
c. Therefore, the efficacy of MAE stems from its ability to
extract high-level semantic representations from low-level
features like image pixels. Moreover, our theory indicates
the possibility of fully identifying a latent hierarchical struc-
ture via properly designed self-supervised objectives, open-
ing up research avenues for future work.
Takeaway :MAE provably recovershigh-level represen-
tations from low-level features likepixels.
How does masking influence the learned representa-
tion? Theorem 1 establishes a direct connection between
the mask mand the shared information c, which is further
connected to the MAE estimate ˆcin Theorem 2. We can ob-
serve that conservative masking with overly small masking
ratios and masking patch sizes inevitably leads to low-level
latent variables. To see this, in Figure 4a, the mask is not
large enough to cover all observable descendants of a de-
sirable high-level variable z1, thus following Algorithm 1
a low-level variable z3will mix in ˆc, preventing the model
from learning z1. This insight highlights the necessity of
nontrivial masking ratios and patch sizes and resonates with
the empirical observations in [22, 28, 63].
Surprisingly, the above reasoning can be applied to the
case with extremely aggressive masking: in Figure 4b low-
level latent variables z6will be learned by MAE when the
visible part is too small to cover all observable descendants
of a desirable high-level variable z2. Thus, the learned rep-
resentation does not become monotonically more high-level
with increasing masking aggressiveness – overly aggressive
masking also gives rise to low-level representations. This
insight echoes the empirical finding in [61, 63] where the
extremely large masking degrades the performance of high-
level downstream tasks like classification [63] but yields
relatively low-level representations like the object loca-z1z2
z3z4z5z6
x1x2x3x4x5x6
(a)Conservative maskz1z2
z3z4z5z6
x1x2x3x4x5x6
(b)Aggressive maskz1z2
z3z4z5z6
x1x2x3x4x5x6
(c)Ideal mask
Figure 4. The impact of masking on the learned representation.
We label the masked pixels with x. We locate the MAE learned
latent variables with Algorithm 1 and label them with blue. We
can observe that extremely low (left) and high (middle) masking
intensities lead to low-level representations, whereas the desirable
masking intensity that yields a high-level representation lies in the
intermediate masking aggressiveness.
tions/scales in the image [61]. In Section 3, we present
empirical evidence to verify our theoretical insights.
Takeaway :(1)MAE underdifferentmask ingintensi-
tieslearns representations ofdifferentabstrac tion levels;
(2)Learn inghigh-level representations isvery hard with
extreme mask ing.
Is current MAE optimal for representation learning?
As reflected in the discussion above, although MAE offers
the flexibility of tuning the masking scheme to learn repre-
sentations of various levels, it is inherently challenging to
learn high-level representations by random masking with-
out prior knowledge of the latent structure. In contrast, con-
trastive learning [5,9,10,12,14,23,64] actively leverages the
prior knowledge encoded in data augmentations to extract
the augmentation-invariant latent variables [58] which cor-
respond to the high-level latent variables in our hierarchical
model. Our theory suggests an explanation for why rep-
resentations learned by contrastive learning are superior to
those of MAE on high-level tasks like linear-probing clas-
sification.
Takeaway :Learn inghigh-level representations canbe
challengingforrandom mask ing.
3. Experiments
We conduct five sets of experiments and then provide in-
sights into possible empirical improvements over MAE. We
investigate the following question: how does the masking
aggressiveness influence the representation? To this end,
we pretrain MAE using different masking ratios and mak-
ing patch sizes, and then conduct the following evaluations:
1) measuring structure-level and pixel-level similarities be-
tween the reconstructed and the original images; 2) visual-
izing self-attentions to understand what is learned; 3) per-
forming linear probing on ImageNet-1K (IN1K) and dif-
ferent ImageNet variants; 4) measuring the shape bias [19]
which estimates how much a network leverages high-level
shape information over low-level texture information; and
Figure 5. Reconstruction evaluation using the validation set without masking, based on two structural-level similarity metrics (SSIM and
FSIM) and two pixel-level metrics (PSNR and MSE). We plot negative MSE for easier visualization. Higher SSIM and FSIM indicate
high-level information is better captured, while higher PSNR and negative MSE indicates better low-level reconstruction.
5) transfer learning on object detection and segmentation.
Details of experiments can be found in Appendix.
Pretraining overview. We conduct pretraining on IN1K
using the MAE pipeline [22], with ViT-Base as the back-
bone of our study. We conduct two sets of pretraining: 1)
fixing patch size at 16and varying the masking ratios from
{0.1,0.25,0.5,0.75,0.9}. Larger masking ratios suggest
larger portions of pixels being masked, i.e., 0.9suggests
90% of pixels being randomly masked for the encoder. 2)
Fix the masking ratio at 0.75and vary the patch size from
{8,16,32}. To decouple the patch size for masking im-
ages and the patch size hyperparameter in the Vision Trans-
former, we adopt the implementation from [28]. The patch
size studied in this paper refers to the minimal masking unit
size, and the hyperparameter of the ViT patch size remains
fixed at 8.
3.1. Reconstructing High-level or Low-level Repre-
sentations
Setup. We begin our study by evaluating the high-level
structural and low-level pixel-wise similarities between the
reconstructed images from MAE and the original inputs.
We choose two metrics for high-level similarities and two
metrics for low-level similarities. If the structural similari-
ties are high, MAE captures more perceivable structural se-
mantics from the input. The two high-level similarities are
structural similarity index measure [60] ( SSIM ) and fea-
ture similarity index measure [65] ( FSIM ). Both metrics
consider the change of perceptions in structural informa-
tion [33]. SSIM considers the normalized mean value of the
structural similarity between the original and reconstructed
images, and FSIM considers the normalized mean value of
the feature similarity between the two images. A higher
SSIM or a higher FSIM suggests a better reconstruction
of high-level information (structural or feature-wise). On
the other hand, if the pixel-level similarity between recon-structed images and the original input is high, then MAE is
deemed to capture the low-level information about the in-
put better. The two low-level metrics are the mean squared
error ( MSE ), which is the squared differences between the
original and reconstructed images in the pixel space, and
the peak signal-to-noise ratio ( PSNR ), which measures the
ratio between the power of the maximum possible pixel
value and the power of corruption noise. A lower MSE or
ahigher PSNR suggests a better reconstruction at the pixel
level. Note that a very low MSE or a very high PSNR may
also suggest that the model captures high-level information
well. All four metrics are full reference, meaning the as-
sessment is based on comparing original and reconstructed
images rather than the reconstructed output. We introduce
the high-level and low-level metrics below and perform the
reconstructions on the IN1K evaluation set. The full details
and comparisons of the four metrics can be found in [51].
Evaluation of image reconstructions. We include the re-
sults in Figure 5. We plot the negative of the MSE to show
a consistent trend with PSNR, so higher means better low-
level reconstruction. From the first row, varying masking
ratios from 0.1to0.75, higher masking ratios produce re-
constructions with higher structural information similari-
ties with the original image (higher SSIM and FSIM), but
the model trained with the extremely high ratio 0.9 cap-
tures more low-level information (higher PSNR and higher
negative MSE). On the other hand, lower masking ratios
tend to reconstruct images that capture low-level informa-
tion better. From the second row, larger patch sizes pro-
duce image reconstructions that capture high-level similar-
ities better, while smaller patch sizes have low-level met-
rics. The empirical observations validate our insight from
Section 2.3: higher masking ratios and patch sizes capture
high-level structural information better , but extreme mask-
ing ratios (both low and high) capture less high-level and
more low-level information.
Figure 6. Self-attention of the [CLS] tokens averaged across the
heads of the last layer in MAE.
Figure 7. Self-attention of an object-related token . Chosen to-
kens are shown in red squares: dog nose, cat chin, bee abdomen,
chicken head, and football center, respectively.
3.2. Attention Analysis
In this section, we measure the property of the learned
representations of MAE by probing the attention heads. We
would like to understand visually how masking ratios and
patch sizes influence MAE’s capacity to capture object-
centric semantics. We provide two types of visualization:
self-attention on the [CLS] token and self-attention on an
object-related token. [CLS] has been considered a com-
pact token to represent the whole image for downstream
tasks, although recent work [22] suggests that the average
pooling of all tokens may achieve slightly better results.
Therefore, we also provide an analysis of object-related to-
kens to evaluate if MAE can contextualize object informa-tion across tokens.
We plot examples of self-attention of the [CLS] token in
Figure 6 and self-attention of non-CLS tokens related to the
object in Figure 7. From the visualizations, as the masking
ratio increases from 10% to90%, the model is increasingly
more able to grasp succinct information about the holistic
objects rather than only focusing on the regions around the
chosen token. However, extreme ratio 0.9contains more
low-level information and background information and can-
not capture most of the remaining tokens related to objects
(e.g., the dog, cat, and bee images in Figure 7). Extremely
low masking ratios such as 0.1capture both object-related
and background tokens. Similarly, extreme masking ratios
contextualize over other object-related tokens worse than
intermediate masking ratios. We include the visualizations
for patch sizes in Appendix. We observe that models trained
with larger patch sizes better capture high-level informa-
tion, but extreme patch size hurts, which validates our theo-
retical insight that moderate masking ratios and patch sizes
are critical for MAE to learn succinct and comprehensive
object information.
3.3. Representation Linear Separability
T-SNE embedding visualizations. To gain a visual un-
derstanding of how masking ratios and patch sizes influ-
ence the representation structure, we visualize T-SNE [57]
embeddings of different models. We randomly select ten
classes from ImageNet. The results are shown in Figure 8.
From 0.1to0.75, a larger masking ratio consistently pro-
duces a more linearly separable representation, while the
linear separabilities of representations with masking ratios
0.75and0.9are visually similar. For different patch sizes,
the embeddings are more separated as the patch sizes grow.
Non-extreme masking ratios and larger patch sizes generate
more linearly separable embeddings.
Linear probing on IN1K. We use linear probing to test
how linearly separable the features are in the learned MAE
representation. We show the linear probing results in Table
1 in row 1N1K. For different masking ratios, similar to the
observation in [22], the accuracy increases steadily until the
masking ratio reaches the sweet point of 0.75. An extremely
large masking ratio (0.9) hurts performance. For different
patch sizes, which are not shown in [22], we observe that
the accuracy increases first from 8 to 16, then decreases sig-
nificantly when the patch size is 32. From the results, higher
masking ratios and larger patch sizes perform better at lin-
ear probing than lower masking ratios, but extreme masking
hurts linear probing.
Robustness evaluation on ImageNet variants. We eval-
uate the robustness of the MAE models on different variants
of ImageNet validation datasets, or object detection datasets
Figure 8. T-SNE embeddings of different MAE models under varied masking ratios and patch sizes. We fix the patch size at 16to vary the
masking ratios and fix the masking ratio at 0.75to change the patch sizes. Each color represents one ImageNet class.
mask ratio patch size IN1K IN-v2 OJN IN-R IN-A IN-S
0.1 16 47.45 34.72 9.42 14.63 2.00 7.25
0.25 16 53.58 40.34 11.54 18.68 2.49 10.27
0.5 16 60.07 46.71 13.94 22.44 2.89 12.58
0.75 16 67.41 54.23 18.24 25.20 3.76 15.51
0.9 16 62.97 49.52 15.87 19.11 2.76 10.46
0.75 8 62.57 49.17 13.44 19.42 3.73 10.73
0.75 16 68.96 55.94 13.73 24.23 6.29 18.81
0.75 32 73.31 61.35 19.03 27.84 12.69 28.30
Table 1. Accuracy ( %) of linear probing and robustness evalu-
ation on ImageNet variants and ObjectNet. We linear-probe MAE
via supervised training on IN1K, and then perform inference on
IN1K as well as other evaluation sets.
that share similar class information with ImageNet-1K:
ImageNet-v2 (INV2) [52], ObjectNet (OJN) [4], ImageNet-
Adversarial (IN-A) [25], ImageNet-Rendition [4], and
ImageNet-Sketch (IN-S) [59]. These datasets share similar
semantics and labels with ImageNet but are under different
data distributions. The MAE models are first trained in a su-
pervised fashion on IN1K for linear probing, and inference
is run on the evaluation sets without any training. Table 1
shows for all evaluation datasets, a reasonably large mask-
ing ratio (i.e., 0.75) achieves better robustness than smaller
(i.e.,0.25) masking ratios, although extremely large ( 0.9) or
small ( 0.1) masking ratios hurt the performance. For patch
sizes, larger patch sizes yield better robustness evaluations
on IN-v2, OJN, IN-R, and IN-S. Non-extreme masking ra-
tios and large patch sizes have stronger robustness perfor-
mances than extreme masking ratios or patch sizes.
3.4. Shape Bias
Texture vs. shape bias. Next, we analyze to what ex-
tent different MAE models rely on high-level vs. low-level
information. We follow the analysis in [19], where the au-
thors study whether a model leverages more low-level tex-
tures than high-level shapes for classification. As shown
in Table 2, intermediate masking ratios (i.e., 0.25,0.5, and
0.75) show a high level of shape bias, suggesting that the
corresponding models exploit more high-level shape infor-
mation. In contrast, extreme masking ratios (i.e., 0.1and
0.9) leverage more low-level textures. This suggests that
extreme masking schemes make it more difficult to capture
high-level shapes for MAE.
3.5. Transfer Learning
Next, we evaluate the quality of MAE models on differ-
ent downstream tasks. Specifically, we look at object de-mask ratio 0.1 0.25 0.5 0.75 0.9
shape bias 0.1352 0.2545 0.2458 0.2563 0.2014
Table 2. Shape bias [19] measurement, a higher metric indicates
that the model classifies images relying on the high-level shape
feature rather than the low-level texture feature.
mask ratio mask size APboxAPmask
0.1 16 30.47 28.24
0.25 16 32.38 29.95
0.5 16 34.87 32.11
0.75 16 39.72 36.35
0.9 16 37.17 34.35
Table 3. COCO object detection and segmentation using a ViT
Mask R-CNN baseline.
tection and segmentation on the COCO dataset [43], which
requires a strong semantic understanding of the scenes.
We finetune Mask R-CNN [24] end-to-end using MAE-
pretrained ViT weights. Following the practice in [22], we
adapt the ViT backbone to make it compatible with FPN
[42]. In Table 3, we report box AP for object detection and
mask AP for instance segmentation. We reduce the num-
ber of epochs to 45due to computational constraints. We
observe that the 0.75masking ratio yields the best detec-
tion and segmentation average precision, suggesting that the
masking ratio 0.75generates representation with the best
semantic understanding. The extremely high masking ratio
of0.9and a low masking ratio of 0.1hurt the performance.
Results of different patch size experiments are included in
Appendix. The results suggest that higher, but not extreme,
masking ratios generate the best representation of object
detection and segmentation tasks.
3.6. Potential Algorithmic Improvements
Lastly, we discuss empirical suggestions based on our
results that could benefit the performance of MAE.
First, as discussed in Section 2, when reconstructing the
masked pixels near the boundary between the masked and
unmasked regions, the model uses nearby visible pixels to
interpolate, therefore capturing low-level pixel information.
If high-level representation is desired for downstream tasks,
the boundary pixels may be ignored when calculating the
objective function.
Next, in light of the limitation of random masking in Sec-
tion 2, one may leverage the latent structure of the underly-
ing data-generating process for masking designs, which can
serve as a more principled approach than recent work that
exploits auxiliary information for masking [34, 40, 41, 53].
To this end, one may take advantage of the recent devel-
opment of causal discovery [29, 62] to identify the latent
structure.
Lastly, if low-level information is preferable for down-
stream tasks, an extremely high masking ratio can retain
such information and is more computationally efficient than
its low masking ratio counterpart.
4. Related work
4.1. Masked Autoencoders
Masked image modeling (MIM) [1–3, 11, 13, 22, 41, 63,
69] has been gaining momentum recently due to their sota-
of-the-art performances over many downstream tasks. The
pretraining objective is simple in its basic form: the model
is tasked to predict the masked-out image pixels with the
information of the unmasked part. Despite the simplic-
ity of the task, many intriguing properties have been ob-
served on MIM that escape rigorous analysis. For instance,
small masking ratios and masking patch sizes are empiri-
cally shown detrimental to downstream tasks like classifi-
cation [22, 28]. It is hypothesized that aggressive masking
forces to model to leverage more global information, rather
than local interpolation [22]. However, whether such in-
tuition is theoretically justifiable remains elusive. In this
work, we provide theoretical verification of such intuitions
and further derive insights into MAE’s empirical behavior.
4.2. Theoretical Understanding of MAE
Despite the prominent success of MAE, only a limited
number of papers are dedicated to understanding its un-
derlying mechanism in a principled manner [8, 39, 48, 66].
Lee et al. [39] establish the connection between the inpaint-
ing pretraining task and downstream tasks by assuming that
the downstream task target captures the statistical depen-
dency between the visible part and the masked part in the
inpainting. Under this assumption, they show that the sam-
pling complexity of the downstream task can be largely re-
duced by pretraining. Cao et al. [8] inquire into the inter-
actions between the transformer architecture and the MAE
representation, highlighting the critical role of the attention
mechanism in the success of MAE. Pan et al. [48] make a
multi-view assumption on the samples, showing that MAE
can extract class-relevant semantics with shallow convolu-
tional models. Zhang et al. [66] study masking through the
data-augmentation perspective and employ the augmenta-
tion graph [21] to illustrate the impact of masking on down-
stream task performance. In contrast, our work employs the
hierarchical latent variable model, which lets us directly ex-
amine the relationship between the masking operation and
the learned representations. Also, our theoretical guarantee
is on the statistical identifiability of the true data-generatingprocess rather than the statistical/optimization complexities
as in most prior work.
4.3. Identifiability Guarantees for Nonlinear
Latent-variable Models
In unsupervised learning, identifiability means latent
variables involved in the underlying data-generating process
can be estimated from observational data. This is critical
to tasks like feature disentanglement [7, 26, 30, 35, 62] in
the image generation community. However, principled dis-
entanglement in the non-linear regime is challenging and
even proved impossible without additional assumptions on
the data-generating process [44]. Recent advances in inde-
pendent component analysis (ICA) [6, 15, 31] obtain iden-
tifiability in the non-linear regime by imposing additional
constraints on either the latent variable distribution or the
function class variables [20, 32, 36–38, 45, 54, 58, 68]. Most
relevant to ours are the identifiability theories in [45, 58]
in which similar latent causal models (Figure 3) are stud-
ied. Specifically, our model allows the generating func-
tionsgm̸=gmcto be distinct (cf. identical functions as-
sumed in [58]) and statistical dependence between cand
smc(cf. independence assumed in [46]). Additionally, both
works [46, 58] focus on contrastive learning with data aug-
mentation, while our subject is MAE.
5. Conclusion
In this work, we formulate the data-generating process
as a hierarchical latent variable model and provide guaran-
tees that MAE can identify the true variables in such a hi-
erarchical latent model. We then show how different mask-
ing ratios and patch sizes determine the set of true latent
variables to be recovered, which influences the represen-
tation abstractions learned in MAE. Empirically, we show
that non-extreme masking ratios or patch sizes often cap-
ture succinct and robust high-level information, while ex-
treme masking ratios capture more low-level information.
Acknowledgement We thank the Google TRC program for
the TPU Research Cloud support, Ronghang Hu and Xinlei Chen
for the MAE TPU code, Biwei Huang for technical discussions,
Tao Lin for feedback on the manuscript, and anonymous reviewers
for valuable feedback. The work of LK and YC is supported in part
by NSF under the grants CCF-1901199 and DMS-2134080, and
by ONR under the grant N00014-19-1-2404. The work of MM and
LP is partially supported by BMW, National Science Foundation
awards 1722822 and 1750439, and National Institutes of Health
awards R01MH125740, R01MH096951, R21MH130767 and
R01MH132225. This project is also partially supported by the Na-
tional Institutes of Health (NIH) under Contract R01HL159805,
by the NSF-Convergence Accelerator Track-D award 2134901, by
a grant from Apple Inc., a grant from KDDI Research Inc, and gen-
erous gifts from Salesforce Inc., Microsoft Research, and Amazon
Research.
References
[1] Animashree Anandkumar, Daniel Hsu, Adel Javanmard, and
Sham Kakade. Learning linear bayesian networks with latent
variables. In International Conference on Machine Learning ,
pages 249–257. PMLR, 2013. 1, 2, 8
[2] Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Ji-
atao Gu, and Michael Auli. Data2vec: A general framework
for self-supervised learning in speech, vision and language.
arXiv preprint arXiv:2202.03555 , 2022. 1, 8
[3] Hangbo Bao, Li Dong, and Furu Wei. Beit: Bert pre-training
of image transformers. arXiv preprint arXiv:2106.08254 ,
2021. 1, 8
[4] Andrei Barbu, David Mayo, Julian Alverio, William Luo,
Christopher Wang, Dan Gutfreund, Josh Tenenbaum, and
Boris Katz. Objectnet: A large-scale bias-controlled dataset
for pushing the limits of object recognition models. Ad-
vances in neural information processing systems , 32, 2019.
7, 15
[5] Adrien Bardes, Jean Ponce, and Yann LeCun. Vi-
creg: Variance-invariance-covariance regularization for self-
supervised learning. arXiv preprint arXiv:2105.04906 , 2021.
4
[6] Anthony J Bell and Terrence J Sejnowski. An information-
maximization approach to blind separation and blind decon-
volution. Neural computation , 7(6):1129–1159, 1995. 8
[7] Christopher P Burgess, Irina Higgins, Arka Pal, Loic
Matthey, Nick Watters, Guillaume Desjardins, and Alexan-
der Lerchner. Understanding disentangling in beta vae. arXiv
preprint arXiv:1804.03599 , 2018. 8
[8] Shuhao Cao, Peng Xu, and David A. Clifton. How to under-
stand masked autoencoders, 2022. 8
[9] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Pi-
otr Bojanowski, and Armand Joulin. Unsupervised learning
of visual features by contrasting cluster assignments. ArXiv ,
abs/2006.09882, 2020. 4
[10] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv ´e J´egou,
Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-
ing properties in self-supervised vision transformers. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 9650–9660, 2021. 4, 15
[11] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Hee-
woo Jun, David Luan, and Ilya Sutskever. Generative pre-
training from pixels. In Hal Daum ´e III and Aarti Singh,
editors, Proceedings of the 37th International Conference
on Machine Learning , volume 119 of Proceedings of Ma-
chine Learning Research , pages 1691–1703. PMLR, 13–18
Jul 2020. 1, 8
[12] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-
offrey Hinton. A simple framework for contrastive learning
of visual representations. In International conference on ma-
chine learning , pages 1597–1607. PMLR, 2020. 1, 4
[13] Xiaokang Chen, Mingyu Ding, Xiaodi Wang, Ying Xin,
Shentong Mo, Yunhao Wang, Shumin Han, Ping Luo,
Gang Zeng, and Jingdong Wang. Context autoencoder
for self-supervised representation learning. arXiv preprint
arXiv:2202.03026 , 2022. 1, 8[14] Xinlei Chen, Saining Xie, and Kaiming He. An empiri-
cal study of training self-supervised vision transformers. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 9640–9649, 2021. 1, 4
[15] Pierre Comon. Independent component analysis, a new con-
cept? Signal processing , 36(3):287–314, 1994. 8
[16] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova. Bert: Pre-training of deep bidirectional
transformers for language understanding. arXiv preprint
arXiv:1810.04805 , 2018. 1
[17] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al. An image is worth 16x16 words: Trans-
formers for image recognition at scale. arXiv preprint
arXiv:2010.11929 , 2020. 14
[18] Locatello et al. Weakly-supervised disentanglement without
compromises. 2, 4
[19] Robert Geirhos, Patricia Rubisch, Claudio Michaelis,
Matthias Bethge, Felix A Wichmann, and Wieland Brendel.
Imagenet-trained cnns are biased towards texture; increasing
shape bias improves accuracy and robustness. arXiv preprint
arXiv:1811.12231 , 2018. 4, 7, 15
[20] Hermanni H ¨alv¨a and Aapo Hyvarinen. Hidden markov non-
linear ica: Unsupervised learning from nonstationary time
series. In Conference on Uncertainty in Artificial Intelli-
gence , pages 939–948. PMLR, 2020. 8
[21] Jeff Z HaoChen, Colin Wei, Adrien Gaidon, and Tengyu Ma.
Provable guarantees for self-supervised deep learning with
spectral contrastive loss. Advances in Neural Information
Processing Systems , 34:5000–5011, 2021. 8
[22] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr
Doll´ar, and Ross Girshick. Masked autoencoders are scalable
vision learners, 2021. 1, 3, 4, 5, 6, 7, 8, 14, 16
[23] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross
Girshick. Momentum contrast for unsupervised visual rep-
resentation learning. In Proceedings of the IEEE/CVF con-
ference on computer vision and pattern recognition , pages
9729–9738, 2020. 1, 4
[24] Kaiming He, Georgia Gkioxari, Piotr Doll ´ar, and Ross Gir-
shick. Mask r-cnn. In Proceedings of the IEEE international
conference on computer vision , pages 2961–2969, 2017. 7,
16
[25] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Stein-
hardt, and Dawn Song. Natural adversarial examples. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 15262–15271, 2021. 7,
15
[26] Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess,
Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and
Alexander Lerchner. beta-V AE: Learning basic visual con-
cepts with a constrained variational framework. In Interna-
tional Conference on Learning Representations , 2017. 8
[27] Geoffrey Hinton. How to represent part-whole hierarchies in
a neural network. arXiv preprint arXiv:2102.12627 , 2021. 1,
2
[28] Ronghang Hu, Shoubhik Debnath, Saining Xie, and Xinlei
Chen. Exploring long-sequence masked autoencoders. arXiv
preprint arXiv:2210.07224 , 2022. 1, 4, 5, 8, 14
[29] Biwei Huang, Charles Jia Han Low, Feng Xie, Clark
Glymour, and Kun Zhang. Latent hierarchical causal
structure discovery with rank constraints. arXiv preprint
arXiv:2210.01798 , 2022. 1, 2, 8
[30] Xun Huang, Ming-Yu Liu, Serge Belongie, and Jan Kautz.
Multimodal unsupervised image-to-image translation. In
Proceedings of the European conference on computer vision
(ECCV) , pages 172–189, 2018. 8
[31] A. Hyv ¨arinen, J. Karhunen, and E. Oja. Independent Com-
ponent Analysis . John Wiley & Sons, Inc, 2001. 8
[32] Aapo Hyvarinen, Hiroaki Sasaki, and Richard Turner. Non-
linear ica using auxiliary variables and generalized con-
trastive learning. In The 22nd International Conference on
Artificial Intelligence and Statistics , pages 859–868. PMLR,
2019. 8
[33] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual
losses for real-time style transfer and super-resolution. In
European conference on computer vision , pages 694–711.
Springer, 2016. 5
[34] Ioannis Kakogeorgiou, Spyros Gidaris, Bill Psomas, Yan-
nis Avrithis, Andrei Bursuc, Konstantinos Karantzalos, and
Nikos Komodakis. What to hide from your students:
Attention-guided masked image modeling. arXiv preprint
arXiv:2203.12719 , 2022. 8
[35] Tero Karras, Samuli Laine, and Timo Aila. A style-based
generator architecture for generative adversarial networks.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR) , June 2019. 8
[36] Ilyes Khemakhem, Diederik Kingma, Ricardo Monti, and
Aapo Hyvarinen. Variational autoencoders and nonlinear ica:
A unifying framework. In International Conference on Ar-
tificial Intelligence and Statistics , pages 2207–2217. PMLR,
2020. 8
[37] Lingjing Kong, Shaoan Xie, Weiran Yao, Yujia Zheng,
Guangyi Chen, Petar Stojanov, Victor Akinwande, and Kun
Zhang. Partial disentanglement for domain adaptation. In
Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba
Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceed-
ings of the 39th International Conference on Machine Learn-
ing, volume 162 of Proceedings of Machine Learning Re-
search , pages 11455–11472. PMLR, 17–23 Jul 2022. 8
[38] S ´ebastien Lachapelle, Pau Rodr ´ıguez L ´opez, Yash Sharma,
Katie Everett, R ´emi Le Priol, Alexandre Lacoste, and Si-
mon Lacoste-Julien. Disentanglement via mechanism spar-
sity regularization: A new principle for nonlinear ica. arXiv
preprint arXiv:2107.10098 , 2021. 8
[39] Jason D. Lee, Qi Lei, Nikunj Saunshi, and Jiacheng Zhuo.
Predicting what you already know helps: Provable self-
supervised learning, 2021. 8
[40] Gang Li, Heliang Zheng, Daqing Liu, Chaoyue Wang, Bing
Su, and Changwen Zheng. Semmae: Semantic-guided
masking for learning masked autoencoders. arXiv preprint
arXiv:2206.10207 , 2022. 8
[41] Zhaowen Li, Zhiyang Chen, Fan Yang, Wei Li, Yousong
Zhu, Chaoyang Zhao, Rui Deng, Liwei Wu, Rui Zhao, MingTang, et al. Mst: Masked self-supervised transformer for
visual representation. Advances in Neural Information Pro-
cessing Systems , 34:13165–13176, 2021. 1, 8
[42] Tsung-Yi Lin, Piotr Doll ´ar, Ross Girshick, Kaiming He,
Bharath Hariharan, and Serge Belongie. Feature pyra-
mid networks for object detection. In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pages 2117–2125, 2017. 7, 16
[43] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll ´ar, and C Lawrence
Zitnick. Microsoft coco: Common objects in context. In
European conference on computer vision , pages 740–755.
Springer, 2014. 7, 16
[44] Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar
Raetsch, Sylvain Gelly, Bernhard Sch ¨olkopf, and Olivier
Bachem. Challenging common assumptions in the unsuper-
vised learning of disentangled representations. In interna-
tional conference on machine learning , pages 4114–4124.
PMLR, 2019. 8
[45] Qi Lyu, Xiao Fu, Weiran Wang, and Songtao Lu. Latent
correlation-based multiview learning and self-supervision:
A unifying perspective. arXiv preprint arXiv:2106.07115 ,
2021. 2, 8
[46] Qi Lyu, Xiao Fu, Weiran Wang, and Songtao Lu. Un-
derstanding latent correlation-based multiview learning and
self-supervision: An identifiability perspective. In Interna-
tional Conference on Learning Representations , 2022. 8
[47] Lars Maaløe, Marco Fraccaro, Valentin Li ´evin, and Ole
Winther. Biva: A very deep hierarchy of latent variables for
generative modeling. Advances in neural information pro-
cessing systems , 32, 2019. 2
[48] Jiachun Pan, Pan Zhou, and Shuicheng Yan. Towards un-
derstanding why mask-reconstruction pretraining helps in
downstream tasks. arXiv preprint arXiv:2206.03826 , 2022.
8
[49] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-
ing transferable visual models from natural language super-
vision. In International Conference on Machine Learning ,
pages 8748–8763. PMLR, 2021. 1
[50] Sara Sabour, Nicholas Frosst, and Geoffrey E Hinton. Dy-
namic routing between capsules. Advances in neural infor-
mation processing systems , 30, 2017. 1, 2
[51] Umme Sara, Morium Akter, and Mohammad Shorif Ud-
din. Image quality assessment through fsim, ssim, mse and
psnr—a comparative study. Journal of Computer and Com-
munications , 7(3):8–18, 2019. 5
[52] Vaishaal Shankar, Rebecca Roelofs, Horia Mania, Alex
Fang, Benjamin Recht, and Ludwig Schmidt. Evaluating ma-
chine accuracy on imagenet. In International Conference on
Machine Learning , pages 8634–8644. PMLR, 2020. 7, 15
[53] Yuge Shi, N. Siddharth, Philip H. S. Torr, and Adam R. Ko-
siorek. Adversarial masking for self-supervised learning,
2022. 8
[54] Peter Sorrenson, Carsten Rother, and Ullrich K ¨othe. Dis-
entanglement by nonlinear ica with general incompressible-
flow networks (gin). arXiv preprint arXiv:2001.04872 , 2020.
8
[55] Arash Vahdat and Jan Kautz. Nvae: A deep hierarchical vari-
ational autoencoder. Advances in Neural Information Pro-
cessing Systems , 33:19667–19679, 2020. 2
[56] Laurens Van Der Maaten. Accelerating t-sne using tree-
based algorithms. The Journal of Machine Learning Re-
search , 15(1):3221–3245, 2014. 15
[57] Laurens Van der Maaten and Geoffrey Hinton. Visualiz-
ing data using t-sne. Journal of machine learning research ,
9(11), 2008. 6
[58] Julius V on K ¨ugelgen, Yash Sharma, Luigi Gresele, Wieland
Brendel, Bernhard Sch ¨olkopf, Michel Besserve, and
Francesco Locatello. Self-supervised learning with data aug-
mentations provably isolates content from style. Advances
in neural information processing systems , 34:16451–16467,
2021. 2, 4, 8
[59] Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P
Xing. Learning robust global representations by penalizing
local predictive power. Advances in Neural Information Pro-
cessing Systems , 32, 2019. 7, 15
[60] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Si-
moncelli. Image quality assessment: from error visibility to
structural similarity. IEEE transactions on image processing ,
13(4):600–612, 2004. 5
[61] Zhirong Wu, Zihang Lai, Xiao Sun, and Stephen Lin. Ex-
treme masking for learning instance and distributed visual
representations. arXiv preprint arXiv:2206.04667 , 2022. 1,
4
[62] Feng Xie, Biwei Huang, Zhengming Chen, Yangbo He, Zhi
Geng, and Kun Zhang. Identification of linear non-gaussian
latent hierarchical structure. In International Conference on
Machine Learning , pages 24370–24387. PMLR, 2022. 1, 2,
8
[63] Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin
Bao, Zhuliang Yao, Qi Dai, and Han Hu. Simmim: A simple
framework for masked image modeling. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 9653–9663, 2022. 1, 4, 8
[64] Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and
St´ephane Deny. Barlow twins: Self-supervised learning via
redundancy reduction. In International Conference on Ma-
chine Learning , pages 12310–12320. PMLR, 2021. 4
[65] Lin Zhang, Lei Zhang, Xuanqin Mou, and David Zhang.
Fsim: A feature similarity index for image quality assess-
ment. IEEE transactions on Image Processing , 20(8):2378–
2386, 2011. 5
[66] Qi Zhang, Yifei Wang, and Yisen Wang. How mask matters:
Towards theoretical understandings of masked autoencoders.
arXiv preprint arXiv:2210.08344 , 2022. 8
[67] Shengjia Zhao, Jiaming Song, and Stefano Ermon. Learning
hierarchical features from generative models. arXiv preprint
arXiv:1702.08396 , 2017. 2
[68] Yujia Zheng, Ignavier Ng, and Kun Zhang. On the identifia-
bility of nonlinear ica: Sparsity and beyond. arXiv preprint
arXiv:2206.07751 , 2022. 8[69] Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang
Xie, Alan Yuille, and Tao Kong. ibot: Image bert pre-training
with online tokenizer. arXiv preprint arXiv:2111.07832 ,
2021. 1, 8
A. Proof for Theorem 1
In this section, we provide the proof for Theorem 1.
Theorem 1. (Locating the shared information c): In a hierarchi-
cal latent variable structure G, for each specific mask m, there
exists a corresponding minimal set of latent variables csuch that
the generating process of xcan be expressed as in Figure 3 where
the following conditions are satisfied:
1.xm=gxm(c,sm)andxmc=gxmc(c,smc)where both
gxmandgxmcare invertible;
2.sm⊥ ⊥(c,smc);
3.cis minimal: ∀c′⊂Zsuch that dim (c′)<dim(c),c′
cannot satisfy the two conditions above.
Suchcand the corresponding smare unique and can be located
from the hierarchical structure by executing Algorithm 1. Further-
more, smccan be found through Algorithm 2.
Algorithm 1 Search for the minimal candsm.candsmdis-
cussed in text can be viewed as the concatenations of vectors in Cand
Sm. LocateParents (·)pins down the locations Z’s parents (including ex-
ogenous variables) in the graph. DirectedPaths (d,Xc
m)returns the set of
variables on the directed paths between dandXc
m.
1:inputs : The hierarchical graph structure G, and the
partitioned observables Xm,Xmc.
2:C,Sm← ∅,∅.
3:Selection stage:
4:forx∈ X mdo
5: Z ← { x}.
6: while Z ̸=∅do
7: Z,E ← LocateParents (Z)
8: Sm← S m∪ E
9: forp∈ Z do
10: ifp∈Ancestors (xmc)then
11: C ← C ∪ { p}
12: Z ← Z \ { p}
13:Pruning stage:
14:ford∈ C do
15: ford′∈ C \ { d}do
16: ifd′∈DirectedPaths (d,Xmc)then
17: C ← C \ { d}
return C,SmAlgorithm 2 Search for smcgivenC.LocateParents (p)pins
down the locations p’s parents (including exogenous variables) in
the graph.
1:inputs : The hierarchical graph structure G, the parti-
tioned observables Xm,Xmc, andCreturned by Algo-
rithm 1.
2:Smc← ∅.
3:forx∈ X mcdo
4: P,P′← {x},∅.
5: while P ̸=∅do
6: forp∈ P do
7: forp′∈LocateParents (p)do
8: ifp′is exogenous then
9: Smc← S mc∪ {p′}
10: else if p′∈ C then
11: Smc← S mc∪(LocateParents (p)\
{p′})
12: else
13: P′← P′∪ {p′}
14: P ← P′
return Smc
Proof. We will show that Algorithm 1 returns the minimal set of
variables that satisfy all conditions in Theorem 1, which implies its
existence. We will then argue that such Cis unique for a specific
maskm.
Condition 1: We first discuss the invertibility of gxm. Due to
the invertibility assumption of the generating process, each back-
track step in Algorithm 1 is invertible (lossless). Thus, before the
pruning stage, the mapping between (C,Sm)andXmis invertible,
as the information of Xmis either stored in either CorSm. We
now show that the pruning stage does not break this invertibility.
To see this, we note that for each cthat is removed in the pruning
stage, there exists c′∈ Con the directed path from ctoXmc(per
Algorithm 1). Therefore, cis a parent/ancestor of c′and can thus
be retrieved by backtracking from c′thanks to the invertibility of
the generating process. Therefore, the mapping between (C,Sm)
andXmis invertible.
We now address the invertibility of gxmc, i.e., the mapping be-
tween (C,Smc)andXmc. We observe that a similar argument
applies: Algorithm 2 dictates that the latent variables from the
backtracking from Xmcare either stored in either CorSmc. It
follows that gxmcis invertible.
Condition 2: We show that (c,sm,smc)returned by Algo-
rithm 1 and Algorithm 2 satisfies Condition 2 by contradiction. We
suppose that sm̸⊥ ⊥(c,smc). Then it implied that ∃d∈(c,smc),
∃ε∈sm, such that d∈Descendants (ε). More precisely, it fol-
lowed that there was a directed path that started from εand ended
atd, and a child of ε, denoted as δ, was located on this path. If
d̸∈Descendants (ε), there would be no directed paths from εtod
and thus at least one V-structure would sit on each path between ε
anddthat blocked the path. According to Algorithm 1, as ε∈sm,
it implied that δ̸∈candδ̸∈Ancestors (xm)∩Ancestors (xmc).
We first investigate the case where d∈c, i.e., sm̸⊥ ⊥c.
The fact that d∈cimplied that d∈Ancestors (xm)∩
Ancestors (xmc)which further implied that δ∈Ancestors (xm)∩
Ancestors (xmc)asδwas an ancestor of d. Therefore, we have ar-
rived at a contraction to the observation that δ∈Ancestors (xm)∩
Ancestors (xmc).
We now discuss the scenario where d∈smc. By design, Al-
gorithm 2 ensures that smccontains two types of latent variables,
exogenous variables and a spouse of latent variables in c. Assm
consists solely of exogenous variables and exogenous variables are
independent mutually, it could only be the case that dwas a spouse
of a latent variable in c. By Algorithm 1, there would be a di-
rected path from δtoxm. Also, Algorithm 2 ensured that dlied
on a path directed to xmc. As there existed a directed path from δ
tod, there must exist a directed path from δtoxmc. Therefore,
δ∈Ancestors (xm)∩Ancestors (xmc)which contradicts the fact
established above.
Therefore, these contradiction implies that sm⊥ ⊥(c,smc).
So far, we have shown that Algorithm 1 and Algorithm 2 yield
(c,sm,smc)that fulfills the conditions of Figure 3. In the follow-
ing, we show that (c,sm)is the minimal solution and is unique.
Uniqueness and minimality of (c,sm):We now reason
about that given the mask and the hierarchical structure, (c,sm)
returned by Algorithm 1 is the set of minimal dimensionality that
can fulfill the conditions, and such a minimal set is unique.
By construction, Algorithm 1 ensures that for each c∈ Cthere
exists an undirected path that is made up of a directed path from
cto the masked variable xmand a directed path from cto the
unmasked variable xmcand no other c′∈ C sits on this entire
undirected path. To see this, there must exist a directed path from
ctoxmwithout any other c′∈ C on it, otherwise cwould not
be placed in Cin Algorithm 1. In addition, the pruning stage of
Algorithm 1 mandates that there must exist xmcsuch that the path
fromctoxmcdoes not contain other c′∈ C. We note that c
chosen by Algorithm 1 is the variable with the smallest possible
dimension to block such a path, as it resides on the highest level
compared to other variables on the path and the variable dimension
increases monotonically along directed paths.
Therefore, the choice of each cis minimal, and such a choice
is unique. As Smis the set of exogenous variables necessary for
Cto restore Xm, the selection of Smis also unique. Hence, we
conclude that the (C,Sm)returned by Algorithm 1 is the minimal
choice and is unique.
B. Identifiability proof
In this section, we present the proof for Theorem 2. We first
give a general identifiability theory (i.e., Theorem 3) for the gen-
erating process in Figure 3 and then make the connection to the
proof of Theorem 2.
Theorem 3. The generating process in Figure 3 is defined as fol-lows:
[v1,v2] =g(c,s1,s2), (3)
v1=g1(c,s1), (4)
v2=g2(c,s2), (5)
where c∈ C ⊂ Rdc,s1∈ S ⊂ Rds1, ands2∈ S2⊂Rds2. Both
g1andg2are smooth and have non-singular Jacobian matrices
almost anywhere, and gis invertible.
Ifˆg1:Z → V 1andˆg2:Z → V 2assume the generating
process of the true model (g1, g2)and match the joint distribution
pv1,v2, then there is a one-to-one mapping between the estimate
ˆcand the ground truth coverC × S × S , that is, cis block-
identifiable.
Proof. For(v1,v2)∼pv1,v2, because of the matched joint distri-
bution, we have the following relations between the true variables
(c,s1,s2)and the estimated ones (ˆc,ˆs1,ˆs2):
v1=g1(c,s1) = ˆg1(ˆc,ˆs1), (6)
v2=g2(c,s2) = ˆg2(ˆc,ˆs2), (7)
(ˆc,ˆs1,ˆs2) = ˆg−1(v1,v2) = ˆg−1(g(c,s1,s2)) := h(c,s1,s2),
(8)
where we define the smooth and invertible function h:=
ˆg−1◦gthat transforms the true variables (c,s1,s2)to estimates
(ˆc,ˆs1,ˆs2).
Plugging Equation 8 into Equation 6 yields the following:
g1(c,s1) = ˆg1(hc,s1(c,s1,s2)).
Fori∈ {1, . . . , d v1}and (j∈ {1, . . . , d s2}), taking partial
derivative of the i-th dimension of both sides w.r.t. s2,j:
0 =∂g1,i(c,s1)
∂s2,j=∂ˆg1,i(hc,s1(c,s1,s2))
∂s2,j.
The equation equals zero because there is no s2,jin the left-hand
side of the equation. Expanding the derivative on the right-hand
side gives:
X
k∈{1,...,dc+ds1}∂ˆg1,i
∂h(c,s1),k·∂h(c,s1),k
∂s2,j(c,s1,s2) = 0 (9)
For(ˆc,ˆs1)∈ C × S \ E 1where E1denotes some subset with
zero measure, there are at least dc+ds1values of ifor which
vectors [∂ˆg1,i
∂h(c,s1),1(ˆc,ˆs1), . . . ,∂ˆg1,i
∂h(c,s1),dc+ds1(ˆc,ˆs1)]are linearly
independent, which is equivalent to the non-singular Jacobian ma-
trix condition. Therefore, the (dc+ds1)×(dc+ds1)linear system
is invertible and the solution states that:
∂h(c,s1),k
∂s2,j(c,s1,s2) = 0,
for any k∈ {1, . . . , d c+ds1},j∈ {1, . . . , d s2}, and (ˆc,ˆs1)∈
C ×S \E 1. Therefore, we have shown that hc,s1, i.e.(ˆc,ˆs1), does
not depend on s2.
Applying the same reasoning to hc,s2, we can obtain that hc,s2,
i.e.(ˆc,ˆs2)does not depend on s1onC × S .
Thus, for (ˆc,ˆs1,ˆs2)∈ C × S × S , we can observe that ˆcdoes
not depend on s1ands2, that is, ˆc=hc(c).
Notice that in all procedures above, the roles of the
true quantities (c,s1,s2, g, g 1, g2)and the estimated quantities
(ˆc,ˆs1,ˆs2,ˆg,ˆg1,ˆg2)are symmetric. Therefore, we can switch the
two sets of quantities and derive the relation: for (c,s1,s2)∈
(C × S × S ),cdoes not depend on ˆs1andˆs2, that is, c=h′
c(ˆc).
In sum, we have shown that on (C × S × S ), there is a one-to-
one mapping between candˆc.
We now show that Theorem 2 follows directly from Theorem 3.
Theorem 2. (Identifiability of c): For each mask m, given the
dimensions (dc, dsm)the encoder function Emc(·)recovers all
information of clocated in Theorem 1, i.e., there exists a one-to-
one mapping h, s.t.,h(c) =ˆc.
Proof. We invoke Theorem 3 and establish the connection be-
tween the MAE training and the estimation model in Theorem 3.
In particular, we show that under Assumption 2, any solution pro-
duced by the MAE objective satisfies the conditions in Theorem 3
and consequently is equipped with the identifiability guarantee.
We establish the correspondence between the MAE configura-
tion and the estimation models in Theorem 3:
•v1←xm;
•v2←xmc;
•ˆg1←Dm(·,ˆsm);
•ˆg2←˜gmc, where Emc(·) = [˜g−1
mc(·)]1:dc.
We can observe that the minimizer of MAE satisfies the condi-
tions specified in Theorem 3. This is because for the optimal solu-
tionEmcof the MAE objective, we can always construct a ˜gmc,
which, together with Dm, matches the joint distribution pxm,xmc
and shares ˆc, as stipulated in Theorem 3. Thus, as shown in The-
orem 3, there exists a one-to-one mapping between the MAE es-
timate ˆc:=Emc(xmc)and the true variable c, which concludes
our proof.
C. Experimental Setup
In this section, we provide the details of the experimen-
tal setups for our empirical results. Checkpoints and some
codes are in https://github.com/martinmamql/mae_
understand .
C.1. Masked Autoencoder
Masked Autoencoder (MAE) is an auto-encoding approach
based on Vision Transformers (ViT) [17]. It consists of five
steps: masking, encoding, unmasking, decoding, and reconstruc-
tion. First, an image is divided into non-overlapping patches.
Then MAE samples a subset of patches and discards the remain-
ing patches. MAE uses a hyper-parameter, masking ratio, to de-
termine the percentage of patches to discard. For instance, if the
masking ratio is 75% ,3
4of the patches in an image will be dis-
carded, and only1
4of the patches will be fed into the encoder.
The sampling of patches follows a uniform distribution. Next, a
ViT encoder first embeds patches using a linear projection withpositional embeddings and then uses the processed embeddings to
feed into transformer blocks. For decoding, MAE first re-arranges
the encoded embeddings from the visible patches according to
their corresponding positions in the original image and then uses
a shared learned mask token to fill in the patches that are masked.
Essentially, this means the input of the decoder is a combination of
encoded visible patches and the mask tokens, where the positions
of the mask tokens are the masked patches in the original image.
The decoder is another lightweight ViT, and it processes the de-
coder input through transformer blocks. Lastly, the last layer of
the decoder linearly projects output patches to pixels, and the pixel
output is reshaped to form a reconstruction of the original image.
The objective function is the mean squared error between the re-
construction and the original image. MAE has thrived because of
its simple design and strong empirical performance.
In the main text, inspired by a follow-up work of MAE [28],
we study the effect of masking by decoupling the patch size for
masking images and the patch size hyperparameter in the ViT. Par-
ticularly, in the main text, we only vary the masking patch size and
fix the ViT patch size at 8. Nevertheless, the original MAE [22]
does not decouple the two patch sizes. Therefore, for the reference
of readers, in Appendix, we provide some analysis and results pro-
duced based on the patch size design from the original MAE [22],
where the masking patch size and the ViT patch size are equal.
We study three patch sizes: {8,16,32}. The experimental setup
in [28] and the setup in [22] are interchangeable except for whether
the patch size for the Vision Transformer varies.
C.2. Pretraining and Linear Probing
For pretraining MAE under different masking ratios or patch
sizes, we leverage the Tensor Processing Unit (TPU) from Google
Cloud. We train separate MAE models for each (masking ratio,
patch size )pair, and each pretrained MAE corresponds to a unique
masking ratio and patch size. We train all MAEs for 800epochs.
Training time varies, with the shortest (patch size = 32 ) taking 18
hours on a TPU v3-128 Pod, and the longest (patch size = 8) tak-
ing 40 hours on a TPU v3-128 pod. The architecture follows the
exact implementation from the original MAE paper [22], without
any hyper-parameter tuning except masking ratio and patch size,
which we study in this paper. Details of augmentation, initializa-
tion, and base learning rate scaling can be found in the Appendix
section of [22], all of which we follow.
After pretraining, we also follow the original MAE work to use
linear probing to evaluate the representation quality. After pre-
training, we remove the projection layers and add a supervised
learning classifier on frozen features of MAE encoders. The de-
coders are discarded during linear probing. Other details of linear
probing can be found in the Appendix section of [22]. We use the
same hyper-parameters of linear probing as in [22].
C.3. Reconstructing high-level or low-level repre-
sentations
To perform reconstruction, we use both the encoder and the
decoder from the pretrained MAEs. All samples from ImageNet-
1K are passed through the encoder without any masking, and the
decoder reconstructs images in the original input space. Since no
masking is applied, no masking token is applied to the input of the
decoder. We use the reconstructed images and the original images
Figure 9. Reconstruction evaluation using the validation set without masking, based on two structural-level similarity metrics (SSIM and
FSIM) and two pixel-level metrics (PSNR and MSE). We plot negative MSE for easier visualization. Higher SSIM and FSIM indicate
high-level information is better captured, while higher PSNR and negative MSE indicates better low-level reconstruction. Here the patch
size refers to the patch size in the original MAE, where the masking patch size and the patch size of ViT are equal.
to perform evaluations of four metrics: SSIM, FSIM, MSE, and
PSNR. No training is performed, and the weights of the encoder
and the decoder are frozen.
In Fig. 9, we show the reconstruction analysis using the origi-
nal patch size design in MAE. Similar to the result in the main text,
higher patch sizes produce image reconstructions capturing high-
level similarities better, while low patch sizes have reconstructions
better on low-level metrics.
C.4. Attention Analysis
We follow the attention heatmap visualization in DINO [10],
where the chosen token is the [CLS] token or an object-related
token. We visualize the self-attention module from the last block
of the MAE encoder ViT. Brighter colors suggest larger attention
weights. For easier visualization, attentions below a threshold of
activation scores are not shown. We use the same threshold as
[10]. For the self-attention visualization on the [CLS] token, we
use an average of all heads in the last layer of the encoder ViT.
For the self-attention visualization of the object-related token, we
use the first head of the last layer of the encoder ViT, because
using the average attention over all heads will result in a heatmap
with much higher overall attention scores across pixels, making
the visualization hard to interpret.
Figure 10. T-SNE embeddings of different MAE models under
varied masking ratios and patch sizes. We fix the masking ratio at
0.75to change patch sizes. Each color represents one ImageNet
class. The patch size refers to the patch size in the original MAE,
where the masking patch size and the patch size of ViT are equal.
C.5. Linear separability
To illustrate the linear separability of different MAEs under
varied masking ratios or patch sizes, we sample ten random classes
from ImageNet, and then use each MAE encoder to process im-
ages in the 10 classes to produce embeddings. We then project
embeddings of all samples using PCA to a 50-dimension spacebefore t-SNE, as recommended by [56]. For t-SNE, we use a per-
plexity of 20.
In Fig. 10, we show the t-SNE plot using the original patch size
design in MAE. Similar to the main text, embeddings are more
separated in patch sizes 16 and 32 than 8, but differently, there are
no significant differences between 16 and 32. Larger patch sizes
generate more linearly separable embeddings in this case, although
the separability seems indistinguishable for sizes 16 and 32.
For the robustness evaluation, we evaluate different vari-
ants of ImageNet validation datasets: ImageNet-v2 (INV2) [52],
ImageNet-Adversarial (IN-A) [25], ImageNet-Rendition [4], and
ImageNet-Sketch (IN-S) [59]. We also include another object
classification dataset, ObjectNet (OJN) [4]. ImageNet-v2 con-
tains three new test sets with 10,000 new images each, sampled a
decade after the collection of the original ImageNet dataset, and is
independent of existing models to prevent overfitting. ImageNet-
Adversarial consists of natural images with adversarial filtration,
meaning samples that can be classified with spurious cues are
removed. Examples in ImageNet-A are harder to classify cor-
rectly and can cause mistakes across various models. ImageNet-
Rendition contains renditions of ImageNet classes, such as art,
cartoons, graffiti, and paintings. These examples share the same
high-level object labels as ImageNet examples but differ in style
and texture. ImageNet-Sketch contains black and white images of
ImageNet classes, also differing in color and texture compared to
original ImageNet samples. ObjectNet is a set of images captured
at unusual poses in cluttered, natural scenes, which can severely
degrade recognition performance.
Note that for evaluating these datasets, no training is per-
formed; we use the MAE encoders after linear probings, therefore
the checkpoints that are pretrained and linear-probed on ImageNet,
and evaluate the checkpoints on these validation datasets without
any parameter updates.
In Table 4, we show the robustness analysis using the original
patch size design in MAE. A moderate patch size 16yields the
best robustness evaluation on IN-v2, OJN, IN-R, and IN-S. If we
follow the original MAE and do not decouple masking patch size
and ViT patch size, a medium patch size has stronger robustness
performances than extreme patch sizes.
C.6. Shape bias
The cue-conflict dataset was introduced by [19] to evaluate
how much deep learning models rely on shape information for
prediction, which reflects the model’s robustness to spurious cor-
relation like textures. This dataset consists of 1280 images syn-
thesized from 160 images of objects and 48 images of textures.
The shape accuracy is measured by the fraction of images pre-
mask ratio patch size IN1K IN-v2 OJN IN-R IN-A IN-S
0.75 8 62.57 49.17 13.44 19.42 3.73 10.73
0.75 16 67.41 54.23 18.24 25.20 3.76 15.51
0.75 32 55.51 42.35 13.46 18.70 1.89 9.48
Table 4. Accuracy ( %) of linear probing and robustness evalu-
ation on ImageNet variants and ObjectNet. We linear probe MAE
via supervised training on IN1K, and then perform inference on
IN1K as well as other evaluation sets. We fix the masking ratio at
0.75to change patch sizes. The patch size refers to the patch size
in the original MAE, where the masking patch size and the patch
size of ViT are equal.
mask ratio patch size APboxAPmask
0.75 8 34.21 32.28
0.75 16 33.77 32.04
0.75 32 32.39 30.54
Table 5. COCO object detection and segmentation using a ViT
Mask R-CNN baseline. We fix the masking ratio at 0.75to change
patch sizes. The patch size refers to the patch size in the original
MAE, where the masking patch size and the patch size of ViT are
equal.
dicted correctly by their shape. We directly run the pretrained
MAE models with linear probes trained on ImageNet-1K on the
cue-conflict dataset to examine the representation resulting from
MAE pretraining without any adaptation to the test dataset.
C.7. Transfer learning
We use the pretrained MAE ViT encoder as an FPN [42] back-
bone in Mask-RCNN [24], following [22]. To do so, [22] uses a
stack of pretrained transformer blocks in MAE to produce feature
maps at a single scale; for instance, patch size 16will produce
stride 16features. Then the features are equally divided, and up-
sampling or downsampling is applied to create features at differ-
ent scales. Lastly, the FPN is built on multi-scale features. Below
we include the transfer learning results of different patch sizes on
COCO object detection and segmentation [43]. Because different
patch sizes in ViT will influence the scale of feature maps in the
FPN, we enforce the same combinations of multi-scale features:
i.e., stride 4,8,16, and 32.
From Table 5, we show the transfer learning results of MAE
under different patch sizes. Patch size 8performs the best, and
patch size 16is better than 32. The reason for the better perfor-
mance at patch size 8may be due to a smaller batch size used,
compared to patch size 16and32(we can only fit batch size 1
for patch size 8due to the increased number of tokens to pro-
cess because of a smaller patch size.) We use the same batch size
for32and16, and the comparison between the two supports our
claim: an extreme masking scheme can hurt the model’s capac-
ity to capture high-level information or, in this case, the semantic
understanding of the scene.