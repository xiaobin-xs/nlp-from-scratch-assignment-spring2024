arXiv:2309.08876v2  [eess.AS]  9 Jan 2024DECODER-ONLY ARCHITECTURE FOR SPEECH RECOGNITION WITH
CTC PROMPTS AND TEXT DATA AUGMENTATION
Emiru Tsunoo, Hayato Futami, Yosuke Kashiwagi
Sony Group Corporation, JapanSiddhant Arora, Shinji Watanabe
Carnegie Mellon University, USA
ABSTRACT
Collecting audio–text pairs is expensive; however, it is mu ch eas-
ier to access text-only data. Unless using shallow fusion, e nd-to-
end automatic speech recognition (ASR) models require arch itec-
ture modiﬁcations or additional training schemes to use tex t-only
data. Inspired by recent advances in decoder-only language mod-
els (LMs), such as GPT-3 and PaLM adopted for speech-process ing
tasks, we propose using a decoder-only architecture for ASR with
simple text augmentation. To provide audio information, en coder
features compressed by CTC prediction are used as prompts fo r the
decoder, which can be regarded as reﬁning CTC prediction usi ng the
decoder-only model. Because the decoder architecture is th e same as
an autoregressive LM, it is simple to enhance the model by lev erag-
ing external text data with LM training. An experimental com parison
using LibriSpeech and Switchboard shows that our proposed m od-
els with text augmentation training reduced word error rate s from
ordinary CTC by 0.3% and 1.4% on LibriSpeech test-clean and t est-
other set, respectively, and 2.9% and 5.0% on Switchboard an d Call-
Home. The proposed model had advantage on computational efﬁ -
ciency compared with conventional encoder–decoder ASR mod els
with a similar parameter setup, and outperformed them on the Lib-
riSpeech 100h and Switchboard training scenarios.
Index Terms —Speech recognition, Decoder-only ASR, CTC,
Prompt
1. INTRODUCTION
End-to-end automatic speech recognition (ASR) requires a l arge
amount of audio–text pair data, which is difﬁcult to obtain, whereas
a large amount of text-only data can be obtained much more eas ily.
How to exploit unpaired text data is not a trivial problem for ASR
tasks. It is common to perform score interpolation with an ex ternal
language model (LM) trained with text data with an ASR model
[1, 2]. Some studies have revealed that estimating the lingu istic bias
in ASR models, an internal LM, is more suitable for further ef fective
fusion [3–5]. To exploit the accessible text-only data dire ctly to the
models, some studies modiﬁed the model architecture to acce pt both
audio–text pairs and text-only data as the input for trainin g in a
multitask learning manner [6–9].
Recently, such text resources have been effectively used by
introducing pretrained large LMs (LLMs) including decoder -only
LMs such as GPT-3 [10] and PaLM [11] into the ASR formulation.
Studies have successfully adapted decoder-only LMs for spe ech-
processing tasks [12–17]. To bridge the audio–text modalit ies,
audio information is injected into the LLMs as a prompt, whic h are
discrete audio units [12–14] or continuous representation s injected
directly into the linguistic embedding space [15, 16]. In th e latter
approach, encoded audio features are compressed by convolu tion
layers [15, 16] or by CTC predictions [16], which have also be enintroduced in speech translation (ST) [18] and RNN transduc er
(RNN-T) decoding [19,20]. Owing to the strong potential of L LMs,
these methods perform well in multi-task scenarios of speec h-to-text
processing, such as ASR, speech synthesis, and ST. Inspired by
this, we adopt a decoder-only architecture for ASR tasks tha t can be
effectively enhanced with external text-only data.
This study aims to build a decoder-only architecture for ASR
tasks and enhance its performance with text augmentation us ing
external text-only data. We provide audio information as pr ompts
compressed by CTC prediction, which can also be regarded as r e-
ﬁning CTC prediction using a decoder-only model. Although W u
et al. attempted to train a decoder-only from scratch, they obtain ed
a slightly degraded ST model compared to conventional encod er–
decoder models. However, we train the decoder for ASR tasks u sing
not only the audio–text pair but also external text-only dat a as aug-
mentation. Thus, the model is trained from scratch for ASR an d LM
tasks simultaneously. Experimentally, we conﬁrmed that th e pro-
posed model successfully reﬁned the CTC results while achie ving
faster inference owing to the compression mechanism. Using Lib-
riSpeech 100h subset and Switchboard with text augmentatio n, the
decoder-only model outperformed conventional encoder–de coder
models with a similar parameter setup. The main contributio ns of
this study are as follows:
• We propose a new training scheme to build a decoder-only mod el
from scratch for ASR tasks simultaneously augmented by text -
only data.
• To the best of our knowledge, this is the ﬁrst study to succes s-
fully outperform conventional encoder–decoder models wit h the
decoder-only model having similar parameter sizes by explo iting
external text-only data effectively.
• We experimentally show that our proposed approach achieve s
lower word error rates (WERs) by 0.3% and 1.4% for Lib-
riSpeech test-clean and test-other, respectively, than th e encoder–
decoder model, with approximately half the computational c ost,
when we used paired data of 100h with text augmentation of
960h transcription data.
2. CTC PROMPTS FOR DECODER-ONLY ASR
2.1. Decoder-only architecture
We follow the encoder–decoder conformer ASR model [21], exc ept
that there are no source–target attention layers in the deco der trans-
former. A transformer decoder that does not have source–tar get at-
tention layers is considered an autoregressive decoder-on ly LM as
GPT-3 [10] or PaLM [11].
ASR is a task to predict the most probable I-length token se-
quenceYIgiven aT-length input audio XT, i.e.,YIwith the high-
est probability p(YI|XT). Instead of directly using audio input XT
EncoderCTC
<aud> <sos><eos>
Next token prediction
Speech 
domainLinguistic 
domain
Decoder
embedding
space
tokens
audio signalsCTC prediction
Fig. 1 . Model architecture of decoder-only model for ASR with CTC
prompts.
in the decoder, it is generally approximated by compact audi o rep-
resentation. The audio information is provided as τ-length prompts
ˆHτ={ˆht|1≤t≤τ}directly in the embedding space of the de-
coder. We propose using CTC prediction to efﬁciently genera te the
prompts, namely, CTC prompts, which are presented in Sec.2. 2. The
decoder autoregressively predicts the next linguistic tok enyigiven
an audio prompt token ⟨aud⟩, CTC prompts ˆHτ, and the previous
outputsY<i={yj|0≤j < i}. Thus,
p(Yi) =i∏
j=1p(yj|ˆHτ,Y<j)
=i∏
j=1Dec(ˆHτ,Y<j), (1)
wherey0is a start-of-sequence token, ⟨sos⟩. An overview of the
proposed method is presented in Fig. 1.
2.2. Audio prompt using CTC prediction
CTC prompts are generated using an encoder. Generally, the l ength
of audio is longer than that of the text sequence, which somet imes
becomes problematic because the decoder self-attention co mputa-
tion grows quadratically with the input length. A common app roach
is to use convolution layers [15,16], which downsample the a udio in-
put to a frame rate of 80 ms or higher to match the granularity o f the
linguistic tokens. However, Wu et al. showed that the CTC-based
compression [18] is more effective than convolution layers [16]. In
the literature, they compared removing frames that CTC pred icts as
“blank” and averaging frames of the same CTC predictions. We
adopt the frame-removing approach, and all approaches were exper-
imentally compared in Sec. 3.2.
The conformer encoder outputs T′-length encoded audio fea-
turesHT′={ht|1≤t≤T′}from audio input XTwith a down-
sample rate of Nas
HT′= Enc(XT), (2)
whereT′=T/N . Subsequently, the CTC module maps HT′to
the probability distribution of vocabulary Vaugmented with a blank
token⟨blank⟩, which is jointly trained as in [22].
p(at) = CTC( ht), (3)<aud>(a)
(b)<sos><eos>
<sos><eos>Decoder
Decoder
Fig. 2 . LM training methods. (a) is ordinary LM training. (b) is a
pseudo prompt with the ground truth embedding vectors.
whereat∈ {V ∪⟨blank⟩}.
Since the length of audio features T′is generally longer than
that of text sequences, audio prompts ˆHτare downsampled so that
τ < T′. We remove frames predicted as blank by the CTC and map
them to the embedding space of the decoder, as follows.
ˆHτ={MLP(ht)|t: ˆat̸=⟨blank⟩}, (4)
whereˆatis the most probable one in the probability distribution (3)
andMLP(·)is a mapping function from the encoder output to the
embedding space, for which we adopt a linear layer for simpli city.
Thus, the compressed audio prompt is directly injected into the con-
tinuous embedding space of the decoder. The decoder can also be
seen as a reﬁnement of the CTC prediction.
2.3. Training with text augmentation
While ASR training requires audio–text pair data, a large te xt corpus
is much easier to collect. Therefore, it is reasonable to enh ance ASR
performance with such text data, generally with a shallow fu sion of
external LM models [1,2]; for example, the LibriSpeech corp us [23]
provides signiﬁcantly more text-only data for training ext ernal LMs
in addition to the 960h audio–transcription pair data. We fo cus on
these scenarios and aim to enhance the decoder-only archite cture us-
ing an additional text set. For the ASR task, the encoder for C TC
prompts and the autoregressive decoder are jointly trained . Because
the architecture of the proposed decoder-only model and gen eral au-
toregressive LMs are represented by the same autoregressiv e trans-
former, we use text-only data for text augmentation to train only the
decoder for an LM task.
The encoder and the decoder can be pre-trained using the pair
data and unpaired text data, respectively, followed by ﬁne- tuning
with paired data as in [15]. However, we aimed to train them fr om
scratch because it is considered to converge to more optimal mod-
els, which was conﬁrmed in Sec. 3.2. To conduct combined trai ning,
we split the mini-batch Bfor both the tasks and compute each loss
simultaneously, that is, B={BASR,BLM}.
2.3.1. ASR training
Following the joint training of the encoder–decoder and CTC in [22],
we train the encoder, decoder, and CTC simultaneously. In ea ch
training step, in addition to the CTC loss Lctc, the decoder loss Latt
Table 1 . ASR results with LibriSpeech 100h audio–text pair data and 960h text data results. External LM was trained using extern al text data.
IDs Models Training Data Prompt Decoding Fusion WER Param RTF
ext-text CTC ext-LM dev-clean dev-other test-clean test-other
B1 Baseline CTC [24] 9.2 22.8 9.6 23.5 34.8M 0.14
B2 Baseline CTC [24] ✓ ✓ 7.1 18.8 7.3 19.6 44.2M 0.29
B3 Baseline RNN-T [25] ✓ ✓ 7.0 18.6 7.5 19.4 54.4M 0.67
B4 Baseline EncDec [21] 8.6 20.9 8.9 21.5 46.8M 0.53
B5 ✓ ✓ ✓ 6.9 18.0 7.3 18.9 56.2M 0.58
P1 Dec-only downsample 14.1 26.9 14.8 27.7 45.3M 0.38
P2 CTC average 30.5 41.8 30.2 41.2 45.3M 0.55
P3 CTC remove 9.3 22.4 9.5 23.0 45.3M 0.23
D1 Dec-only w/ textAug ✓ CTC remove 8.7 19.6 9.3 19.9 45.3M 0.24
D2 ✓ CTC remove ✓ 7.0 18.1 7.2 18.5 45.3M 0.24
D3 ✓ CTC remove ✓ ✓ 6.7 16.8 7.0 17.5 54.7M 0.29
F1 Dec-only (ﬁne-tuned) [15] ✓ CTC remove ✓ ✓ 7.0 18.0 7.3 18.7 54.7M 0.29
is considered only for the transcription part, not includin g prompts,
using teacher-forcing as:
LASR=λLctc+(1−λ)Latt
=λLctc+(1−λ)I∑
i=0−logp(yi|ˆHτ,˜Y<i), (5)
where˜Y<icomes from the ground truth and ˆHτis provided by the
CTC prediction of the current model introduced in (4). The gr adients
are passed through ˆHτsuch that the encoder parameters are updated
based on (5). At the beginning of training, the CTC predictio ns are
not sufﬁciently mature and tend to have fewer blank tokens. T his
results in longer CTC prompts, which can be problematic for t raining
purposes. Therefore, we use tunable threshold θto detect immature
predictions. When τ > θI , we consider the CTC predictions to be
immature and, for the particular training sentence, we repl aceLatt
withLLM, which will be introduced in Sec. 2.3.2.
2.3.2. LM training
Because the architecture of the decoder-only model and an LM are
represented by the same autoregressive transformer, we can only
train the decoder with an LM task using randomly selected tex t sen-
tences from the augmented data. A simple method is to follow o r-
dinary LM training and minimize the negative log-likelihoo d, which
is equivalent to minimizing the perplexity.
LLM=I∑
i=0−logp(yi|˜Y<i) (6)
This process is illustrated in Fig. 2(a). However, in ASR tas ks, the
decoder always expects CTC prompts. Therefore, it is reason able
to minimize the gap between LM training and ASR inference by
creating pseudo audio prompts with the ground-truth embedd ing se-
quence as in Fig. 2(b). For a subset of mini-batches B∗
LM, the CTC
prompts in (1) are replaced by ˜YI, as
L∗
LM=I∑
i=0−logp(yi|˜YI,˜Y<i). (7)
The latter task is much easier because it copies the prompt se quence
and outputs it directly. Therefore, we split batch BLMinto batches
with and without pseudo-CTC prompts to guarantee the token p re-
diction ability based on (6), that is, BLM→ {BLM,B∗
LM}.3. EXPERIMENTS
3.1. Experimental setup
To evaluate the proposed decoder-only ASR model with CTC
prompts, we used LibriSpeech [23], which contains an extern al
text-only LM corpus. We also used the Switchboard dataset, w hich
is generally combined with the Fisher corpus, as the externa l text-
only data for LM. Following [21], we trained both the baselin e
conformer model (Baseline EncDec) and the decoder-only mod el
with CTC prompts (Decoder-only); the difference was that th e latter
did not have source–target attention layers as described in Sec. 2.1.
The input acoustic features were 80-dimensional ﬁlter-ban k fea-
tures. The decoder (Eq. (1)) was a 6-block transformer, and t he
encoder (Eq. (2)) was a 12-block conformer, both with four-h ead
256-unit attention layers and 2048-unit feed-forward laye rs. The
models were trained using multi-task learning with CTC loss , as
described in Sec 2.3.1, with a weight of λ= 0.3in (5). Because we
empirically found that ASR training (Eq. (5)) was more impor tant
than LM training with text augmentation (Eq. (6)) for ASR tas ks, we
allocated most of the mini-batches to the ASR tasks and used 1 0% of
them for the LM tasks. Furthermore, we split the minibatch fo r LM
tasks as|BLM|:|B∗
LM|= 1 : 1 as discussed in Sec. 2.3.2. Threshold
θin Sec. 2.3.1 was set to two. We used the Adam optimizer with
Noam learning rate decay.
The external LMs were trained using the text-only data, incl ud-
ing an additional text corpus from LibriSpeech or Fisher. Th e LMs
were two-layer LSTMs with 512 units. We applied byte-pair en cod-
ing subword tokenization with 5,000 token classes for Libri Speech
and 2000 for Switchboard.
For comparison, we evaluated the WERs of CTC [24] and RNN-
T [25]. The baseline CTC used the same architecture as the afo re-
mentioned conformer encoder. RNN-T used a 15-block conform er
encoder with a one-layer 256-unit LSTM for the prediction ne twork.
While the baseline EncDec and the proposed Decoder-only mod els
were decoded in a label-synchronous manner, the CTC and RNN- T
models used time-synchronous beam search with a beam size of 10,
an LM-fusion weight of 0.4, and a length penalty of 1.0.
3.2. LibriSpeech 100h experiments
First, we used the 100h clean subset of the LibriSpeech pair d ata
and compared our proposed model under various conditions. W hen
using the external text-only data for the text augmentation or LM
training, the transcriptions of the 960h full training set w ere used.
We trained the Baseline EncDec conformer, RNN-T, and the pro -
posed decoder-only model using CTC prompts for 100 epochs. F or
Table 2 . An example of transcription reﬁnement by the decoder-only architecture.
Reference the life of every man in the castle shall answer it if a hair of h is head be signed show me his chamber
CTC the life of every man in the council shall answer it if a hair of his head be singinged show me his chamber
Decoder-only the life of every man in the castle shall answer it if a hair of his head be signed show me his chamber
Table 3 . ASR results with LibriSpeech 960h paired data and external
text results. The external LM was fused to all the models.
Models WER RTF
test-clean test-other
Baseline CTC [24] 3.1 7.0 0.30
Baseline EncDec [21] 2.6 6.2 0.54
Decoder-only w/ textAug 3.0 6.6 0.29
the Baseline EncDec and proposed Decoder-only models, we fu sed
CTC and LM with weights of 0.4 and 0.6, respectively, and the b eam
size was 10. To evaluate the pure performance of decoder-onl y mod-
els, we also trained the models with only the audio–text pair of 100h
set, i.e.,B=BASR, without any fusion. Furthermore, we measured
the real-time factor (RTF) of the inference on the test-clea n set using
an 8 core 3.60 GHz Intel i9-9900K CPU.
The results are listed in Table 1. First, the prompt compres-
sion methods discussed in Sec. 2.2 were compared (P1–3). Fol low-
ing [15], we applied convolution layers to downsample the fr ame
rate of the encoder to 80 ms (P1). Although they reported that a
frame rate of 80 ms performed best in a multilingual evaluati on,
it did not perform well in our setup. [16] reported that avera ging
the frames of the same CTC predictions slightly outperforme d re-
moving blank frames for ﬁne-tuning in the ST tasks; however, they
did not use it for the models trained from scratch. We also fou nd
that the averaging approach (P2) struggled in training for A SR tasks.
Thus, we conﬁrmed that removing frames of blank predictions was
the best choice for compressing the prompts (P3), and we used this
method for the following experiments. However, with only au dio–
text pair data (P3), the decoder-only architecture could no t outper-
form the Baseline EncDec results (B4), as [16] reported that training
the decoder-only model from scratch was slightly worse than the
conventional encoder–decoder model.
Next, we trained models with proposed text augmentation (D1 –
3). In contrast to pair-data-only training, text augmentat ion using an
external text-only corpus (D1) signiﬁcantly reduced the WE Rs from
9.5% and 23.0% to 9.3% and 19.9% for the test-clean and test-o ther
sets, respectively, compared to (P3). As discussed in Sec. 2 .3, be-
cause the structure of decoder is identical to autoregressi ve LMs, it is
an advantage of the decoder-only architecture to enhance th e model
with LM training without any modiﬁcation. Compared to the Ba se-
line EncDec fused with CTC and LM (B5), the proposed method
with only CTC fusion (D2) achieved comparable WERs. The prom pt
compression contributed to speeding up the computation; it was less
than half of the RTF of the Baseline EncDec (B5)1. The external
LM is also effective and complementary to the text augmentat ion,
as fusing both CTC and LM (D3) achieved the best performance;
7.0% and 17.5% for test-clean and test-other. It was also com pa-
rably fast with the Baseline CTC with LM fusion (B2), in which
LM computation was costly in time-synchronous decoding. As in
[15], we also evaluated the pre-training of the CTC and decod er
with the paired data and unpaired text data, respectively, f ollowed
by ﬁne-tuning using the paired data (F1), as described in Sec . 2.3.
However, the ﬁne-tuning approach was slightly worse than tr aining
from scratch (D3), presumably because the ﬁne-tuning appro ach was
1The number of frames was reduced down to 14.55% on average.Table 4 . ASR results with Switchboard paired data and Fisher text
corpus results. The external LM was fused to all the models.
Models WER RTF
Switchboard CallHome
Baseline CTC [24] 8.9 15.5 0.28
Baseline EncDec [21] 8.1 14.8 0.42
Decoder-only w/ textAug 7.3 13.3 0.28
less optimal. Thus, using text augmentation, this study suc cessfully
outperforms the conventional encoder–decoder model with a smaller
parameter size, lower RTF, and lower WERs by training the dec oder-
only model using same amount of data from scratch.
3.3. LibriSpeech 960h experiments
We also evaluated a full 960h pair set of LibriSpeech and its e x-
ternal text-only corpus. We trained the models for 30 epochs and
used the same beam size and fusion weights as in Sec. 3.2 for in fer-
ence. The results are listed in Table 3. The decoder-only mod el suc-
cessfully reﬁned CTC transcription, particularly in the te st-other set,
from 7.0% to 6.6%. We sampled an utterance from the test-othe r set
to show how the decoder reﬁnes CTC predictions in Table 2. How -
ever, compared with the Baseline EncDec, our method did not r each
its performance. We assume that the capacity of the decoder i s rela-
tively small because it is a 6-block transformer while a tran sformer
LM generally has 12 blocks or more [26]. However, our propose d
method still showed an advantage in the computational speed ; 0.29
of RTF against 0.54.
3.4. Switchboard and Fisher experiments
We evaluated the Switchboard models using Hub5’00 with the
Switchboard and CallHome subsets. We fused CTC and LM with
weights of 0.4 and 0.4, respectively, with a beam size of 10. T able 4
lists the results. We observed a similar tendency as in Secti on. 3.2;
the proposed model successfully outperformed the Baseline EncDec
model in both the test subsets, owing to effective text augme ntation.
4. CONCLUSION
We proposed a decoder-only architecture for ASR tasks that e ffec-
tively applies text augmentation using text-only addition al data. Au-
dio information was provided as CTC prompts, which mapped th e
output of the encoder module compressed by CTC predictions t o the
embedding space of the decoder. Although the model was train ed us-
ing an ASR task, the decoder was simultaneously trained for a n LM
task using augmented text data. We experimentally conﬁrmed that
the proposed methods reﬁned the baseline CTC using the decod er
and outperformed conventional RNN-T and encoder–decoder m od-
els with approximately half the computational cost in Libri Speech
100h and Switchboard setups.
Future work will include scaling up the decoder to have more c a-
pacity for LM while maintaining the advantage in inference s peed.
Extending the streaming approach can also be considered by e xploit-
ing its compactness and efﬁciency of the proposed approach.
5. REFERENCES
[1] Awni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, G reg
Diamos, Erich Elsen, Ryan Prenger, Sanjeev Satheesh, Shubh o
Sengupta, Adam Coates, et al., “Deep speech: Scaling up end-
to-end speech recognition,” arXiv preprint arXiv:1412.5567 ,
2014.
[2] Jan K. Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk,
Kyunghyun Cho, and Yoshua Bengio, “Attention-based mod-
els for speech recognition,” in Proc. of NIPS , 2015, pp. 577–
585.
[3] Zhong Meng, Sarangarajan Parthasarathy, Eric Sun, Yash esh
Gaur, Naoyuki Kanda, Liang Lu, Xie Chen, Rui Zhao, Jinyu
Li, and Yifan Gong, “Internal language model estimation
for domain-adaptive end-to-end speech recognition,” in 2021
IEEE Spoken Language Technology Workshop (SLT) , 2021, pp.
243–250.
[4] Albert Zeyer, Andr´ e Merboldt, Wilfried Michel, Ralf Sc hl¨ uter,
and Hermann Ney, “Librispeech transducer model with inter-
nal language model prior correction,” in Proc. of Interspeech ,
2021, pp. 2052–2056.
[5] Emiru Tsunoo, Yosuke Kashiwagi, Chaitanya Prasad Naris etty,
and Shinji Watanabe, “Residual language model for end-to-
end speech recognition,” in Proc. of Interspeech 2022 , 2022,
pp. 3899–3903.
[6] Adithya Renduchintala, Shuoyang Ding, Matthew Wiesner ,
and Shinji Watanabe, “Multi-modal data augmentation for en d-
to-end asr,” Proc. of Interspeech 2018 , pp. 2394–2398, 2018.
[7] Haihua Xu, Yerbolat Khassanov, Zhiping Zeng, Eng Siong
Chng, Chongjia Ni, Bin Ma, Haizhou Li, et al., “Independent
language modeling architecture for end-to-end ASR,” in Proc.
of ICASSP , 2020, pp. 7059–7063.
[8] Peidong Wang, Tara N. Sainath, and Ron J. Weiss, “Multita sk
training with text data for end-to-end speech recognition, ” in
Proc. of Interspeech , 2021, pp. 2566–2570.
[9] Zhehuai Chen, Yu Zhang, Andrew Rosenberg, Bhuvana Ram-
abhadran, Pedro J. Moreno, Ankur Bapna, and Heiga Zen,
“MAESTRO: Matched speech text representations through
modality matching,” in Proc. of Interspeech , 2022, pp. 4093–
4097.
[10] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakanta n,
Pranav Shyam, Girish Sastry, Amanda Askell, et al., “Lan-
guage models are few-shot learners,” Proc. of NurIPS , vol. 33,
pp. 1877–1901, 2020.
[11] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham,
Hyung Won Chung, Charles Sutton, Sebastian Gehrmann,
et al., “PaLM: Scaling language modeling with pathways,”
arXiv preprint arXiv:2204.02311 , 2022.
[12] Kai-Wei Chang, Yu-Kai Wang, Hua Shen, Iu-thing Kang, We i-
Cheng Tseng, Shang-Wen Li, and Hung-yi Lee, “Speech-
prompt v2: Prompt tuning for speech classiﬁcation tasks,”
arXiv preprint arXiv:2303.00733 , 2023.
[13] Dong Zhang, Shimin Li, Xin Zhang, Jun Zhan, Pengyu Wang,
Yaqian Zhou, and Xipeng Qiu, “SpeechGPT: Empowering
large language models with intrinsic cross-modal conversa -
tional abilities,” arXiv preprint arXiv:2305.11000 , 2023.[14] Paul K Rubenstein, Chulayuth Asawaroengchai, Duc Dung
Nguyen, Ankur Bapna, Zal´ an Borsos, F´ elix de Chaumont
Quitry, Peter Chen, Dalia El Badawy, Wei Han, Eugene
Kharitonov, et al., “AudioPaLM: A large language model that
can speak and listen,” arXiv preprint arXiv:2306.12925 , 2023.
[15] Yassir Fathullah, Chunyang Wu, Egor Lakomkin, Junteng Jia,
Yuan Shangguan, Ke Li, Jinxi Guo, Wenhan Xiong, Jay Ma-
hadeokar, Ozlem Kalinli, et al., “Prompting large language
models with speech recognition abilities,” arXiv preprint
arXiv:2307.11795 , 2023.
[16] Jian Wu, Yashesh Gaur, Zhuo Chen, Long Zhou, Yimeng
Zhu, Tianrui Wang, Jinyu Li, Shujie Liu, Bo Ren, Linquan
Liu, et al., “On decoder-only architecture for speech-to-
text and large language model integration,” arXiv preprint
arXiv:2307.03917 , 2023.
[17] Siddhant Arora, Hayato Futami, Yosuke Kashiwagi, Emir u
Tsunoo, Brian Yan, and Shinji Watanabe, “Integrating pre-
trained ASR and LM to perform sequence generation for spo-
ken language understanding,” in Proc. INTERSPEECH 2023 ,
2023, pp. 720–724.
[18] Marco Gaido, Mauro Cettolo, Matteo Negri, and Marco
Turchi, “CTC-based compression for direct speech transla-
tion,” in Proceedings of the 16th Conference of the Euro-
pean Chapter of the Association for Computational Linguis-
tics: Main Volume , 2021, pp. 690–696.
[19] Zhengkun Tian, Jiangyan Yi, Ye Bai, Jianhua Tao, Shuai
Zhang, and Zhengqi Wen, “FSR: Accelerating the inference
process of transducer-based models by applying fast-skip r eg-
ularization,” in Proc. of Interspeech , 2021, pp. 4034–4038.
[20] Yongqiang Wang, Zhehuai Chen, Chengjian Zheng, Yu Zhan g,
Wei Han, and Parisa Haghani, “Accelerating RNN-T training
and inference using CTC guidance,” in Proc. of ICASSP , 2023,
pp. 1–5.
[21] Pengcheng Guo, Florian Boyer, Xuankai Chang, Tomoki
Hayashi, Yosuke Higuchi, et al., “Recent developments
on espnet toolkit boosted by conformer,” arXiv preprint
arXiv:2010.13956 , 2020.
[22] Shinji Watanabe, Takaaki Hori, Suyoun Kim, John R. Hers hey,
and Tomoki Hayashi, “Hybrid CTC/attention architecture fo r
end-to-end speech recognition,” Journal of Selected Topics in
Signal Processing , vol. 11, no. 8, pp. 1240–1253, 2017.
[23] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanje ev
Khudanpur, “LibriSpeech: an ASR corpus based on public
domain audio books,” in Proc. of ICASSP , 2015, pp. 5206–
5210.
[24] Alex Graves, Santiago Fern´ andez, Faustino Gomez, and J¨ urgen
Schmidhuber, “Connectionist temporal classiﬁcation: la-
belling unsegmented sequence data with recurrent neural ne t-
works,” in Proc. of 23rd International Conference on Machine
Learning , 2006, pp. 369–376.
[25] Alex Graves, Abdel-Rahman Mohamed, and Geoffrey Hinto n,
“Speech recognition with deep recurrent neural networks,” in
Proc. of ICASSP , 2013, pp. 6645–6649.
[26] Kazuki Irie, Albert Zeyer, Ralf Schl¨ uter, and Hermann Ney,
“Language modeling with deep transformers,” in Proc. of In-
terspeech , 2019, pp. 3905–3909.