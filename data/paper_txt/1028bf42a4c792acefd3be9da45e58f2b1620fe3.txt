arXiv:2302.14132v1  [cs.CL]  27 Feb 2023STRUCTURED PRUNING OF SELF-SUPERVISED PRE-TRAINED MODELS
FOR SPEECH RECOGNITION AND UNDERSTANDING
Yifan Peng2∗, Kwangyoun Kim1, Felix Wu1, Prashant Sridhar1, Shinji Watanabe2
1ASAPP Inc., Mountain View, CA, USA2Carnegie Mellon University, Pittsburgh, PA, USA
{yifanpen,swatanab }@andrew.cmu.edu {kkim,fwu,psridhar }@asapp.com
ABSTRACT
Self-supervised speech representation learning (SSL) has shown to
be effective in various downstream tasks, but SSL models are usually
large and slow. Model compression techniques such as prunin g aim
to reduce the model size and computation without degradatio n in ac-
curacy. Prior studies focus on the pruning of Transformers; however,
speech models not only utilize a stack of Transformer blocks , but
also combine a frontend network based on multiple convoluti onal
layers for low-level feature representation learning. Thi s frontend
has a small size but a heavy computational cost. In this work, we pro-
pose three task-speciﬁc structured pruning methods to deal with such
heterogeneous networks. Experiments on LibriSpeech and SL URP
show that the proposed method is more accurate than the origi nal
wav2vec2-base with 10% to 30% less computation, and is able t o
reduce the computation by 40% to 50% without any degradation .
Index Terms —Structured pruning, self-supervised models,
speech recognition, spoken language understanding
1. INTRODUCTION
Self-supervised speech representation learning (SSL) has achieved
great success in a variety of speech processing tasks [1, 2, 3 , 4, 5,
6, 7]. However, SSL pre-trained models (e.g., wav2vec2 [8], Hu-
BERT [9] and WavLM [10]) usually require large memory and hig h
computational cost. Hence, it is difﬁcult to deploy them in r eal-
world applications. Recent studies have utilized model com pression
techniques to reduce the model size and computation without degra-
dation in accuracy. One common approach is knowledge distil la-
tion [11], which trains a small student model with a pre-spec iﬁed ar-
chitecture to match the soft targets generated by a large pre -trained
model. Distillation has shown to be effective in natural lan guage
processing (NLP) [12, 13] and speech processing [14, 15, 16, 17],
but it usually performs general distillation using large am ounts of
unlabeled data before task-speciﬁc distillation or ﬁne-tu ning. This
can make the training procedure computationally expensive .
Another compression technique is pruning, which extracts a
compact and accurate subnetwork from the original model. Pr uning
has been widely used in computer vision (CV) [18, 19, 20, 21] a nd
NLP [21, 22, 23, 24]. For speech models, [25, 26] prune recurr ent
neural networks (RNNs) for resource-limited applications . Another
work [27] prunes deep neural networks (DNNs) based speech en -
hancement models using the sparse group lasso regularizati on [28].
These studies do not consider SSL models. PARP [29] is one of
the ﬁrst pruning methods designed for SSL speech models, whi ch
prunes individual weights based on magnitudes. Despite its good
performance in low-resource automatic speech recognition (ASR),
∗Work done during an internship at ASAPP.PARP is a type of unstructured pruning and thus cannot achiev e an
actual speedup without an efﬁcient sparse matrix computati on li-
brary, which is not usually available in many deployment sce narios.
Another limitation is that PARP only prunes the Transformer layers
while keeping the convolutional feature extractor (CNN) ﬁx ed. As
discussed in [30], although the CNN has much fewer parameter s
than the Transformer, its computational cost is large and ca nnot
simply be ignored. For example, in wav2vec2-base, the CNN ha s
less than 5% of the total parameters but its computational co st is
nearly 33% in terms of multiply-accumulate (MAC) operation s for
a 10-second audio.
In this work, we propose HJ-Pruning (Heterogeneous Joint
Pruning) where both CNN and Transformer components are prun ed
jointly. We consider three variants: (a) HJ-Pruning based on the
overall model size sets a single sparsity for the entire model size.
(b)HJ-Pruning based on separate model sizes introduces a sepa-
rate sparsity hyperparameter for each component which allo ws ﬁne-
grained tuning to ﬁnd a trade-off between CNN and Transforme r. (c)
HJ-Pruning based on the overall MACs uses multiply–accumulate
(MAC) operations as the sparsity criterion to ﬁnd the best al location
of the computation budget across different components. We e valuate
our methods in the ASR and spoken language understanding (SL U)
tasks. Experiments on LibriSpeech and SLURP show that all HJ -
Pruning methods outperform the previous Transformer-only pruning
strategy. Our HJ-Pruning-MAC is more accurate than the orig inal
wav2vec2-base with 10% to 30% less computation, and is able t o
reduce the computation by 40% to 50% without any degradation .
2. BACKGROUND
2.1. Self-supervised pre-trained speech models
We evaluate the pruning algorithms mainly using wav2vec2 [8 ], but
our proposed methods can be easily applied to other SSL model s
with as a similar architecture such as HuBERT [9] (see Sec. 4. 5),
SEW-D [30], and WavLM [10]. The wav2vec2-base model (pre-
trained on Librispeech 960h [31]) consists of a convolution al feature
extractor (CNN) and a Transformer [32] encoder. The CNN con-
tains seven temporal convolutions with 512 channels and GeL U [33]
activations. The Transformer encoder is a stack of 12 Transf ormer
layers with a hidden dimension of 768 and 12 attention heads.
2.2. Structured pruning using L0regularization
We follow [22, 23, 34] to formulate the structured pruning ta sk as
a regularized learning problem, which aims to learn a sparse model.
Letf(·;θ)be a model with parameters θ={θj}n
j=1, where each θj
is a group of parameters (e.g., an attention head) and nis the num-
ber of groups. The pruning decisions are given by a set of bina ry
variables called gates :z={zj}n
j=1wherezj∈ {0,1}. The model
parameters after pruning are ˜θ={˜θj}n
j=1such that ˜θj=θjzj. We
usually sample gates from some distributions (e.g., Bernou lli) and
update their parameters during training. Suppose the gates follow a
distribution q(z;α)with parameters α={αj}n
j=1, then our train-
ing objective is:
min
θ,αEz∼q[
1
DD∑
i=1L(f(xi;˜θ),yi)+λ∥˜θ∥0]
, (1)
where{(xi,yi)}D
i=1is the training data containing Dsamples,Lis
the training loss (i.e., CTC loss for ASR, cross entropy loss for SLU),
andλ >0is a hyperparameter to control the sparsity. However, it
is intractable to optimize Eq. (1) using gradient descent be cause the
gates are discrete. Louizos et al. [34] propose a reparamete rization
trick to make the loss differentiable, which has been widely used in
sparse model learning. Here we only introduce their ﬁnal app roach.
Please refer to [34] for the derivation. Louizos et al. adopt the Hard
Concrete Distribution [34] to model the gates z:
u∼ U(0,1),v(α) =σ(1
β(
logu
1−u+logα))
,
¯v(α) = (r−l)·v(α)+l,z= min(1,max(0,¯v(α))),(2)
whereU(0,1)is a uniform distribution over the interval [0,1],σ(·)
is the sigmoid function and βis a temperature constant. The actual
parameters are α.l <0andr >0are two constants to stretch
the output of sigmoid to [l,r], which is ﬁnally rectiﬁed to [0,1]. It
is proven that the ﬁrst term in Eq. (1) now becomes differenti able
w.r.t. all parameters. We can write the second term in a close d-form
expression based on the distribution of zshown in Eq. (2):
Ez[
∥˜θ∥0]
=n∑
j=1P(zj̸= 0) =n∑
j=1σ(
logαj−βlog−l
r)
,
(3)
which is also differentiable. P(·)denotes the probability.
Now we can train a sparse model using Eq. (1). However, it is
difﬁcult to exactly control the pruned model size [22, 23]. I nstead of
adding a regularizer λ∥˜θ∥0, prior studies [22, 23] suggest optimizing
the training loss subject to an explicit equality constrain t on sparsity:
min
θ,αEz[
1
DD∑
i=1L(f(xi;˜θ),yi)]
s.t.s(α) =t, (4)
wheres(α)is the current sparsity and tis a pre-speciﬁed target spar-
sity. The sparsity is deﬁned as the percentage of parameters that are
pruned. Similar to Eq. (3), given the current parameters α, we can
calculate the expected number of nonzero gates in every modu le of
the model. Recall that each gate is associated with a group of pa-
rameters. Hence, we know the expected number of parameters t hat
are still kept, which further gives us the sparsity s(α). Eq. (4) can
be rewritten as an adversarial game according to the augment ed La-
grangian method [22]:
max
λmin
θ,αEz[
1
DD∑
i=1L(f(xi;˜θ),yi)]
+g(λ,α), (5)
g(λ,α) =λ1(s(α)−t)+λ2(s(α)−t)2, (6)
whereλ1,λ2∈Rare two Lagrange multipliers that are jointly
trained with other parameters. Once the game reaches equili brium,
the equality constraint will be satisﬁed. Hence, we can prec isely
control the sparsity of the pruned model. To facilitate trai ning, we
linearly increase the target sparsity tfrom zero to the desired value.2.3. Structured pruning of Transformer layers
A Transformer [32] layer consists of a multi-head self-atte ntion
(MHA) block and a position-wise feed-forward network (FFN) . We
consider three pruning units, i.e., attention heads (12 per layer),
intermediate size of FFN (3072 per layer), and the model’s hi dden
size (768). We deﬁne a gate for each pruning unit. Given an inp ut
sequence X∈RT×dof length Tand feature size d, the MHA and
FFN at a particular layer are the following:
MHA(X) =h∑
k=1(zhead
k·ATT(X;Watt
k)), (7)
FFN(X) =GeLU(XWffn
1)·diag(zint)·Wffn
2, (8)
where ATT (·;Watt
k)denotes the k-th attention head parameterized
byWatt
k, andzhead
kis a scalar gate. There are hheads in total. zint
is adint-dimensional gate for the FFN intermediate size. diag (·)
creates a diagonal matrix with its argument vector on the dia go-
nal. GeLU is an activation [33]. FFN has two linear layers Wffn
1∈
Rd×dint,Wffn
2∈Rdint×d. Each Transformer layer has its own gates
and their parameters are independent. For the main hidden si ze, we
deﬁne a gate zhidof sizedand share it across layers as in [23].
3. PROPOSED METHODS
3.1. Joint pruning based on the model size
As introduced in Sec. 1, the convolutional feature extracto r (CNN) in
SSL models is small in size but heavy in computation. To optim ize
the overall computation, we propose to jointly prune the CNN and
Transformer. We have introduced the pruning units for Trans former
in Sec. 2.3. For CNN, we prune convolution channels by introd uc-
ing gate variables for every channel in every CNN layer, i.e. , each
output channel is multiplied with a gate. To train the model u sing
Eq. (5), we need to deﬁne the model sparsity s(α). Our ﬁrst pro-
posed method is HJ-Pruning-Size (HJ-Pruning based on the overall
model size), which can be viewed as a direct extension from pr ior
work [22, 23]. Speciﬁcally, given the current distribution parame-
tersα, we can calculate the probability of each gate being nonzero
(i.e., the corresponding module is kept) as in Eq. (3). We the n know
the current sizes of all modules, including the model’s hidd en size,
CNN channels, attention heads, and FFN intermediate sizes. Based
on these sizes, we can compute the percentage of parameters t hat are
pruned, which is the overall size sparsity sall
size(α).
However, Sec. 4.2 shows that this approach does not work well
in practice, because the CNN has much fewer parameters than t he
Transformer. If we simply set an overall sparsity, paramete rs will be
pruned mainly from Transformer. To solve this problem, we pr opose
the second method, i.e., HJ-Pruning-SepSize (HJ-Pruning based on
separate model sizes). We calculate the size sparsity separ ately for
CNN (scnn
size(α)) and Transformer ( strans
size(α)). We also specify sepa-
rate target sparsities tcnn
size,ttrans
sizeand extend Eq. (6) to have two terms:
gsize=λcnn
1(scnn
size(α)−tcnn
size)+λcnn
2(scnn
size(α)−tcnn
size)2
+λtrans
1(strans
size(α)−ttrans
size)+λtrans
2(strans
size(α)−ttrans
size)2.(9)
As shown in Sec. 4.2, this method achieves strong performanc e.
However, it requires careful tuning of the separate target s parsities.
We always need to search over the two sparsities to meet a part icular
budget, which is computationally expensive.
3.2. Joint pruning based on the MACs
The third method we propose is HJ-Pruning-MAC (HJ-Pruning
based on the overall MACs). Unlike prior methods, we prune th e
entire model to directly meet a computational budget measur ed by
MACs. We follow the formulas used in the DeepSpeed ﬂops proﬁl er
to calculate MACs.1For an input sequence of length Tand hidden
sized, the MACs for each MHA and FFN block are as follows:
MACmha= 4Thddhead+2T2hdhead, (10)
MACffn= 2Tddint, (11)
wherehis the number of attention heads and dheadis the size per
head.dintdenotes the intermediate size of FFN. The MACs of a 1-D
convolution can be computed by
MACcnn=ToutCoutCinK, (12)
whereToutis the output length and Kis the kernel size. Cinand
Coutare the input and output channels, respectively. Note that
h,d,dint,Cin,Coutare calculated from the current gate distributions
(similar to Eq. (3)). They are differentiable functions of α. We
deﬁne the percentage of MACs that are pruned as the MACs-base d
sparsitysall
macs(α).2It is differentiable w.r.t. parameters α. Hence,
we can still train the model using Eq. (5).
4. EXPERIMENTS
4.1. Experimental setup
We focus on task-speciﬁc structured pruning of SSL speech mo dels.
We mainly prune wav2vec2-base, but we also show that our meth ods
can be directly applied to HuBERT-base in Sec. 4.5. We conduc t ex-
periments using PyTorch [35] and HuggingFace’s transforme rs [36].
Our implementation of the basic pruning algorithm is based o n prior
work in NLP [23]. For each task, we add a linear layer on top of
the pre-trained SSL model and ﬁne-tune the entire model to ob tain
an unpruned model. Then, we prune this ﬁne-tuned model to rea ch
a speciﬁc sparsity using Eq. (5). We employ an AdamW optimize r
and a linear learning rate scheduler for all experiments.
ASR : The 100-hour clean set of LibriSpeech [31] is utilized. In
Sec. 4.3, the Tedlium [37] test set is used as out-of-domain d ata to
demonstrate the robustness of structured pruning. The trai ning loss
is CTC [38]. We ﬁne-tune a pre-trained model for 25 epochs and
prune for 30 epochs with a learning rate of 1.5e-4 and a batch s ize
of 64. The target sparsity is linearly increased to the desir ed value
during the ﬁrst 5 epochs. The learning rate of αandλis selected
from{0.02,0.05}. The pruned model is ﬁne-tuned for another 10
epochs with a learning rate of 5e-5. The learning rate warmup steps
are 3k, 3k, and 1k for training, pruning, and ﬁne-tuning, res pectively.
SLU : The SLURP corpus [39] is used for intent classiﬁcation. A
pre-trained SSL model is ﬁne-tuned for 50 epochs and pruned f or 50
epochs with a learning rate of 1e-4 and a batch size of 80. The ﬁ nal
ﬁne-tuning has 10 epochs with a learning rate of 1e-5. The lea rning
rate warmup is performed for 4k, 4k, 1k steps for training, pr uning,
and ﬁne-tuning, respectively. Other conﬁgs are the same as A SR.
1https://github.com/microsoft/DeepSpeed
2The computation of MACs also depends on the sequence length T, be-
cause MHA has quadratic complexity w.r.t. T. We use 10 seconds to compute
MACs in our experiments. This is a “virtual” length used only for computing
MACs. We do not modify any training utterances.20 30 40 50 60 705.777911
MACs (×109)% Word Error Rate ( ↓)Unpruned wav2vec2-base
Pruning Transformer Only
HJ-Pruning-Size
HJ-Pruning-SepSize
HJ-Pruning-MAC
Fig. 1 : Word Error Rate (%) vs. Multiply-Accumulate Operations
on LibriSpeech test-clean. Our proposed HJ-Pruning method s con-
sistently outperform the baseline.
10 20 30 40 50 60 7075808586.1
MACs (×109)% Accuracy ( ↑)
Unpruned wav2vec2-base
Pruning Transformer Only
HJ-Pruning-Size
HJ-Pruning-SepSize
HJ-Pruning-MAC
Fig. 2 : Intent Classiﬁcation Accuracy (%) vs. Multiply-Accumula te
Operations on the SLURP test set. Our proposed HJ-Pruning me th-
ods consistently outperform the baseline.
4.2. Main results
Fig. 1 compares various pruning methods for LibriSpeech ASR .
The unpruned model has good performance (5.77% WER) but is
computationally expensive (74.4 GMACs). At a low sparsity ( >55
GMACs), all pruned models achieve similar WERs which are eve n
better than the original result, because the pruning target can regu-
larize the training. As the sparsity increases, the baselin e method
which only prunes Transformer drastically degrades. Our pr oposed
three algorithms which jointly prune CNN and Transformer co nsis-
tently outperform the baseline by a large margin. We can redu ce
over 40% of the total computation without degradation in WER . HJ-
Pruning-MAC has similar performance with HJ-Pruning-SepS ize,
both outperforming HJ-Pruning-Size. This is because the CN N has
much fewer parameters than Transformer. If we simply set an o verall
size sparsity, the pruned parameters are mainly from Transf ormer,
while CNN still has high computational overhead. To prune th em
based on separate sizes (Eq. (9)), we have to search for the be st com-
bination of the two target sparsities. This model selection procedure
is presented in Fig. 3a, where we perform a grid search and sel ect
the Pareto frontiers. This requires much more computation t han the
other methods. Hence, the HJ-Pruning-MAC is probably the be st
method in terms of performance and complexity.
Fig. 2 shows the results of intent classiﬁcation on SLURP. Th e
overall trend is very similar to that of ASR. Our joint prunin g meth-
ods outperform the baseline by a large margin, especially at a high
sparsity (low MACs). HJ-Pruning-SepSize is comparable wit h HJ-
Pruning-MAC, but again, it requires a grid search over the tw o target
sparsities as shown in Fig. 3b. This high complexity hinders its usage
in practice. Compared to ASR, we can achieve a higher compres sion
rate (over 55%) without loss in accuracy. This is probably be cause
30 40 50 60 705.566.57
MACs (×109)% Word Error Rate ( ↓)Unpruned wav2vec2-base
Transformer Sparsity = 0.1
Transformer Sparsity = 0.2
Transformer Sparsity = 0.3
Transformer Sparsity = 0.4
Selected Points
(a) Word Error Rates (%) on LibriSpeech test-clean.
30 40 50 60 7084858687
MACs (×109)% Accuracy ( ↑)
Unpruned wav2vec2-base
Transformer Sparsity = 0.1
Transformer Sparsity = 0.2
Transformer Sparsity = 0.3
Transformer Sparsity = 0.4
Transformer Sparsity = 0.5
Transformer Sparsity = 0.6
Selected Points
(b) Intent Classiﬁcation Accuracy (%) on the SLURP test set.
Fig. 3 : Model selection for HJ-Pruning-SepSize. As described in
Sec. 3.1, we perform grid search over the Transformer sparsi ty (0.1
to 0.4/0.6) and CNN sparsity (0.1 to 0.95). The Pareto fronti ers are
shown in blue, which are also presented in Fig. 1 and Fig. 2.
20 30 40 50 60 701820222426
MACs (×109)% Word Error Rate ( ↓)Unpruned wav2vec2-base
Pruning Transformer Only
HJ-Pruning-Size
HJ-Pruning-SepSize
HJ-Pruning-MAC
Fig. 4 : Robustness analysis. All models are trained on LibriSpeec h
100h and then directly evaluated on the out-of-domain Tedlium test
set. The trend is similar to that of the in-domain evaluation in F ig. 1.
the classiﬁcation task is easier and thus requires less info rmation
than the sequence-to-sequence task.
4.3. Robustness of structured pruning
To investigate the robustness of the proposed structured pr uning
methods, we test the ASR models using an out-of-domain corpus,
TED-LIUM [37]. Note that these models are trained only with L ib-
riSpeech data. As shown in Fig. 4, again, our joint pruning me thods
consistently outperform the baseline, and the trend is very similar to
that of the in-domain evaluation (see Fig. 1). This demonstr ates that
our pruning methods are robust.
4.4. Architectures of pruned models
Fig. 5 shows the remaining CNN channels, attention heads and FFN
intermediate sizes after HJ-Pruning-MAC. The target spars ity ranges
from 10% to 40%. For CNN, the sequence length gradually de-
creases due to downsampling. The ﬁrst few layers have higher com-
putational cost, so they tend to be pruned more. For MHA and FF N,1 2 3 4 5 6 7200300400500
CNN LayerChannels10% 20% 30% 40%
1 2 3 4 5 6 7 8 9 10 11 1204812
MHA LayerHeads
1 2 3 4 5 6 7 8 9 10 11 121,0002,0003,000
FFN LayerInterm. Sizes
Fig. 5 : ASR model architectures after HJ-Pruning-MAC. The target
sparsity ranges from 10% to 40%.
20 30 40 50 60 705.73791113
MACs (×109)% Word Error Rate ( ↓)
Unpruned HuBERT-base
DistilHuBERT∗
LightHuBERT-small∗
LightHuBERT-base∗
HJ-Pruning-MAC
Fig. 6 : Results of pruning HuBERT-base on LibriSpeech test-clean .
∗WERs from SUPERB [1]. See Sec. 4.5 for discussions.
the upper layers are pruned the most, indicating that upper l ayers are
more redundant. Prior studies had similar observations by a nalyz-
ing the self-attention patterns in speech encoders [40, 41, 42]. The
overall trend is also consistent with a prior work in NLP [23] .
4.5. Comparison with other compression methods
As introduced in Sec. 2.1, HJ-Pruning can be directly applie d to
other SSL models. In Fig. 6, we prune the HuBERT-base model
based on the overall MACs for ASR. The performance is similar to
that of the wav2vec2. We also include other compressed model s for
comparison, including DistilHuBERT [15] and LightHuBERT [ 16].
Note that these results are not really comparable due to: (1) Their
WERs are from SUPERB [1], which combines a frozen SSL model
with another learnable RNN. We also tried to replace the RNN w ith
a single linear layer and ﬁne-tune the entire model (same as o ur set-
ting), but the performance was clearly worse. (2) Their comp ressed
models are initially distilled using the 960h unlabeled Lib riSpeech
data and then ﬁne-tuned on the 100h labeled data, but our task -
speciﬁc pruning only utilizes the 100h data. This comparison shows
that our task-speciﬁc pruning method is highly effective.
5. CONCLUSION
In this paper, we propose HJ-Pruning to jointly prune hetero ge-
neous components of SSL speech models, which achieves stron g
performance-efﬁciency tradeoffs compared to several base lines. At
a small sparsity (0.1 to 0.3), HJ-Pruning improves the wav2v ec2
baseline while being faster. Depending on the task, HJ-Prun ing
saves 40% or 50% MACs while maintaining the performance of
wav2vec2. HJ-Pruning is a general method that can be applied to
most of speech SSL models such as HuBERT. In the future, we pla n
to explore the application of HJ-Pruning on encoder-decode r SSL
models [43] and other SLU tasks [44, 5].
6. REFERENCES
[1] S. w. Yang, P.-H. Chi, Y .-S. Chuang, et al., “SUPERB: Spee ch
Processing Universal PERformance Benchmark,” in Proc. In-
terspeech , 2021.
[2] A. Mohamed, H.-y. Lee, L. Borgholt, et al., “Self-
Supervised Speech Representation Learning: A Review,”
arXiv:2205.10643 , 2022.
[3] X. Chang, T. Maekaku, P. Guo, et al., “An exploration of se lf-
supervised pretrained representations for end-to-end spe ech
recognition,” in Proc. ASRU , 2021.
[4] Z. Huang, S. Watanabe, S.-w. Yang, et al., “Investigatin g
Self-Supervised Learning for Speech Enhancement and Sep-
aration,” in Proc. ICASSP , 2022.
[5] S. Shon, A. Pasad, F. Wu, et al., “SLUE: New Benchmark
Tasks For Spoken Language Understanding Evaluation on Nat-
ural Speech,” in Proc. ICASSP , 2022.
[6] S. Arora, S. Dalmia, P. Denisov, et al., “ESPnet-SLU: Ad-
vancing Spoken Language Understanding Through ESPnet,”
inProc. ICASSP , 2022.
[7] Y . Peng, S. Arora, Y . Higuchi, et al., “A Study on the Integ ra-
tion of Pre-trained SSL, ASR, LM and SLU Models for Spoken
Language Understanding,” in Proc. SLT , 2022.
[8] A. Baevski, Y . Zhou, A. Mohamed, and M. Auli, “wav2vec
2.0: A framework for self-supervised learning of speech rep re-
sentations,” in Proc. NeurIPS , 2020.
[9] W.-N. Hsu, B. Bolte, Y .-H. H. Tsai, et al., “HuBERT: Self-
supervised speech representation learning by masked predi c-
tion of hidden units,” IEEE/ACM Trans. Audio, Speech, Lang.
Process. , vol. 29, pp. 3451–3460, 2021.
[10] S. Chen, C. Wang, Z. Chen, et al., “WavLM: Large-scale se lf-
supervised pre-training for full stack speech processing, ”IEEE
Journal of Selected Topics in Signal Processing , 2022.
[11] G. Hinton, O. Vinyals, J. Dean, et al., “Distilling the k nowl-
edge in a neural network,” arXiv:1503.02531 , 2015.
[12] V . Sanh, L. Debut, J. Chaumond, and T. Wolf, “DistilBERT , a
distilled version of BERT: smaller, faster, cheaper and lig hter,”
arXiv:1910.01108 , 2019.
[13] X. Jiao, Y . Yin, et al., “TinyBERT: Distilling BERT for n atural
language understanding,” in Findings of EMNLP , 2020.
[14] Z. Peng, A. Budhkar, I. Tuil, et al., “Shrinking bigfoot : Reduc-
ing wav2vec 2.0 footprint,” in SustaiNLP , 2021.
[15] H.-J. Chang, S.-w. Yang, and H.-y. Lee, “DistilHuBERT:
Speech representation learning by layer-wise distillatio n of
hidden-unit BERT,” in Proc. ICASSP , 2022.
[16] R. Wang, Q. Bai, J. Ao, et al., “LightHuBERT: Lightweigh t
and Conﬁgurable Speech Representation Learning with Once-
for-All Hidden-Unit BERT,” in Proc. Interspeech , 2022.
[17] Y . Lee, K. Jang, J. Goo, et al., “FitHuBERT: Going Thin-
ner and Deeper for Knowledge Distillation of Speech Self-
Supervised Models,” in Proc. Interspeech , 2022.
[18] S. Han, J. Pool, et al., “Learning both weights and conne ctions
for efﬁcient neural network,” in Proc. NeurIPS , 2015.
[19] H. Li, A. Kadav, I. Durdanovic, et al., “Pruning Filters for
Efﬁcient ConvNets,” in Proc. ICLR , 2017.
[20] Z. Liu, J. Li, Z. Shen, et al., “Learning Efﬁcient Convol utional
Networks Through Network Slimming,” in Proc. ICCV , 2017.
[21] Q. Zhang, S. Zuo, C. Liang, et al., “PLATON: Pruning larg e
transformer models with upper conﬁdence bound of weight im-
portance,” in Proc. ICML , 2022.[22] Z. Wang, J. Wohlwend, and T. Lei, “Structured Pruning of
Large Language Models,” in Proc. EMNLP , 2020.
[23] M. Xia, Z. Zhong, and D. Chen, “Structured Pruning Learn s
Compact and Accurate Models,” in Proc. ACL , 2022.
[24] C. Liang, S. Zuo, M. Chen, et al., “Super tickets in pre-t rained
language models: From model compression to improving gen-
eralization,” in Proc. ACL , 2021.
[25] P. Dong, S. Wang, W. Niu, et al., “Rtmobile: Beyond real-
time mobile acceleration of rnns for speech recognition,” i n
ACM/IEEE Design Automation Conference (DAC) , 2020.
[26] S. Wang, P. Lin, R. Hu, et al., “Acceleration of LSTM With
Structured Pruning Method on FPGA,” IEEE Access , 2019.
[27] K. Tan and D.L. Wang, “Compressing Deep Neural Networks
for Efﬁcient Speech Enhancement,” in Proc. ICASSP , 2021.
[28] S. Scardapane, D. Comminiello, A. Hussain, and A. Uncin i,
“Group sparse regularization for deep neural networks,” Neu-
rocomputing , vol. 241, pp. 81–89, 2017.
[29] C.-I J. Lai, Y . Zhang, A. H. Liu, et al., “PARP: Prune, Ad-
just and Re-Prune for Self-Supervised Speech Recognition, ”
inProc. NeurIPS , 2021.
[30] F. Wu, K. Kim, J. Pan, et al., “Performance-Efﬁciency Tr ade-
offs in Unsupervised Pre-training for Speech Recognition, ” in
Proc. ICASSP , 2022.
[31] V . Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Lib-
rispeech: An ASR corpus based on public domain audio
books,” in Proc. ICASSP , 2015.
[32] A. Vaswani, N. Shazeer, N. Parmar, et al., “Attention is all you
need,” in Proc. NeurIPS , 2017.
[33] D. Hendrycks and K. Gimpel, “Gaussian Error Linear Unit s
(GELUs),” arXiv:1606.08415 , 2016.
[34] C. Louizos, M. Welling, and D. P. Kingma, “Learning Spar se
Neural Networks through L0 Regularization,” in ICLR , 2018.
[35] A. Paszke et al., “Pytorch: An imperative style, high-
performance deep learning library,” Proc. NeurIPS , 2019.
[36] T. Wolf et al., “Huggingface’s transformers: State-of -the-art
natural language processing,” arXiv:1910.03771 , 2019.
[37] A. Rousseau et al., “TED-LIUM: an automatic speech reco g-
nition dedicated corpus.,” in Proc. LREC , 2012.
[38] A. Graves, S. Fern´ andez, F. Gomez, et al., “Connection ist tem-
poral classiﬁcation: labelling unsegmented sequence data with
recurrent neural networks,” in Proc. ICML , 2006.
[39] E. Bastianelli, A. Vanzo, P. Swietojanski, and V . Riese r,
“SLURP: A Spoken Language Understanding Resource Pack-
age,” in Proc. EMNLP , 2020.
[40] S. Zhang, E. Loweimi, P. Bell, and S. Renals, “On the use-
fulness of self-attention for automatic speech recognitio n with
transformers,” in Proc. SLT , 2021.
[41] Y . Peng, S. Dalmia, I. Lane, and S. Watanabe, “Branch-
former: Parallel MLP-attention architectures to capture l ocal
and global context for speech recognition and understandin g,”
inProc. ICML , 2022.
[42] T. Maekaku, Y . Fujita, Y . Peng, and S. Watanabe, “Attent ion
Weight Smoothing Using Prior Distributions for Transforme r-
Based End-to-End ASR,” in Proc. Interspeech , 2022.
[43] F. Wu, K. Kim, S. Watanabe, et al., “Wav2seq: Pre-
training speech-to-text encoder-decoder models using pse udo
languages,” arXiv:2205.01086 , 2022.
[44] L. Lugosch, M. Ravanelli, P. Ignoto, et al., “Speech mod el
pre-training for end-to-end spoken language understandin g,” in
Interspeech , 2019.